{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abc4ec6",
   "metadata": {},
   "source": [
    "# RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e847422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    text: str\n",
    "    metadata: Dict[str, any]\n",
    "    chunk_id: str\n",
    "\n",
    "class FocusedThaiMedicalProcessor:\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\"):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            print(\"BGE-M3 model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_name}: {e}\")\n",
    "            print(\"Trying fallback model...\")\n",
    "            self.model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "            print(\"Fallback model loaded!\")\n",
    "\n",
    "        # Initialize storage for embeddings and chunks\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "\n",
    "    def extract_focused_sections(self, pdf_path: str, page_offset: int = 5) -> Dict[str, List[Dict[str, any]]]:\n",
    "      \"\"\"Extract only the two target sections (page-wise with page numbers).\"\"\"\n",
    "      reader = PdfReader(pdf_path)\n",
    "\n",
    "      # Sections you want (TOC/page-number based, 1-indexed in doc)\n",
    "      target_sections = {\n",
    "          'general_knowledge': (4, 23),            # ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©\n",
    "          'investigation_guidelines': (24, 61),    # ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©\n",
    "      }\n",
    "\n",
    "      extracted_sections: Dict[str, List[Dict[str, any]]] = {}\n",
    "\n",
    "      for section_name, (start_page, end_page) in target_sections.items():\n",
    "          # Convert to 0-indexed actual PDF pages with offset\n",
    "          actual_start = start_page + page_offset - 1\n",
    "          actual_end_exclusive = end_page + page_offset - 1  # we'll use as exclusive in range()\n",
    "\n",
    "          print(\n",
    "              f\"Extracting {section_name}: TOC pages {start_page}-{end_page} \"\n",
    "              f\"(actual PDF pages {actual_start+1}-{actual_end_exclusive})\"\n",
    "          )\n",
    "\n",
    "          pages_list: List[Dict[str, any]] = []\n",
    "          # Iterate page-by-page; note: range() stop is exclusive\n",
    "          for p in range(actual_start, min(actual_end_exclusive, len(reader.pages))):\n",
    "              page_text = reader.pages[p].extract_text() or \"\"\n",
    "              page_text = self._clean_text(page_text)\n",
    "              pages_list.append({\n",
    "                  \"page_number\": p + 1,  # human-readable (1-based)\n",
    "                  \"text\": page_text\n",
    "              })\n",
    "\n",
    "          extracted_sections[section_name] = pages_list\n",
    "\n",
    "      return extracted_sections\n",
    "\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean up extracted text\"\"\"\n",
    "        # Remove multiple newlines\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # Keep Thai characters, English, numbers, and basic punctuation\n",
    "        text = re.sub(r'[^\\u0E00-\\u0E7F\\u0020-\\u007E\\u00A0-\\u00FF\\n\\r\\t]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def create_focused_chunks(self, sections: Dict[str, List[Dict[str, any]]], chunk_size: int = 800) -> List[DocumentChunk]:\n",
    "      \"\"\"\n",
    "      Create chunks from page-wise sections and carry page numbers into metadata.\n",
    "\n",
    "      Expected sections format:\n",
    "      {\n",
    "        'section_name': [\n",
    "          { 'page_number': int, 'text': str },\n",
    "          ...\n",
    "        ],\n",
    "        ...\n",
    "      }\n",
    "      \"\"\"\n",
    "      all_chunks: List[DocumentChunk] = []\n",
    "\n",
    "      for section_name, pages in sections.items():\n",
    "          # Backward compatibility: if old code passes a big string, wrap it as a single pseudo-page\n",
    "          if isinstance(pages, str):\n",
    "              pages = [{\"page_number\": None, \"text\": pages}]\n",
    "\n",
    "          total_chars = sum(len(p[\"text\"]) for p in pages if p.get(\"text\"))\n",
    "          print(f\"\\nChunking {section_name} ({total_chars} characters across {len(pages)} pages)\")\n",
    "\n",
    "          # Build a list of (paragraph_text, page_number)\n",
    "          para_items: List[Dict[str, any]] = []\n",
    "          for item in pages:\n",
    "              pnum = item.get(\"page_number\")\n",
    "              ptxt = item.get(\"text\") or \"\"\n",
    "              # split by double newline into paragraphs (like old behavior)\n",
    "              for para in (ptxt.split(\"\\n\\n\") if ptxt else []):\n",
    "                  para = para.strip()\n",
    "                  if para:\n",
    "                      para_items.append({\"page_number\": pnum, \"text\": para})\n",
    "\n",
    "          # Greedy pack paragraphs into chunks\n",
    "          current_text = \"\"\n",
    "          current_pages_set = set()\n",
    "          chunks_for_section = []\n",
    "          for para in para_items:\n",
    "              candidate = (current_text + (\"\\n\\n\" if current_text else \"\") + para[\"text\"])\n",
    "              if current_text and len(candidate) > chunk_size:\n",
    "                  # flush current chunk\n",
    "                  if current_text.strip():\n",
    "                      page_list = sorted([p for p in current_pages_set if p is not None])\n",
    "                      page_start = page_list[0] if page_list else None\n",
    "                      page_end = page_list[-1] if page_list else None\n",
    "                      chunks_for_section.append({\n",
    "                          \"text\": current_text.strip(),\n",
    "                          \"page_start\": page_start,\n",
    "                          \"page_end\": page_end,\n",
    "                          \"pages\": page_list\n",
    "                      })\n",
    "                  # reset with current paragraph\n",
    "                  current_text = para[\"text\"]\n",
    "                  current_pages_set = set([para[\"page_number\"]])\n",
    "              else:\n",
    "                  # accumulate\n",
    "                  current_text = candidate\n",
    "                  if para[\"page_number\"] is not None:\n",
    "                      current_pages_set.add(para[\"page_number\"])\n",
    "\n",
    "          # flush last chunk\n",
    "          if current_text.strip():\n",
    "              page_list = sorted([p for p in current_pages_set if p is not None])\n",
    "              page_start = page_list[0] if page_list else None\n",
    "              page_end = page_list[-1] if page_list else None\n",
    "              chunks_for_section.append({\n",
    "                  \"text\": current_text.strip(),\n",
    "                  \"page_start\": page_start,\n",
    "                  \"page_end\": page_end,\n",
    "                  \"pages\": page_list\n",
    "              })\n",
    "\n",
    "          # Convert into DocumentChunk objects with page metadata\n",
    "          for i, ch in enumerate(chunks_for_section):\n",
    "              chunk = DocumentChunk(\n",
    "                  text=ch[\"text\"],\n",
    "                  metadata={\n",
    "                      'section_name': section_name,\n",
    "                      'chunk_index': i,\n",
    "                      'section_total_chunks': len(chunks_for_section),\n",
    "                      'language': 'thai',\n",
    "                      'source': 'thai_medical_guide',\n",
    "                      'length': len(ch[\"text\"]),\n",
    "                      # NEW: page info\n",
    "                      'page_start': ch[\"page_start\"],\n",
    "                      'page_end': ch[\"page_end\"],\n",
    "                      'pages': ch[\"pages\"],  # list[int]\n",
    "                  },\n",
    "                  chunk_id=f\"{section_name}_chunk_{i}\"\n",
    "              )\n",
    "              all_chunks.append(chunk)\n",
    "\n",
    "          print(f\"  Created {len(chunks_for_section)} chunks for {section_name}\")\n",
    "\n",
    "      print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
    "      return all_chunks\n",
    "\n",
    "\n",
    "    def _split_into_chunks(self, text: str, chunk_size: int) -> List[str]:\n",
    "        \"\"\"Split text into chunks by paragraphs and size\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            # If adding this paragraph would exceed chunk size and we have content\n",
    "            if len(current_chunk + paragraph) > chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "\n",
    "        # Add the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def generate_embeddings(self, chunks: List[DocumentChunk], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for chunks\"\"\"\n",
    "        texts = [chunk.text for chunk in chunks]\n",
    "\n",
    "        print(f\"Generating embeddings for {len(texts)} chunks...\")\n",
    "\n",
    "        try:\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            print(f\"Embeddings generated! Shape: {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error with batch_size {batch_size}: {e}\")\n",
    "            print(\"Retrying with smaller batch...\")\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                batch_size=8,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            print(f\"Embeddings generated with smaller batch! Shape: {embeddings.shape}\")\n",
    "            return embeddings\n",
    "\n",
    "    def build_system(self, pdf_path: str, chunk_size: int = 800):\n",
    "        \"\"\"Build the complete system\"\"\"\n",
    "        print(\"=== Building Focused Thai Medical System ===\")\n",
    "\n",
    "        # Step 1: Extract focused sections\n",
    "        print(\"\\nStep 1: Extracting target sections...\")\n",
    "        sections = self.extract_focused_sections(pdf_path)\n",
    "\n",
    "        # Show what we extracted\n",
    "        for section_name, content in sections.items():\n",
    "            print(f\"  {section_name}: {len(content)} characters\")\n",
    "\n",
    "        # Step 2: Create chunks\n",
    "        print(\"\\nStep 2: Creating chunks...\")\n",
    "        self.chunks = self.create_focused_chunks(sections, chunk_size)\n",
    "\n",
    "        # Step 3: Generate embeddings\n",
    "        print(\"\\nStep 3: Generating embeddings...\")\n",
    "        self.embeddings = self.generate_embeddings(self.chunks)\n",
    "\n",
    "        print(\"\\n=== System Ready! ===\")\n",
    "        print(f\"Total chunks: {len(self.chunks)}\")\n",
    "\n",
    "        # Show chunk distribution by section\n",
    "        section_counts = {}\n",
    "        for chunk in self.chunks:\n",
    "            section = chunk.metadata['section_name']\n",
    "            section_counts[section] = section_counts.get(section, 0) + 1\n",
    "        print(f\"Chunk distribution: {section_counts}\")\n",
    "\n",
    "    def save_system(self, filepath: str):\n",
    "        \"\"\"Save the system to local files\"\"\"\n",
    "        print(f\"üíæ Saving system to {filepath}...\")\n",
    "\n",
    "        # Prepare data to save\n",
    "        data = {\n",
    "            'chunks': self.chunks,\n",
    "            'embeddings': self.embeddings,\n",
    "            'model_info': {\n",
    "                'model_name': 'BAAI/bge-m3',\n",
    "                'embedding_dim': self.embeddings.shape[1] if self.embeddings is not None else None,\n",
    "                'num_chunks': len(self.chunks)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save with pickle\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "        file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB\n",
    "        print(f\"‚úÖ System saved successfully! File size: {file_size:.2f} MB\")\n",
    "\n",
    "    def load_system(self, filepath: str):\n",
    "        \"\"\"Load the system from local files\"\"\"\n",
    "        print(f\"üìÇ Loading system from {filepath}...\")\n",
    "\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        self.chunks = data['chunks']\n",
    "        self.embeddings = data['embeddings']\n",
    "\n",
    "        print(f\"‚úÖ System loaded successfully!\")\n",
    "        print(f\"  Chunks: {len(self.chunks)}\")\n",
    "        print(f\"  Embeddings shape: {self.embeddings.shape}\")\n",
    "        print(f\"  Model info: {data.get('model_info', 'N/A')}\")\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5, section_filter: str = None) -> List[Dict]:\n",
    "        \"\"\"Search for relevant chunks\"\"\"\n",
    "        if self.embeddings is None or not self.chunks:\n",
    "            raise ValueError(\"System not built or loaded. Please build or load system first.\")\n",
    "\n",
    "        print(f\"üîç Searching: '{query}'\")\n",
    "\n",
    "        # Filter chunks by section if specified\n",
    "        if section_filter:\n",
    "            filtered_indices = [\n",
    "                i for i, chunk in enumerate(self.chunks)\n",
    "                if chunk.metadata['section_name'] == section_filter\n",
    "            ]\n",
    "            filtered_embeddings = self.embeddings[filtered_indices]\n",
    "            filtered_chunks = [self.chunks[i] for i in filtered_indices]\n",
    "            print(f\"  Filtering to section: {section_filter} ({len(filtered_chunks)} chunks)\")\n",
    "        else:\n",
    "            filtered_embeddings = self.embeddings\n",
    "            filtered_chunks = self.chunks\n",
    "            filtered_indices = list(range(len(self.chunks)))\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.model.encode([query])\n",
    "\n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, filtered_embeddings)[0]\n",
    "\n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            chunk = filtered_chunks[idx]\n",
    "            results.append({\n",
    "                'text': chunk.text,\n",
    "                'metadata': chunk.metadata,\n",
    "                'score': float(similarities[idx]),\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'relevance_score': float(similarities[idx])\n",
    "            })\n",
    "\n",
    "        print(f\"  Found {len(results)} results\")\n",
    "        return results\n",
    "\n",
    "#     def answer_question(self, question: str, top_k: int = 3, section_filter: str = None) -> str:\n",
    "#         \"\"\"Answer question based on retrieved context\"\"\"\n",
    "#         # Search for relevant chunks\n",
    "#         results = self.search(question, top_k, section_filter)\n",
    "\n",
    "#         if not results:\n",
    "#             return \"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°\"\n",
    "\n",
    "#         # Combine context\n",
    "#         context_parts = []\n",
    "#         for r in results:\n",
    "#             section_name = r['metadata']['section_name']\n",
    "#             context_parts.append(f\"[{section_name}]: {r['text'][:200]}\")\n",
    "\n",
    "#         context = f\"{'...'*3}\\n\\n\".join(context_parts)\n",
    "\n",
    "#         # Generate answer\n",
    "#         answer = f\"\"\"‡∏ï‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©:\n",
    "\n",
    "# {context}\n",
    "\n",
    "# ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏™‡πà‡∏ß‡∏ô {', '.join(set([r['metadata']['section_name'] for r in results]))}\n",
    "# ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {[f\"{r['score']:.3f}\" for r in results]}\"\"\"\n",
    "\n",
    "#         return answer\n",
    "\n",
    "def create_system(pdf_path: str, save_filename: str = \"medical_system.pkl\",\n",
    "                 chunk_size: int = 800, model_name: str = \"models/bge-m3\"):\n",
    "    \"\"\"Create or load system with custom filename\"\"\"\n",
    "\n",
    "    # Ensure .pkl extension\n",
    "    if not save_filename.endswith('.pkl'):\n",
    "        save_filename += '.pkl'\n",
    "\n",
    "    processor = FocusedThaiMedicalProcessor(model_name)\n",
    "\n",
    "    # Check if saved system exists\n",
    "    if os.path.exists(save_filename):\n",
    "        print(f\"üìÇ Found existing system: {save_filename}\")\n",
    "        processor.load_system(save_filename)\n",
    "        return processor\n",
    "\n",
    "    # Build new system\n",
    "    print(f\"üöÄ Building new system: {save_filename}\")\n",
    "    processor.build_system(pdf_path, chunk_size)\n",
    "\n",
    "    # Auto save\n",
    "    processor.save_system(save_filename)\n",
    "\n",
    "    return processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd590db2",
   "metadata": {},
   "source": [
    "## RAG System Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "GROQ_API_KEY = \"\"\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "class RAGEnhancementSystem:\n",
    "    def __init__(self, rag_system):\n",
    "        \"\"\"\n",
    "        Initialize with existing RAG system\n",
    "        \n",
    "        Args:\n",
    "            rag_system: ‡∏£‡∏∞‡∏ö‡∏ö FocusedThaiMedicalProcessor ‡∏ó‡∏µ‡πà‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏•‡πâ‡∏ß\n",
    "        \"\"\"\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "    def extract_actionable_items(self, analysis_json: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å Actionable_Instructions ‡πÅ‡∏•‡∏∞ Future_plan_add-ons\n",
    "        \n",
    "        Args:\n",
    "            analysis_json: JSON string ‡∏à‡∏≤‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå pipeline ‡∏´‡∏•‡∏±‡∏Å\n",
    "            \n",
    "        Returns:\n",
    "            List of actionable items to query\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = json.loads(analysis_json)\n",
    "            items = []\n",
    "            \n",
    "            # Extract from Actionable_Instructions\n",
    "            actionable = data.get(\"Actionable_Instructions\", {})\n",
    "            \n",
    "            # Next_24-72_hr\n",
    "            next_24_72 = actionable.get(\"Next_24-72_hr\", [])\n",
    "            for item in next_24_72:\n",
    "                items.append({\n",
    "                    \"text\": item,\n",
    "                    \"category\": \"Next_24-72_hr\",\n",
    "                    \"type\": \"actionable\"\n",
    "                })\n",
    "            \n",
    "            # Next_1-2_weeks\n",
    "            next_1_2_weeks = actionable.get(\"Next_1-2_weeks\", [])\n",
    "            for item in next_1_2_weeks:\n",
    "                items.append({\n",
    "                    \"text\": item,\n",
    "                    \"category\": \"Next_1-2_weeks\", \n",
    "                    \"type\": \"actionable\"\n",
    "                })\n",
    "            \n",
    "            # Future_plan_add-ons\n",
    "            future_plans = data.get(\"Future_plan_add-ons\", [])\n",
    "            for item in future_plans:\n",
    "                items.append({\n",
    "                    \"text\": item,\n",
    "                    \"category\": \"Future_plan_add-ons\",\n",
    "                    \"type\": \"future_plan\"\n",
    "                })\n",
    "            \n",
    "            print(f\"üìã ‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ {len(items)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ query:\")\n",
    "            for i, item in enumerate(items, 1):\n",
    "                print(f\"   {i}. [{item['category']}] {item['text'][:60]}...\")\n",
    "            \n",
    "            return items\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"‚ùå Error parsing JSON: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error extracting items: {e}\")\n",
    "            return []\n",
    "\n",
    "    def query_each_item(self, items: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Query ‡πÅ‡∏ï‡πà‡∏•‡∏∞ item ‡πÉ‡∏ô RAG system\n",
    "        \n",
    "        Args:\n",
    "            items: List of items ‡∏à‡∏≤‡∏Å extract_actionable_items\n",
    "            top_k: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô results ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡πÅ‡∏ï‡πà‡∏•‡∏∞ query\n",
    "            \n",
    "        Returns:\n",
    "            List ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£ query ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"\\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á query {len(items)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô RAG system...\")\n",
    "        \n",
    "        for i, item in enumerate(items, 1):\n",
    "            # print(f\"\\n--- Query {i}/{len(items)} ---\")\n",
    "            # print(f\"‡∏´‡∏°‡∏ß‡∏î: {item['category']}\")\n",
    "            # print(f\"‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°: {item['text']}\")\n",
    "            \n",
    "            # Query ‡πÉ‡∏ô RAG system\n",
    "            try:\n",
    "                results = self.rag_system.search(item['text'], top_k=top_k)\n",
    "                \n",
    "                item_result = {\n",
    "                    'original_item': item,\n",
    "                    'query_results': results,\n",
    "                    'query_success': True\n",
    "                }\n",
    "                \n",
    "                # print(f\"‚úÖ ‡∏û‡∏ö {len(results)} ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\")\n",
    "                for j, result in enumerate(results, 1):\n",
    "                    section = self._translate_section_name(result['metadata']['section_name'])\n",
    "                    score = result['score']\n",
    "                    print(f\"   {j}. {section} (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: {score:.4f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error querying: {e}\")\n",
    "                item_result = {\n",
    "                    'original_item': item,\n",
    "                    'query_results': [],\n",
    "                    'query_success': False,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "            \n",
    "            all_results.append(item_result)\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def _translate_section_name(self, section_name: str) -> str:\n",
    "        \"\"\"‡πÅ‡∏õ‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠ section ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\"\"\"\n",
    "        translations = {\n",
    "            'general_knowledge': '‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©',\n",
    "            'investigation_guidelines': '‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©'\n",
    "        }\n",
    "        return translations.get(section_name, section_name)\n",
    "\n",
    "    def analyze_with_llm(self, original_json: str, rag_results: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        ‡πÉ‡∏ä‡πâ LLM ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RAG ‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà\n",
    "        \n",
    "        Args:\n",
    "            original_json: JSON ‡πÄ‡∏î‡∏¥‡∏°‡∏à‡∏≤‡∏Å pipeline\n",
    "            rag_results: ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏à‡∏≤‡∏Å RAG queries\n",
    "            \n",
    "        Returns:\n",
    "            Enhanced analysis with RAG information\n",
    "        \"\"\"\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á context ‡∏à‡∏≤‡∏Å RAG results\n",
    "        rag_context = self._format_rag_context(rag_results)\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á prompt\n",
    "        system_prompt = self._create_analysis_prompt()\n",
    "        user_prompt = self._create_user_prompt(original_json, rag_context)\n",
    "        \n",
    "        print(\"\\nüß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM...\")\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"qwen/qwen3-32b\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=4000,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            result = strip_think(result)\n",
    "\n",
    "            try:\n",
    "                parsed = json.loads(result)\n",
    "                return json.dumps(parsed, ensure_ascii=False, indent=2)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå JSON parse error: {e}\")\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error in LLM analysis: {e}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def _format_rag_context(self, rag_results: List[Dict]) -> str:\n",
    "        \"\"\"‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å RAG ‡πÄ‡∏õ‡πá‡∏ô context ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö LLM\"\"\"\n",
    "        \n",
    "        context_parts = []\n",
    "        \n",
    "        for i, result in enumerate(rag_results, 1):\n",
    "            original_item = result['original_item']\n",
    "            query_results = result.get('query_results', [])\n",
    "            \n",
    "            context_parts.append(f\"\\n=== ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà {i} ===\")\n",
    "            context_parts.append(f\"‡∏´‡∏°‡∏ß‡∏î: {original_item['category']}\")\n",
    "            context_parts.append(f\"‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏î‡∏¥‡∏°: {original_item['text']}\")\n",
    "            \n",
    "            if query_results:\n",
    "                context_parts.append(f\"‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏à‡∏≤‡∏Å‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ({len(query_results)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£):\")\n",
    "                \n",
    "                for j, qr in enumerate(query_results, 1):\n",
    "                    section_thai = self._translate_section_name(qr['metadata']['section_name'])\n",
    "                    score = qr['score']\n",
    "                    text_preview = qr['text'][:300] + \"...\" if len(qr['text']) > 300 else qr['text']\n",
    "                    \n",
    "                    context_parts.append(f\"\\n  {j}. ‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏≤: {section_thai}\")\n",
    "                    context_parts.append(f\"     ‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á: {score:.4f}\")\n",
    "                    context_parts.append(f\"     ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤: {text_preview}\")\n",
    "                    context_parts.append(f\"     ‡∏´‡∏ô‡πâ‡∏≤: {qr['metadata'].get('page_start','?')}-{qr['metadata'].get('page_end','?')}\")\n",
    "            else:\n",
    "                context_parts.append(\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "\n",
    "    def _create_analysis_prompt(self) -> str:\n",
    "        \"\"\"Create system prompt for guideline-based analysis\"\"\"\n",
    "\n",
    "        return \"\"\"You are an expert in foodborne disease outbreak investigation and control.\n",
    "    You are tasked with refining preliminary recommendations using the official guideline document as a reference.\n",
    "\n",
    "    Your responsibilities:\n",
    "    1. Analyze whether information from the guideline is relevant to improve the recommendation.\n",
    "    2. If relevant, enhance the recommendation to make it more accurate, complete, and aligned with the guideline.\n",
    "    3. If not relevant, keep the original recommendation unchanged.\n",
    "    4. Clearly specify whether RAG information was used or not.\n",
    "\n",
    "    Evaluation criteria:\n",
    "    - Consistency with guideline content\n",
    "    - Scientific accuracy\n",
    "    - Practical feasibility\n",
    "    - Completeness of recommendations\n",
    "\n",
    "    Response format:\n",
    "    Return the output strictly in JSON format. \n",
    "    For each recommendation:\n",
    "    - Always include: Original, Enhanced, and Use_RAG.\n",
    "    - If Use_RAG = true ‚Üí also include Guideline_Reference, Page_Number, and Relevance_Score (decimal with 4 digits).\n",
    "    - If Use_RAG = false ‚Üí do not include those fields.\"\"\"\n",
    "\n",
    "\n",
    "    def _create_user_prompt(self, original_json: str, rag_context: str) -> str:\n",
    "        \"\"\"Create user prompt for guideline-based recommendation refinement\"\"\"\n",
    "\n",
    "        return f\"\"\"\n",
    "    Original Analysis JSON:\n",
    "    {original_json}\n",
    "\n",
    "    Supporting context from guideline document:\n",
    "    {rag_context}\n",
    "\n",
    "    Please analyze and return the results strictly as a valid JSON object in the following structure:\n",
    "\n",
    "    {{\n",
    "    \"Analysis_Summary\": {{\n",
    "        \"Total_Items_Analyzed\": <int>,\n",
    "        \"Items_Enhanced_With_Guidelines\": <int>,\n",
    "        \"Items_Kept_Original\": <int>\n",
    "    }},\n",
    "    \"Enhanced_Recommendations\": {{\n",
    "        \"Next_24-72_hr\": [\n",
    "        {{\n",
    "            \"Original\": \"<original recommendation>\",\n",
    "            \"Enhanced\": \"<enhanced recommendation>\",\n",
    "            \"Use_RAG\": true,\n",
    "            \"Guideline_Reference\": \"<specific section or paragraph reference>\",\n",
    "            \"Page_Number\": \"<page number>\",\n",
    "            \"Relevance_Score\": \"<decimal with 4 digits>\"\n",
    "        }},\n",
    "        {{\n",
    "            \"Original\": \"<original recommendation>\",\n",
    "            \"Enhanced\": \"<enhanced recommendation>\",\n",
    "            \"Use_RAG\": false\n",
    "        }}\n",
    "        ],\n",
    "        \"Next_1-2_weeks\": [\n",
    "        {{\n",
    "            \"Original\": \"<original recommendation>\",\n",
    "            \"Enhanced\": \"<enhanced recommendation>\",\n",
    "            \"Use_RAG\": true,\n",
    "            \"Guideline_Reference\": \"<specific section or paragraph reference>\",\n",
    "            \"Page_Number\": \"<page number>\",\n",
    "            \"Relevance_Score\": \"<decimal with 4 digits>\"\n",
    "        }}\n",
    "        ],\n",
    "        \"Future_plan_add-ons\": [\n",
    "        {{\n",
    "            \"Original\": \"<original recommendation>\",\n",
    "            \"Enhanced\": \"<enhanced recommendation>\",\n",
    "            \"Use_RAG\": false\n",
    "        }}\n",
    "        ]\n",
    "    }},\n",
    "    \"Enhancement_Notes\": [\n",
    "        \"<note about how the recommendation was enhanced or why it was unchanged>\"\n",
    "    ]\n",
    "    }}\n",
    "\n",
    "    Notes:\n",
    "    - All recommendations must remain in Thai, but all JSON keys and structure must be in English.\n",
    "    - If Use_RAG = false, copy Original into Enhanced.\n",
    "    - Output must be pure JSON only. Do not include <think>, comments, or markdown fences.\n",
    "    \"\"\"\n",
    "\n",
    "    def run_enhancement(self, analysis_json: str, top_k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        ‡∏£‡∏±‡∏ô‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ enhancement ‡πÅ‡∏ö‡∏ö‡πÄ‡∏ï‡πá‡∏° ‡πÅ‡∏•‡πâ‡∏ß '‡∏£‡∏ß‡∏°‡∏ú‡∏•' ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢\n",
    "        \"\"\"\n",
    "        print(\"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG Enhancement\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Step 1: Extract\n",
    "        items = self.extract_actionable_items(analysis_json)\n",
    "        if not items:\n",
    "            return \"Error: ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÅ‡∏¢‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å JSON ‡πÑ‡∏î‡πâ\"\n",
    "\n",
    "        # Step 2: Query RAG\n",
    "        print(f\"\\nüìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á query {len(items)} ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô knowledge base...\")\n",
    "        rag_results = self.query_each_item(items, top_k=top_k)\n",
    "\n",
    "        # Step 3: LLM Analysis\n",
    "        print(f\"\\nüîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\")\n",
    "        enhanced_result = self.analyze_with_llm(analysis_json, rag_results)\n",
    "\n",
    "        # Step 4: Merge ‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö original\n",
    "        print(f\"\\nüß© ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå RAG ‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏î‡∏¥‡∏°...\")\n",
    "        merged_final = merge_enhancements_with_original(analysis_json, enhanced_result)\n",
    "\n",
    "        print(f\"\\n‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG Enhancement ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\")\n",
    "        return merged_final\n",
    "\n",
    "def strip_think(text: str) -> str:\n",
    "    \"\"\"Remove <think> tags and markdown code fences from text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    cleaned = re.sub(r\"```(?:json)?\", \"\", cleaned)  # remove ``` or ```json\n",
    "    return cleaned.strip()\n",
    "\n",
    "def _sanitize_item(it: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Enforce rules:\n",
    "      - If Use_RAG == False: Enhanced must equal Original, and remove guideline fields.\n",
    "      - If Use_RAG == True: ensure required fields exist and normalize Relevance_Score to 4 decimals.\n",
    "    \"\"\"\n",
    "    it = dict(it)  # shallow copy\n",
    "    original = it.get(\"Original\", \"\")\n",
    "    use_rag = bool(it.get(\"Use_RAG\", False))\n",
    "\n",
    "    it[\"Original\"] = original\n",
    "\n",
    "    if not use_rag:\n",
    "        it[\"Use_RAG\"] = False\n",
    "        it[\"Enhanced\"] = original\n",
    "        # drop guideline-only fields if present\n",
    "        for k in (\"Guideline_Reference\", \"Page_Number\", \"Relevance_Score\"):\n",
    "            it.pop(k, None)\n",
    "    else:\n",
    "        it[\"Use_RAG\"] = True\n",
    "        # ensure Enhanced exists\n",
    "        it[\"Enhanced\"] = it.get(\"Enhanced\", original)\n",
    "        # normalize Relevance_Score\n",
    "        rs = it.get(\"Relevance_Score\", None)\n",
    "        if rs is not None and rs != \"\":\n",
    "            try:\n",
    "                it[\"Relevance_Score\"] = f\"{float(rs):.4f}\"\n",
    "            except Exception:\n",
    "                # keep as-is if cannot coerce\n",
    "                pass\n",
    "        # coerce Page_Number to string if present\n",
    "        if \"Page_Number\" in it and it[\"Page_Number\"] is not None:\n",
    "            it[\"Page_Number\"] = str(it[\"Page_Number\"])\n",
    "\n",
    "    return it\n",
    "\n",
    "def _sanitize_llm_output(llm: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Sanitize LLM JSON in-place: enforce the Use_RAG rules across all buckets.\n",
    "    \"\"\"\n",
    "    llm = dict(llm)\n",
    "    enh = dict(llm.get(\"Enhanced_Recommendations\", {}))\n",
    "    for key in (\"Next_24-72_hr\", \"Next_1-2_weeks\", \"Future_plan_add-ons\"):\n",
    "        arr = enh.get(key) or []\n",
    "        new_arr = []\n",
    "        for it in arr:\n",
    "            if isinstance(it, dict):\n",
    "                new_arr.append(_sanitize_item(it))\n",
    "            else:\n",
    "                # if LLM returned raw strings unexpectedly, wrap them\n",
    "                new_arr.append({\"Original\": it, \"Enhanced\": it, \"Use_RAG\": False})\n",
    "        enh[key] = new_arr\n",
    "    llm[\"Enhanced_Recommendations\"] = enh\n",
    "    return llm\n",
    "\n",
    "def _index_by_original(items):\n",
    "    \"\"\"Build dict: original_text -> sanitized item_dict\"\"\"\n",
    "    idx = {}\n",
    "    for it in items or []:\n",
    "        if not isinstance(it, dict):\n",
    "            # tolerate stray strings\n",
    "            it = {\"Original\": str(it), \"Enhanced\": str(it), \"Use_RAG\": False}\n",
    "        it = _sanitize_item(it)  # <-- enforce rules here\n",
    "        orig = it.get(\"Original\", \"\")\n",
    "        if orig:\n",
    "            idx[orig] = it\n",
    "    return idx\n",
    "\n",
    "def _merge_bucket(orig_list, idx_enh):\n",
    "    \"\"\"\n",
    "    Merge a bucket with enforcement:\n",
    "      - Match by Original text.\n",
    "      - If found in idx_enh: use the sanitized item.\n",
    "      - If not found: produce {Original, Enhanced=Original, Use_RAG=False}.\n",
    "      - Preserve original order.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    for entry in orig_list:\n",
    "        if isinstance(entry, dict):\n",
    "            original_text = entry.get(\"Original\") or entry.get(\"Enhanced\") or \"\"\n",
    "            if original_text in idx_enh:\n",
    "                merged.append(_sanitize_item(idx_enh[original_text]))  # ensure sanitized\n",
    "            else:\n",
    "                base = {\n",
    "                    \"Original\": original_text or entry,\n",
    "                    \"Enhanced\": entry.get(\"Enhanced\", original_text or entry),\n",
    "                    \"Use_RAG\": bool(entry.get(\"Use_RAG\", False)),\n",
    "                }\n",
    "                # final sanitize\n",
    "                merged.append(_sanitize_item(base))\n",
    "        else:\n",
    "            # entry is string\n",
    "            if entry in idx_enh:\n",
    "                merged.append(_sanitize_item(idx_enh[entry]))\n",
    "            else:\n",
    "                merged.append({\n",
    "                    \"Original\": entry,\n",
    "                    \"Enhanced\": entry,  # enforce Use_RAG=False => same as original\n",
    "                    \"Use_RAG\": False\n",
    "                })\n",
    "    return merged\n",
    "\n",
    "def merge_enhancements_with_original(original_json_str: str, llm_json_str: str) -> str:\n",
    "    \"\"\"\n",
    "    - parse both sides\n",
    "    - sanitize LLM output\n",
    "    - index and merge\n",
    "    - attach Enhancement_Notes (if any)\n",
    "    \"\"\"\n",
    "    # parse original\n",
    "    try:\n",
    "        original = json.loads(original_json_str)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Cannot parse original_json: {e}\"}, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # parse LLM (strip <think> / fences first)\n",
    "    llm_clean = strip_think(llm_json_str)\n",
    "    try:\n",
    "        llm_raw = json.loads(llm_clean)\n",
    "    except Exception as e:\n",
    "        final_out = dict(original)\n",
    "        final_out[\"Enhancement_Notes\"] = [\"LLM output could not be parsed as JSON.\", str(e)]\n",
    "        return json.dumps(final_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # --- NEW: sanitize the whole LLM block first ---\n",
    "    llm = _sanitize_llm_output(llm_raw)\n",
    "\n",
    "    # pull enhanced buckets\n",
    "    enh = llm.get(\"Enhanced_Recommendations\", {})\n",
    "    e_24_72 = enh.get(\"Next_24-72_hr\") or []\n",
    "    e_1_2w  = enh.get(\"Next_1-2_weeks\") or []\n",
    "    e_future = enh.get(\"Future_plan_add-ons\") or []\n",
    "\n",
    "    # build indexes\n",
    "    idx_24_72 = _index_by_original(e_24_72)\n",
    "    idx_1_2w  = _index_by_original(e_1_2w)\n",
    "    idx_future = _index_by_original(e_future)\n",
    "\n",
    "    # merge Actionable_Instructions\n",
    "    actionable = original.get(\"Actionable_Instructions\", {})\n",
    "    orig_24_72 = actionable.get(\"Next_24-72_hr\", [])\n",
    "    orig_1_2w  = actionable.get(\"Next_1-2_weeks\", [])\n",
    "    actionable[\"Next_24-72_hr\"] = _merge_bucket(orig_24_72, idx_24_72)\n",
    "    actionable[\"Next_1-2_weeks\"] = _merge_bucket(orig_1_2w, idx_1_2w)\n",
    "    original[\"Actionable_Instructions\"] = actionable\n",
    "\n",
    "    # merge Future_plan-add-ons\n",
    "    orig_future = original.get(\"Future_plan_add-ons\", [])\n",
    "    original[\"Future_plan_add-ons\"] = _merge_bucket(orig_future, idx_future)\n",
    "\n",
    "    # attach Enhancement_Notes at root\n",
    "    notes = llm.get(\"Enhancement_Notes\") or []\n",
    "    if notes:\n",
    "        original[\"Enhancement_Notes\"] = notes\n",
    "\n",
    "    return json.dumps(original, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ==================== ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏™‡∏≠‡∏ö ====================\n",
    "\n",
    "def test_rag_enhancement():\n",
    "    \"\"\"‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏£‡∏∞‡∏ö‡∏ö RAG Enhancement\"\"\"\n",
    "    \n",
    "    # ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á JSON ‡∏à‡∏≤‡∏Å pipeline ‡∏´‡∏•‡∏±‡∏Å\n",
    "    sample_analysis = '''{\n",
    "    \"Actions_Adequacy\": {\n",
    "        \"Adequacy\": false,\n",
    "        \"Recommendations\": [\n",
    "        \"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î\",\n",
    "        \"‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡∏ö‡∏£‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞\",\n",
    "        \"‡∏™‡∏±‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏™‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≥‡∏´‡∏ô‡πà‡∏≤‡∏¢‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏™‡∏∏‡∏Ç\"\n",
    "        ]\n",
    "    },\n",
    "    \"Actionable_Instructions\": {\n",
    "        \"Next_24-72_hr\": [\n",
    "        \"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡∏≠‡∏∏‡∏õ‡πÇ‡∏†‡∏Ñ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢\",\n",
    "        \"‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£\",\n",
    "        \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°\"\n",
    "        ],\n",
    "        \"Next_1-2_weeks\": [\n",
    "        \"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\",\n",
    "        \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\",\n",
    "        \"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÇ‡∏£‡∏Ñ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà\"\n",
    "        ]\n",
    "    },\n",
    "    \"Flaws_Gaps\": [\n",
    "        \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏±‡∏Å‡∏Å‡∏±‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡πÅ‡∏û‡∏£‡πà‡πÄ‡∏ä‡∏∑‡πâ‡∏≠\",\n",
    "        \"‡∏Ç‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\",\n",
    "        \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÉ‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\"\n",
    "    ],\n",
    "    \"Future_plan_add-ons\": [\n",
    "        \"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å\",\n",
    "        \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏ä‡∏∏‡∏°‡∏ä‡∏ô\",\n",
    "        \"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÇ‡∏Ñ‡∏•‡∏¥‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÅ‡∏ö‡∏Ñ‡∏ó‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß\"\n",
    "    ],\n",
    "    \"Rationale\": [\n",
    "        \"‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏≠‡∏≤‡∏à‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏π‡πâ‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∏‡∏°‡∏ä‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á\",\n",
    "        \"‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏ß‡∏á‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤\",\n",
    "        \"‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡πÉ‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏ô‡πÄ‡∏õ‡∏∑‡πâ‡∏≠‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï\"\n",
    "    ],\n",
    "    \"Response_Time\": \"65.71 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\",\n",
    "    \"Model_Used\": \"qwen/qwen3-32b\"\n",
    "    }'''\n",
    "    \n",
    "    print(\"üß™ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Enhancement System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ‡πÇ‡∏´‡∏•‡∏î RAG system (‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå medical_system.pkl ‡∏≠‡∏¢‡∏π‡πà‡πÅ‡∏•‡πâ‡∏ß)\n",
    "    try:\n",
    "        rag_system = create_system(\"Docs/Guideline.pdf\", \"my_medical_ragv2.pkl\")\n",
    "        \n",
    "        # ‡∏™‡∏£‡πâ‡∏≤‡∏á enhancement system\n",
    "        enhancer = RAGEnhancementSystem(rag_system)\n",
    "        \n",
    "        # ‡∏£‡∏±‡∏ô‡∏Å‡∏≤‡∏£ enhancement\n",
    "        result = enhancer.run_enhancement(sample_analysis, top_k=1)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£ Enhancement:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(result)\n",
    "        \n",
    "        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå\n",
    "        # with open(\"enhanced_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        #     f.write(result)\n",
    "        # print(f\"\\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà enhanced_analysis.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in testing: {e}\")\n",
    "        print(\"‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå medical_system.pkl ‡πÅ‡∏•‡∏∞ import ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a347c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏î‡∏™‡∏≠‡∏ö RAG Enhancement System\n",
      "============================================================\n",
      "BGE-M3 model loaded successfully!\n",
      "üìÇ Found existing system: my_medical_ragv2.pkl\n",
      "üìÇ Loading system from my_medical_ragv2.pkl...\n",
      "‚úÖ System loaded successfully!\n",
      "  Chunks: 69\n",
      "  Embeddings shape: (69, 1024)\n",
      "  Model info: {'model_name': 'BAAI/bge-m3', 'embedding_dim': 1024, 'num_chunks': 69}\n",
      "üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG Enhancement\n",
      "============================================================\n",
      "üìã ‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ 9 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ query:\n",
      "   1. [Next_24-72_hr] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡∏≠‡∏∏‡∏õ‡πÇ‡∏†‡∏Ñ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÉ‡∏´‡πâ‡∏™...\n",
      "   2. [Next_24-72_hr] ‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠...\n",
      "   3. [Next_24-72_hr] ‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ...\n",
      "   4. [Next_1-2_weeks] ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á...\n",
      "   5. [Next_1-2_weeks] ‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏á...\n",
      "   6. [Next_1-2_weeks] ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÇ‡∏£‡∏Ñ‡∏ï‡∏≤...\n",
      "   7. [Future_plan_add-ons] ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞...\n",
      "   8. [Future_plan_add-ons] ‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏ä‡∏∏‡∏°‡∏ä‡∏ô...\n",
      "   9. [Future_plan_add-ons] ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÇ‡∏Ñ‡∏•‡∏¥‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÅ‡∏ö‡∏Ñ‡∏ó‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ...\n",
      "\n",
      "üìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á query 9 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô knowledge base...\n",
      "\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á query 9 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô RAG system...\n",
      "üîç Searching: '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡∏≠‡∏∏‡∏õ‡πÇ‡∏†‡∏Ñ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5311)\n",
      "üîç Searching: '‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.6529)\n",
      "üîç Searching: '‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5264)\n",
      "üîç Searching: '‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5473)\n",
      "üîç Searching: '‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.6796)\n",
      "üîç Searching: '‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÇ‡∏£‡∏Ñ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5823)\n",
      "üîç Searching: '‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.6212)\n",
      "üîç Searching: '‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏ä‡∏∏‡∏°‡∏ä‡∏ô'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5611)\n",
      "üîç Searching: '‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÇ‡∏Ñ‡∏•‡∏¥‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÅ‡∏ö‡∏Ñ‡∏ó‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5788)\n",
      "\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\n",
      "\n",
      "üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM...\n",
      "\n",
      "üß© ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå RAG ‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏î‡∏¥‡∏°...\n",
      "\n",
      "‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG Enhancement ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "\n",
      "============================================================\n",
      "üìä ‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏Å‡∏≤‡∏£ Enhancement:\n",
      "============================================================\n",
      "{\n",
      "  \"Actions_Adequacy\": {\n",
      "    \"Adequacy\": false,\n",
      "    \"Recommendations\": [\n",
      "      \"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏Ç‡πâ‡∏°‡∏á‡∏ß‡∏î\",\n",
      "      \"‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÇ‡∏î‡∏¢‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏≠‡∏ö‡∏£‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞\",\n",
      "      \"‡∏™‡∏±‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏™‡πà‡∏á‡∏´‡∏£‡∏∑‡∏≠‡∏à‡∏≥‡∏´‡∏ô‡πà‡∏≤‡∏¢‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏™‡∏∏‡∏Ç\"\n",
      "    ]\n",
      "  },\n",
      "  \"Actionable_Instructions\": {\n",
      "    \"Next_24-72_hr\": [\n",
      "      {\n",
      "        \"Original\": \"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡∏≠‡∏∏‡∏õ‡πÇ‡∏†‡∏Ñ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢\",\n",
      "        \"Enhanced\": \"‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡∏≠‡∏∏‡∏õ‡πÇ‡∏†‡∏Ñ‡∏ö‡∏£‡∏¥‡πÇ‡∏†‡∏Ñ‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (Free Chlorine Test Kit) ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÉ‡∏´‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡πà‡∏ß‡∏á 0.2-0.5 mg/L ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"3) ‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏ô‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (Free Chlorine Test Kit)\",\n",
      "        \"Page_Number\": \"61-61\",\n",
      "        \"Relevance_Score\": \"0.5311\"\n",
      "      },\n",
      "      {\n",
      "        \"Original\": \"‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£\",\n",
      "        \"Enhanced\": \"‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏ô‡πâ‡∏≥‡πÉ‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏¥‡∏î‡πÄ‡∏´‡∏ï‡∏∏‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡πà‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏´‡∏≤‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏Å‡πà‡∏≠‡πÇ‡∏£‡∏Ñ‡∏£‡∏∞‡∏ö‡∏ö‡∏ó‡∏≤‡∏á‡πÄ‡∏î‡∏¥‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÉ‡∏ô‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 7 ‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£ ‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á‡∏™‡πà‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\",\n",
      "        \"Page_Number\": \"52-52\",\n",
      "        \"Relevance_Score\": \"0.6529\"\n",
      "      },\n",
      "      {\n",
      "        \"Original\": \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°\",\n",
      "        \"Enhanced\": \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°\",\n",
      "        \"Use_RAG\": false\n",
      "      }\n",
      "    ],\n",
      "    \"Next_1-2_weeks\": [\n",
      "      {\n",
      "        \"Original\": \"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î\",\n",
      "        \"Enhanced\": \"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏Ç‡∏≠‡∏á‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡∏•‡πÑ‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÉ‡∏ô‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏ô‡∏±‡∏ö‡∏™‡∏ô‡∏∏‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î\",\n",
      "        \"Page_Number\": \"62-62\",\n",
      "        \"Relevance_Score\": \"0.5473\"\n",
      "      },\n",
      "      {\n",
      "        \"Original\": \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á\",\n",
      "        \"Enhanced\": \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô (Preliminary Report) ‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 9 ‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡∏∞‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ\",\n",
      "        \"Page_Number\": \"64-64\",\n",
      "        \"Relevance_Score\": \"0.6796\"\n",
      "      },\n",
      "      {\n",
      "        \"Original\": \"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÇ‡∏£‡∏Ñ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà\",\n",
      "        \"Enhanced\": \"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏≠‡∏á‡∏ó‡∏≤‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÇ‡∏£‡∏Ñ‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏ú‡∏ô‡∏ï‡∏≤‡∏°‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà\",\n",
      "        \"Page_Number\": \"64-64\",\n",
      "        \"Relevance_Score\": \"0.5823\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"Flaws_Gaps\": [\n",
      "    \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏±‡∏Å‡∏Å‡∏±‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏´‡∏•‡πà‡∏á‡πÅ‡∏û‡∏£‡πà‡πÄ‡∏ä‡∏∑‡πâ‡∏≠\",\n",
      "    \"‡∏Ç‡∏≤‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢) ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö\",\n",
      "    \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡πÉ‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\"\n",
      "  ],\n",
      "  \"Future_plan_add-ons\": [\n",
      "    {\n",
      "      \"Original\": \"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å\",\n",
      "      \"Enhanced\": \"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏≤‡∏á‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î\",\n",
      "      \"Use_RAG\": true,\n",
      "      \"Guideline_Reference\": \"‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏à‡∏≤‡∏Å‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏≤‡∏á‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏\",\n",
      "      \"Page_Number\": \"36-36\",\n",
      "      \"Relevance_Score\": \"0.6212\"\n",
      "    },\n",
      "    {\n",
      "      \"Original\": \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏ä‡∏∏‡∏°‡∏ä‡∏ô\",\n",
      "      \"Enhanced\": \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏û‡∏¥‡∏ò‡∏µ‡∏Å‡∏£‡∏£‡∏°‡πÉ‡∏ô‡∏ä‡∏∏‡∏°‡∏ä‡∏ô\",\n",
      "      \"Use_RAG\": false\n",
      "    },\n",
      "    {\n",
      "      \"Original\": \"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÇ‡∏Ñ‡∏•‡∏¥‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÅ‡∏ö‡∏Ñ‡∏ó‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß\",\n",
      "      \"Enhanced\": \"‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÇ‡∏Ñ‡∏•‡∏¥‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÅ‡∏ö‡∏Ñ‡∏ó‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏Ñ‡∏™‡∏ô‡∏≤‡∏° (Test Kits) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß\",\n",
      "      \"Use_RAG\": true,\n",
      "      \"Guideline_Reference\": \"‡∏Å‡∏≤‡∏£ Swab ‡∏°‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏ä‡∏ô‡∏∞‡∏î‡πâ‡∏ß‡∏¢‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏Ç‡∏≠‡∏á‡∏†‡∏≤‡∏ä‡∏ô‡∏∞‡πÅ‡∏•‡∏∞‡∏°‡∏∑‡∏≠\",\n",
      "      \"Page_Number\": \"60-61\",\n",
      "      \"Relevance_Score\": \"0.5788\"\n",
      "    }\n",
      "  ],\n",
      "  \"Rationale\": [\n",
      "    \"‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏õ‡∏¥‡∏î‡πÅ‡∏´‡∏•‡πà‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏≠‡∏≤‡∏à‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏π‡πâ‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏≠‡∏∑‡πà‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∏‡∏°‡∏ä‡∏ô‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á\",\n",
      "    \"‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏ß‡∏á‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡πÑ‡∏î‡πâ‡∏ó‡∏±‡∏ô‡πÄ‡∏ß‡∏•‡∏≤\",\n",
      "    \"‡∏Å‡∏≤‡∏£‡πÑ‡∏°‡πà‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏ä‡πâ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡πÉ‡∏ô‡∏ó‡∏±‡∏ô‡∏ó‡∏µ‡∏≠‡∏≤‡∏à‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡∏¥‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏ô‡πÄ‡∏õ‡∏∑‡πâ‡∏≠‡∏ô‡∏ã‡πâ‡∏≥‡πÉ‡∏ô‡∏≠‡∏ô‡∏≤‡∏Ñ‡∏ï\"\n",
      "  ],\n",
      "  \"Response_Time\": \"65.71 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\",\n",
      "  \"Model_Used\": \"qwen/qwen3-32b\",\n",
      "  \"Enhancement_Notes\": [\n",
      "    \"‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ï‡∏≤‡∏°‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á\",\n",
      "    \"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏™‡∏¥‡πà‡∏á‡∏™‡πà‡∏á‡∏ï‡∏£‡∏ß‡∏à‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\",\n",
      "    \"‡∏£‡∏∞‡∏ö‡∏∏‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ó‡∏µ‡πà‡∏¢‡∏≠‡∏°‡∏£‡∏±‡∏ö‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢\",\n",
      "    \"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏≤‡∏á‡∏Ñ‡∏•‡∏¥‡∏ô‡∏¥‡∏Å‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î\",\n",
      "    \"‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏•‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏•‡πÑ‡∏Å‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°\",\n",
      "    \"‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô‡πÅ‡∏•‡∏∞‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏™‡∏£‡∏∏‡∏õ‡∏ï‡∏≤‡∏°‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÉ‡∏ô‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£\",\n",
      "    \"‡∏£‡∏∞‡∏ö‡∏∏‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏ä‡∏∏‡∏î‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏Ñ‡∏™‡∏ô‡∏≤‡∏°‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÇ‡∏Ñ‡∏•‡∏¥‡∏ü‡∏≠‡∏£‡πå‡∏°‡πÅ‡∏ö‡∏Ñ‡∏ó‡∏µ‡πÄ‡∏£‡∏µ‡∏¢‡∏ï‡∏≤‡∏°‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_rag_enhancement()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae16356",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "from groq import Groq\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "# MODEL_NAME = \"qwen/qwen3-32b\"\n",
    "MODEL_NAME_QWEN = \"qwen/qwen3-32b\"\n",
    "MODEL_NAME_LLAMA = \"llama-3.1-8b-instant\"\n",
    "LLAMA_SHORT_NAME = \"llama3.1-8b\"\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = \"\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"Please set GROQ_API_KEY environment variable\")\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# ==================== PDF EXTRACTION ====================\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF file with basic cleanup.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\\n\".join(p.extract_text() or \"\" for p in reader.pages)\n",
    "    # Basic cleanup\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "# ==================== SECTION PARSING ====================\n",
    "SECTION_KEYWORDS = (\n",
    "    \"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤\", \"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô\", \"‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß\",\n",
    "    \"‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ\", \"‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞\", \"‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤\",\n",
    "    \"‡∏•‡∏á‡∏ä‡∏∑‡πà‡∏≠\", \"‡∏ó‡∏µ‡∏°‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô\"\n",
    ")\n",
    "\n",
    "BULLET_START = re.compile(r'^\\s*(?:\\d+(?:\\.\\d+)*[.)]|[‚Ä¢\\-‚Äì‚Äî])\\s+')\n",
    "SENT_END = re.compile(r'[.!?‚Ä¶]|[)\\]]|‚Äù|‚Äô|\"|\\'|‡∏ô\\.$')\n",
    "\n",
    "CONTINUATION_START = re.compile(\n",
    "    r'^\\s*(?:\\d+[^\\d\\s]|‡πÅ‡∏•‡∏∞|‡∏´‡∏£‡∏∑‡∏≠|‡πÇ‡∏î‡∏¢|‡πÅ‡∏ï‡πà|‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á|‡∏ó‡∏±‡πâ‡∏á‡∏ô‡∏µ‡πâ|‡∏ã‡∏∂‡πà‡∏á|‡∏ó‡∏µ‡πà|‡∏à‡∏≤‡∏Å|‡πÉ‡∏ô|‡∏Ç‡∏≠‡∏á|‡∏ï‡πà‡∏≠‡∏°‡∏≤|‡∏≠‡∏µ‡∏Å‡∏ó‡∏±‡πâ‡∏á|‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á)\\b'\n",
    ")\n",
    "\n",
    "HEADER_FULLLINE = [\n",
    "    re.compile(rf'^\\s*{re.escape(k)}\\s*$', flags=re.I) \n",
    "    for k in SECTION_KEYWORDS\n",
    "]\n",
    "\n",
    "def is_header(text: str) -> bool:\n",
    "    \"\"\"Check if text is a section header.\"\"\"\n",
    "    s = text.strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    \n",
    "    # Exact-line header match\n",
    "    if any(pat.match(s) for pat in HEADER_FULLLINE):\n",
    "        return True\n",
    "    \n",
    "    # Heuristic header shape\n",
    "    if len(s) <= 40 and not SENT_END.search(s) and not BULLET_START.match(s):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def split_report(text: str) -> List[str]:\n",
    "    \"\"\"Split report into paragraphs.\"\"\"\n",
    "    # Normalize\n",
    "    t = re.sub(r'\\r\\n?', '\\n', text)\n",
    "    t = re.sub(r'[ \\t]+$', '', t, flags=re.M)\n",
    "    t = t.replace('\\f', '')\n",
    "    \n",
    "    # Mark headers\n",
    "    def mark_headers(tt):\n",
    "        for k in SECTION_KEYWORDS:\n",
    "            tt = re.sub(rf'(?m)^\\s*({re.escape(k)})(?:\\s*)$', r'\\n\\1\\n', tt)\n",
    "        return tt\n",
    "    \n",
    "    t = mark_headers(t)\n",
    "    raw = [p.strip() for p in re.split(r'\\n\\s*\\n+', t) if p.strip()]\n",
    "    \n",
    "    # Fix paragraph merging\n",
    "    fixed = []\n",
    "    for cur in raw:\n",
    "        if not fixed:\n",
    "            fixed.append(cur)\n",
    "            continue\n",
    "        \n",
    "        prev = fixed[-1]\n",
    "        \n",
    "        if is_header(cur):\n",
    "            fixed.append(cur)\n",
    "            continue\n",
    "        \n",
    "        lines = cur.splitlines()\n",
    "        all_bullets = lines and all(BULLET_START.match(x) or not x.strip() for x in lines)\n",
    "        if all_bullets:\n",
    "            fixed.append(cur)\n",
    "            continue\n",
    "        \n",
    "        # Merge criteria\n",
    "        short = len(cur) < 80\n",
    "        prev_not_end = not SENT_END.search(prev.splitlines()[-1])\n",
    "        contish = CONTINUATION_START.match(cur) or re.match(r'^\\s*\\d+([^\\d]|\\s|$)', cur)\n",
    "        single_line = len(lines) == 1\n",
    "        \n",
    "        if (short and single_line and (prev_not_end or contish)):\n",
    "            fixed[-1] = (prev.rstrip() + ' ' + cur.lstrip()).strip()\n",
    "        else:\n",
    "            fixed.append(cur)\n",
    "    \n",
    "    # Fix hard wraps within paragraphs\n",
    "    final = []\n",
    "    for para in fixed:\n",
    "        ls = [x.strip() for x in para.splitlines() if x.strip()]\n",
    "        if not ls:\n",
    "            continue\n",
    "        out = []\n",
    "        for line in ls:\n",
    "            if not out:\n",
    "                out.append(line)\n",
    "                continue\n",
    "            if BULLET_START.match(line) or BULLET_START.match(out[-1]):\n",
    "                out.append(line)\n",
    "            else:\n",
    "                out[-1] = (out[-1].rstrip() + ' ' + line.lstrip()).strip()\n",
    "        final.append('\\n'.join(out).strip())\n",
    "    \n",
    "    return final\n",
    "\n",
    "def extract_sections(paras: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Extract sections from paragraphs.\"\"\"\n",
    "    sections = {}\n",
    "    i = 0\n",
    "    while i < len(paras):\n",
    "        if is_header(paras[i]):\n",
    "            head = paras[i].strip()\n",
    "            i += 1\n",
    "            buf = []\n",
    "            while i < len(paras) and not is_header(paras[i]):\n",
    "                buf.append(paras[i].strip())\n",
    "                i += 1\n",
    "            sections[head] = \"\\n\\n\".join(buf).strip()\n",
    "        else:\n",
    "            i += 1\n",
    "    return sections\n",
    "\n",
    "ALIAS = {\n",
    "    \"situation\": [\"‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤\", \"‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå\", \"background\", \"situation\"],\n",
    "    \"findings\": [\"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô\", \"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö\", \"‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏®‡∏∂‡∏Å‡∏©‡∏≤\", \"findings\"],\n",
    "    \"actions_done\": [\"‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß\", \"‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß\", \"actions taken\", \"actions done\"],\n",
    "    \"actions_next\": [\"‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ\", \"‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£\", \"next steps\", \"next actions\"],\n",
    "}\n",
    "\n",
    "def pick_section(sections: Dict[str, str], key: str) -> str:\n",
    "    \"\"\"Pick section text by key alias.\"\"\"\n",
    "    targets = ALIAS.get(key, [])\n",
    "    for h, txt in sections.items():\n",
    "        for t in targets:\n",
    "            if re.search(re.escape(t), h, flags=re.I):\n",
    "                return txt\n",
    "    return \"\"\n",
    "\n",
    "# ==================== SUMMARIZATION ====================\n",
    "def strip_think(text: str) -> str:\n",
    "    \"\"\"Remove <think> tags from text.\"\"\"\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "def run_groq(system_prompt: str, user_prompt: str, model_name: str, max_tokens: int = 2000) -> str:\n",
    "    \"\"\"Run Groq API call with error handling and model selection.\"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            max_tokens=max_tokens,\n",
    "            timeout=60\n",
    "        )\n",
    "        return strip_think(resp.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Groq API Error with {model_name}: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Summarization prompts\n",
    "SITUATION_SYSTEM = (\n",
    "    \"You are an assistant that summarizes outbreak investigation reports in Thai.\\n\"\n",
    "    \"Task: Summarize the '‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤' (Situation) section concisely and EXACTLY in the format below.\\n\"\n",
    "    \"STRICT FORMAT RULES:\\n\"\n",
    "    \"1) Return EXACTLY 5 lines. No extra lines, no blank lines, no leading/trailing spaces.\\n\"\n",
    "    \"2) Each line starts with a hyphen, a space, the header, a colon, a single space, then the content.\\n\"\n",
    "    \"3) Content must be on the SAME LINE as the header (do NOT break to a new line).\\n\"\n",
    "    \"4) Use semicolons ' ; ' to separate multiple facts in the SAME LINE.\\n\"\n",
    "    \"5) Use only information from the source text (no guessing). Numbers/dates/places must be exact.\\n\"\n",
    "    \"6) If a field is missing, write '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'.\\n\"\n",
    "    \"OUTPUT (exactly these 5 lines, in this order):\\n\"\n",
    "    \"- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà/‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå: <content>\\n\"\n",
    "    \"- ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤/‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: <content>\\n\"\n",
    "    \"- ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢/‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: <content>\\n\"\n",
    "    \"- ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏/‡∏¢‡∏≤‡∏ô‡∏û‡∏≤‡∏´‡∏ô‡∏∞‡∏™‡∏á‡∏™‡∏±‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ): <content>\\n\"\n",
    "    \"- ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô: <content>\"\n",
    ")\n",
    "\n",
    "FINDINGS_SYSTEM = (\n",
    "    \"You are an assistant that summarizes outbreak investigation reports in Thai.\\n\"\n",
    "    \"Task: Summarize the '‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô' (Findings) section concisely and EXACTLY in the format below.\\n\"\n",
    "    \"STRICT FORMAT RULES:\\n\"\n",
    "    \"1) Return EXACTLY 10 lines. No extra lines, no blank lines, no leading/trailing spaces.\\n\"\n",
    "    \"2) Each line starts with a hyphen, a space, the header, a colon, a single space, then the content.\\n\"\n",
    "    \"3) All content MUST remain on the SAME LINE as its header (do NOT wrap to a new line).\\n\"\n",
    "    \"4) Use semicolons ' ; ' to separate multiple facts in the SAME LINE; keep original numbers/dates/times.\\n\"\n",
    "    \"5) Use only information from the source text (no guessing). If missing, write '‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏'.\\n\"\n",
    "    \"OUTPUT (exactly these 10 lines, in this order):\\n\"\n",
    "    \"- ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢/‡∏™‡∏≥‡∏£‡∏ß‡∏à/‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏õ‡πà‡∏ß‡∏¢: <content>\\n\"\n",
    "    \"- ‡πÄ‡∏û‡∏®‚Äì‡∏≠‡∏≤‡∏¢‡∏∏: <content>\\n\"\n",
    "    \"- ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡πà‡∏ô: <content>\\n\"\n",
    "    \"- ‡πÄ‡∏™‡πâ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î: <content>\\n\"\n",
    "    \"- ‡∏¢‡∏≤‡∏ô‡∏û‡∏≤‡∏´‡∏ô‡∏∞‡∏™‡∏á‡∏™‡∏±‡∏¢: <content>\\n\"\n",
    "    \"- ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤: <content>\\n\"\n",
    "    \"- ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà: <content>\\n\"\n",
    "    \"- ‡∏ú‡∏•‡πÅ‡∏•‡πá‡∏ö: <content>\\n\"\n",
    "    \"- ‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°: <content>\\n\"\n",
    "    \"- ‡πÑ‡∏ó‡∏°‡πå‡πÑ‡∏•‡∏ô‡πå‡∏≠‡∏≤‡∏´‡∏≤‡∏£: <content>\"\n",
    ")\n",
    "\n",
    "\n",
    "def summarize_sections(situation_text: str, findings_text: str, verbose: bool = True) -> Tuple[str, str]:\n",
    "    \"\"\"Summarize situation and findings sections with optional verbose output.\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: CREATING SUMMARIES\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Using {MODEL_NAME_QWEN} for summarization...\")\n",
    "    \n",
    "    # Use Qwen for summarization (since it's better at structured output)\n",
    "    situation_summary = run_groq(SITUATION_SYSTEM, f'{situation_text} /nothink', MODEL_NAME_QWEN, 1500)\n",
    "    findings_summary = run_groq(FINDINGS_SYSTEM, f'{findings_text} /nothink', MODEL_NAME_QWEN, 1500)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"‚úì Summaries completed\")\n",
    "        print(f\"Situation summary length: {len(situation_summary)} characters\")\n",
    "        print(f\"Findings summary length: {len(findings_summary)} characters\")\n",
    "        print(\"\\nSITUATION SUMMARY:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(situation_summary)\n",
    "        \n",
    "        print(\"\\nFINDINGS SUMMARY:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(findings_summary)\n",
    "        \n",
    "    \n",
    "    return situation_summary, findings_summary\n",
    "\n",
    "# ==================== ANALYSIS ====================\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a Thai-speaking field epidemiology reviewer. \"\n",
    "    \"Input: Situation, Findings, Actions, and Future Plan summaries from the Spot Report \"\n",
    "    \"(‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô) of a foodborne outbreak. \"\n",
    "    \"Task: evaluate adequacy of Actions already taken (‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß), \"\n",
    "    \"state if they are sufficient, and suggest improvements if not. \"\n",
    "    \"Also provide recommendations to strengthen the Future Plan. \"\n",
    "    \"Output must be raw JSON only with English key names and Thai content. \"\n",
    "    \"Use only these exact key names: Actions_Adequacy, Adequacy, Recommendations, \"\n",
    "    \"Actionable_Instructions, Next_24-72_hr, Next_1-2_weeks, Flaws_Gaps, Future_plan_add-ons, Rationale, Response_Time. \"\n",
    "    \"Start directly with { and end with }. No markdown, no explanatory text.\"\n",
    ")\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤:\n",
    "- ‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏ñ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ì‡πå\n",
    "- ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô (‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏ô‡πâ‡∏≥ ‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏• ‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏î‡∏¥‡∏ö/‡∏™‡∏∏‡∏Å)\n",
    "- ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢/‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á\n",
    "- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà\n",
    "- ‡∏Å‡∏≤‡∏£‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á/‡∏ú‡∏•‡πÅ‡∏•‡∏ö\n",
    "- ‡πÅ‡∏ú‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏ö\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\n",
    "[SITUATION]\n",
    "{SITUATION}\n",
    "\n",
    "[FINDINGS]\n",
    "{FINDINGS}\n",
    "\n",
    "[ACTIONS]\n",
    "{ACTIONS}\n",
    "\n",
    "[FUTURE_PLAN]\n",
    "{FUTURE_PLAN}\n",
    "\n",
    "[‡πÄ‡∏Å‡∏ì‡∏ë‡πå‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô]\n",
    "{RUBRIC}\n",
    "\n",
    "‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡πÄ‡∏≠‡∏≤‡∏ï‡πå‡∏û‡∏∏‡∏ï: ‡∏™‡∏£‡πâ‡∏≤‡∏á JSON ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ key names ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡∏µ‡πâ‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£:\n",
    "\n",
    "{{\n",
    "  \"Actions_Adequacy\": {{\n",
    "    \"Adequacy\": false,\n",
    "    \"Recommendations\": [\n",
    "      \"‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 1\",\n",
    "      \"‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 2\"\n",
    "    ]\n",
    "  }},\n",
    "  \"Actionable_Instructions\": {{\n",
    "    \"Next_24-72_hr\": [\n",
    "      \"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 1\",\n",
    "      \"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡πÄ‡∏£‡πà‡∏á‡∏î‡πà‡∏ß‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 2\"\n",
    "    ],\n",
    "    \"Next_1-2_weeks\": [\n",
    "      \"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏•‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 1\",\n",
    "      \"‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏ó‡∏≥‡∏£‡∏∞‡∏¢‡∏∞‡∏Å‡∏•‡∏≤‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 2\"\n",
    "    ]\n",
    "  }},\n",
    "  \"Flaws_Gaps\": [\n",
    "    \"‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 1\",\n",
    "    \"‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 2\"\n",
    "  ],\n",
    "  \"Future_plan_add-ons\": [\n",
    "    \"‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 1\",\n",
    "    \"‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 2\"\n",
    "  ],\n",
    "  \"Rationale\": [\n",
    "    \"‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 1\",\n",
    "    \"‡πÄ‡∏´‡∏ï‡∏∏‡∏ú‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ 2\"\n",
    "  ],\n",
    "  \"Response_Time\": \"21 ‡∏ß‡∏±‡∏ô\"\n",
    "}}\n",
    "\n",
    "‡∏Ç‡πâ‡∏≠‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç:\n",
    "- ‡πÉ‡∏ä‡πâ key names ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡∏≠‡∏±‡∏á‡∏Å‡∏§‡∏©‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Ç‡πâ‡∏≤‡∏á‡∏ï‡πâ‡∏ô‡∏ó‡∏∏‡∏Å‡∏õ‡∏£‡∏∞‡∏Å‡∏≤‡∏£\n",
    "- ‡∏´‡πâ‡∏≤‡∏°‡πÉ‡∏ä‡πâ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠ # ‡∏´‡∏£‡∏∑‡∏≠‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏∑‡πà‡∏ô‡πÄ‡∏õ‡πá‡∏ô key names\n",
    "- ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡πÉ‡∏ô values ‡πÅ‡∏•‡∏∞ arrays ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢\n",
    "- Adequacy ‡πÉ‡∏™‡πà true ‡∏´‡∏£‡∏∑‡∏≠ false ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô\n",
    "- ‡∏ï‡πâ‡∏≠‡∏á‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ô‡∏µ‡πâ‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏û‡∏≤‡∏∞ (‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà ‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏î‡∏¥‡∏ö/‡∏™‡∏∏‡∏Å ‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£)\n",
    "- ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà [\"‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏ö‡∏Å‡∏û‡∏£‡πà‡∏≠‡∏á‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç\"] ‡πÉ‡∏ô Flaws_Gaps\n",
    "- ‡πÄ‡∏≠‡∏≤‡∏ï‡πå‡∏û‡∏∏‡∏ï‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô JSON ‡∏ó‡∏µ‡πà valid ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏á ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ ```json wrapper ‡∏´‡∏£‡∏∑‡∏≠ markdown formatting\n",
    "- ‡∏´‡πâ‡∏≤‡∏°‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏´‡∏£‡∏∑‡∏≠‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å JSON\n",
    "\"\"\".strip()\n",
    "\n",
    "def analyze_report_with_model(situation: str, findings: str, actions: str, future_plan: str, \n",
    "                             model_name: str, analysis_type: str = \"\", verbose: bool = True) -> Tuple[str, float]:\n",
    "    \"\"\"Analyze report with specified model and return JSON output with response time.\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nRunning {analysis_type} with {model_name}...\")\n",
    "    \n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        SITUATION=situation.strip(),\n",
    "        FINDINGS=findings.strip(),\n",
    "        ACTIONS=actions.strip() or \"(‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤)\",\n",
    "        FUTURE_PLAN=future_plan.strip() or \"(‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏™‡πà‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏≤)\",\n",
    "        RUBRIC=RUBRIC.strip()\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    output = run_groq(SYSTEM_PROMPT, user_prompt, model_name, 3000)\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"‚úì Completed in {response_time:.2f}s\")\n",
    "    \n",
    "    # Try to parse and add response time to JSON\n",
    "    try:\n",
    "        parsed = json.loads(output)\n",
    "        parsed[\"Response_Time\"] = f\"{response_time:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\"\n",
    "        parsed[\"Model_Used\"] = model_name\n",
    "        output = json.dumps(parsed, ensure_ascii=False, indent=2)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    return output, response_time\n",
    "\n",
    "\n",
    "def _update_response_time(json_str: str, model_seconds: float, rag_seconds: float, model_name: str) -> str:\n",
    "    total = float(model_seconds) + float(rag_seconds)\n",
    "    try:\n",
    "        obj = json.loads(json_str)\n",
    "        obj[\"Response_Time\"] = f\"{total:.2f} ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\"\n",
    "        obj[\"Model_Used\"] = model_name\n",
    "        return json.dumps(obj, ensure_ascii=False, indent=2)\n",
    "    except Exception:\n",
    "        # ‡∏ñ‡πâ‡∏≤ parse ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ ‡∏Å‡πá‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏î‡∏¥‡∏°\n",
    "        return json_str\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "def process_pdf_file(pdf_path: str, verbose: bool = True, rag_system: Optional[object] = None) -> Dict[str, any]:\n",
    "    \"\"\"Process a single PDF file with both models and return analysis results.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: path to PDF\n",
    "        verbose: print progress\n",
    "        rag_system: pre-built RAG system to reuse; if None, will create one here\n",
    "    \"\"\"\n",
    "    filename = Path(pdf_path).stem\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"PROCESSING FILE: {filename}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract text\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 1: EXTRACTING TEXT FROM PDF\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úì Text extracted: {len(raw_text)} characters\")\n",
    "            print(f\"Preview (first 200 chars): {raw_text[:200]}...\")\n",
    "        \n",
    "        # Step 2: Parse sections\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 2: PARSING SECTIONS\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        paras = split_report(raw_text)\n",
    "        sections = extract_sections(paras)\n",
    "        \n",
    "        situation_text = pick_section(sections, \"situation\")\n",
    "        findings_text = pick_section(sections, \"findings\")\n",
    "        actions_text = pick_section(sections, \"actions_done\")\n",
    "        future_text = pick_section(sections, \"actions_next\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"‚úì Found {len(paras)} paragraphs\")\n",
    "            print(f\"‚úì Identified {len(sections)} sections:\")\n",
    "            for section_name in sections.keys():\n",
    "                print(f\"    - {section_name}\")\n",
    "            \n",
    "            print(f\"\\nExtracted key sections:\")\n",
    "            print(f\"  Situation: {len(situation_text)} characters\")\n",
    "            print(f\"  Findings: {len(findings_text)} characters\")\n",
    "            print(f\"  Actions: {len(actions_text)} characters\") \n",
    "            print(f\"  Future: {len(future_text)} characters\")\n",
    "            \n",
    "            # Show preview of original sections\n",
    "            print(\"\\nORIGINAL SITUATION (first 300 chars):\")\n",
    "            print(\"-\" * 40)\n",
    "            print(situation_text[:300] + (\"...\" if len(situation_text) > 300 else \"\"))\n",
    "            \n",
    "            print(\"\\nORIGINAL FINDINGS (first 300 chars):\")\n",
    "            print(\"-\" * 40)\n",
    "            print(findings_text[:300] + (\"...\" if len(findings_text) > 300 else \"\"))\n",
    "        \n",
    "        # Step 3: Summarize (using Qwen)\n",
    "        situation_summary, findings_summary = summarize_sections(\n",
    "            situation_text, findings_text, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Step 4: Run all 4 analyses\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 4: RUNNING ANALYSES WITH BOTH MODELS\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        # Qwen analyses\n",
    "        qwen_no_summary_result, qwen_no_summary_time = analyze_report_with_model(\n",
    "            situation_text, findings_text, actions_text, future_text,\n",
    "            MODEL_NAME_QWEN, \"Qwen No-Summary\", verbose=verbose\n",
    "        )\n",
    "        \n",
    "        qwen_summary_result, qwen_summary_time = analyze_report_with_model(\n",
    "            situation_summary, findings_summary, actions_text, future_text,\n",
    "            MODEL_NAME_QWEN, \"Qwen Summary\", verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Llama analyses\n",
    "        llama_no_summary_result, llama_no_summary_time = analyze_report_with_model(\n",
    "            situation_text, findings_text, actions_text, future_text,\n",
    "            MODEL_NAME_LLAMA, \"Llama No-Summary\", verbose=verbose\n",
    "        )\n",
    "        \n",
    "        llama_summary_result, llama_summary_time = analyze_report_with_model(\n",
    "            situation_summary, findings_summary, actions_text, future_text,\n",
    "            MODEL_NAME_LLAMA, \"Llama Summary\", verbose=verbose\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 5: PROCESSING SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"‚úì File: {filename}\")\n",
    "            print(f\"‚úì Qwen no-summary: {qwen_no_summary_time:.2f}s\")\n",
    "            print(f\"‚úì Qwen summary: {qwen_summary_time:.2f}s\")  \n",
    "            print(f\"‚úì Llama no-summary: {llama_no_summary_time:.2f}s\")\n",
    "            print(f\"‚úì Llama summary: {llama_summary_time:.2f}s\")\n",
    "            print(f\"‚úì Total model time: {(qwen_no_summary_time + qwen_summary_time + llama_no_summary_time + llama_summary_time):.2f}s\")\n",
    "            print(f\"‚úì All analyses completed successfully\")\n",
    "            \n",
    "            # Show response length comparison\n",
    "            print(f\"\\nResponse lengths comparison:\")\n",
    "            print(f\"  Qwen no-summary: {len(qwen_no_summary_result)} chars\")\n",
    "            print(f\"  Qwen summary: {len(qwen_summary_result)} chars\")\n",
    "            print(f\"  Llama no-summary: {len(llama_no_summary_result)} chars\")\n",
    "            print(f\"  Llama summary: {len(llama_summary_result)} chars\")\n",
    "\n",
    "        # ---------------------------\n",
    "        # STEP 6: RAG ENHANCEMENT\n",
    "        # ---------------------------\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 6: RAG ENHANCEMENT (MERGE WITH GUIDELINES)\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "        # rag_total_start = time.time()\n",
    "\n",
    "        # Setup/Load RAG system (once or per call if None)\n",
    "        rag_setup_start = time.time()\n",
    "        if rag_system is None:\n",
    "            if verbose:\n",
    "                print(\"‚Ä¢ Loading/Building RAG system...\")\n",
    "            rag_system = create_system(\"Docs/Guideline.pdf\", \"my_medical_ragv2.pkl\")\n",
    "        rag_setup_time = time.time() - rag_setup_start\n",
    "        if verbose:\n",
    "            print(f\"‚úì RAG system ready ({rag_setup_time:.2f}s)\")\n",
    "\n",
    "        enhancer = RAGEnhancementSystem(rag_system)\n",
    "\n",
    "        # Qwen RAG\n",
    "        rag_qwen_start = time.time()\n",
    "        qwen_summary_rag = enhancer.run_enhancement(qwen_summary_result, top_k=1)\n",
    "        rag_qwen_time = time.time() - rag_qwen_start\n",
    "        qwen_summary_rag = _update_response_time(\n",
    "            qwen_summary_rag, \n",
    "            model_seconds=qwen_summary_time, \n",
    "            rag_seconds=rag_qwen_time, \n",
    "            model_name=MODEL_NAME_QWEN\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"‚úì Qwen summary ‚Üí RAG enhanced in {rag_qwen_time:.2f}s\")\n",
    "\n",
    "        # Llama RAG\n",
    "        rag_llama_start = time.time()\n",
    "        llama_summary_rag = enhancer.run_enhancement(llama_summary_result, top_k=1)\n",
    "        rag_llama_time = time.time() - rag_llama_start\n",
    "        if verbose:\n",
    "            print(f\"‚úì Llama summary ‚Üí RAG enhanced in {rag_llama_time:.2f}s\")\n",
    "        # rag_total_time = time.time() - rag_total_start\n",
    "        \n",
    "        llama_summary_rag = _update_response_time(\n",
    "            llama_summary_rag, \n",
    "            model_seconds=llama_summary_time, \n",
    "            rag_seconds=rag_llama_time, \n",
    "            model_name=MODEL_NAME_LLAMA\n",
    "        )\n",
    "        if verbose:\n",
    "            # print(f\"‚úì Total RAG time: {rag_total_time:.2f}s\")\n",
    "            print(\"‚úì RAG enhancement completed\")\n",
    "\n",
    "        summaries_output = f\"\"\"SITUATION SUMMARY\n",
    "{'-'*40}\n",
    "{situation_summary}\n",
    "        \n",
    "FINDINGS SUMMARY\n",
    "{'-'*40}\n",
    "{findings_summary}\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"file\": Path(pdf_path).stem,\n",
    "            \"situation_findings_summary\": summaries_output,\n",
    "            f\"{MODEL_NAME_QWEN}_no-summary\": qwen_no_summary_result,\n",
    "            f\"{MODEL_NAME_QWEN}_summary\": qwen_summary_result,\n",
    "            f\"{MODEL_NAME_QWEN}_summary_rag\": qwen_summary_rag,\n",
    "            f\"{LLAMA_SHORT_NAME}_no-summary\": llama_no_summary_result,\n",
    "            f\"{LLAMA_SHORT_NAME}_summary\": llama_summary_result,\n",
    "            f\"{LLAMA_SHORT_NAME}_summary_rag\": llama_summary_rag,\n",
    "            \"processing_success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"\\n‚ùå ERROR: {str(e)}\")\n",
    "            print(\"Processing failed\")\n",
    "        \n",
    "        return {\n",
    "            \"file\": filename,\n",
    "            f\"{MODEL_NAME_QWEN}_no-summary\": f\"Error: {str(e)}\",\n",
    "            f\"{MODEL_NAME_QWEN}_summary\": f\"Error: {str(e)}\",\n",
    "            f\"{MODEL_NAME_QWEN}_summary_rag\": f\"Error: {str(e)}\",\n",
    "            f\"{LLAMA_SHORT_NAME}_no-summary\": f\"Error: {str(e)}\",\n",
    "            f\"{LLAMA_SHORT_NAME}_summary\": f\"Error: {str(e)}\",\n",
    "            f\"{LLAMA_SHORT_NAME}_summary_rag\": f\"Error: {str(e)}\",\n",
    "            \"processing_success\": False\n",
    "        }\n",
    "\n",
    "def process_multiple_pdfs(pdf_paths: List[str], verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Process many PDFs. Build/load RAG system once and reuse.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # ‡πÇ‡∏´‡∏•‡∏î/‡∏™‡∏£‡πâ‡∏≤‡∏á RAG system ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß\n",
    "    rag_system = create_system(\"Docs/Guideline.pdf\", \"my_medical_ragv2.pkl\")\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        result = process_pdf_file(pdf_path, verbose=verbose, rag_system=rag_system)  # ‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÄ‡∏•‡∏¢\n",
    "        results.append(result)\n",
    "        time.sleep(2)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    columns = [\n",
    "        \"file\",\n",
    "        \"situation_findings_summary\",\n",
    "        f\"{MODEL_NAME_QWEN}_no-summary\",\n",
    "        f\"{MODEL_NAME_QWEN}_summary\",\n",
    "        f\"{MODEL_NAME_QWEN}_summary_rag\",\n",
    "        f\"{LLAMA_SHORT_NAME}_no-summary\",\n",
    "        f\"{LLAMA_SHORT_NAME}_summary\",\n",
    "        f\"{LLAMA_SHORT_NAME}_summary_rag\",\n",
    "        \"processing_success\"\n",
    "    ]\n",
    "    for c in columns:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b5e452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3 model loaded successfully!\n",
      "üìÇ Found existing system: my_medical_ragv2.pkl\n",
      "üìÇ Loading system from my_medical_ragv2.pkl...\n",
      "‚úÖ System loaded successfully!\n",
      "  Chunks: 69\n",
      "  Embeddings shape: (69, 1024)\n",
      "  Model info: {'model_name': 'BAAI/bge-m3', 'embedding_dim': 1024, 'num_chunks': 69}\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FILE: Exsum_food_poisoning\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "STEP 1: EXTRACTING TEXT FROM PDF\n",
      "============================================================\n",
      "‚úì Text extracted: 10049 characters\n",
      "Preview (first 200 chars): ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô ‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© ‡πÅ‡∏•‡∏∞‡∏≠‡∏∏‡∏à‡∏à‡∏≤‡∏£‡∏∞‡∏£‡πà‡∏ß‡∏á‡πÄ‡∏â‡∏µ‡∏¢‡∏ö‡∏û‡∏•‡∏±‡∏ô \n",
      " ‡∏ï‡∏≥‡∏ö‡∏•‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ß‡∏µ‡∏¢‡∏á ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡∏î‡∏≠‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ï‡πâ ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏∞‡πÄ‡∏¢‡∏≤ ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 17-19 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô 2568 \n",
      " \n",
      "‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤ \n",
      "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 15 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô ‡∏û.‡∏®. 2568 ‡πÄ‡∏ß‡∏•‡∏≤ 18.00 ‡∏ô. ...\n",
      "\n",
      "============================================================\n",
      "STEP 2: PARSING SECTIONS\n",
      "============================================================\n",
      "‚úì Found 14 paragraphs\n",
      "‚úì Identified 5 sections:\n",
      "    - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏≤\n",
      "    - ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô\n",
      "    - ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡πÑ‡∏õ‡πÅ‡∏•‡πâ‡∏ß\n",
      "    - ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡πà‡∏≠‡πÑ‡∏õ\n",
      "    - ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤\n",
      "\n",
      "Extracted key sections:\n",
      "  Situation: 1024 characters\n",
      "  Findings: 6041 characters\n",
      "  Actions: 230 characters\n",
      "  Future: 102 characters\n",
      "\n",
      "ORIGINAL SITUATION (first 300 chars):\n",
      "----------------------------------------\n",
      "‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 15 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô ‡∏û.‡∏®. 2568 ‡πÄ‡∏ß‡∏•‡∏≤ 18.00 ‡∏ô. ‡∏™‡∏≥‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏™‡∏≤‡∏ò‡∏≤‡∏£‡∏ì‡∏™‡∏∏‡∏Ç‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏∞‡πÄ‡∏¢‡∏≤‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏à‡∏≤‡∏Å ‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡∏î‡∏≠‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ï‡πâ ‡∏û‡∏ö‡∏Å‡∏•‡∏∏‡πà‡∏°‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏©‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 40 ‡∏£‡∏≤‡∏¢ ‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ ‡∏≠‡∏≤‡πÄ‡∏à‡∏µ‡∏¢‡∏ô ‡∏õ‡∏ß‡∏î‡∏ö‡∏¥‡∏î ‡πÅ‡∏ô‡πà‡∏ô‡∏ó‡πâ‡∏≠‡∏á ‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡∏µ‡∏¢‡∏ô‡∏®‡∏µ‡∏£‡∏©‡∏∞ ‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏°‡∏∑‡πâ‡∏≠‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏à‡∏Å‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û ‡∏ì ‡∏´‡∏°‡∏π‡πà 5 ‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏∏‡πà‡∏á‡∏Å‡∏≤‡πÑ‡∏ä‡∏¢ ‡∏ï‡∏≥‡∏ö‡∏•‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ß‡∏µ‡∏¢‡∏á ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠...\n",
      "\n",
      "ORIGINAL FINDINGS (first 300 chars):\n",
      "----------------------------------------\n",
      "‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏î‡πâ‡∏ß‡∏¢‡∏ô‡∏¥‡∏¢‡∏≤‡∏° ‡∏Ñ‡∏∑‡∏≠ (1) ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏ó‡∏µ‡πà‡∏°‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û ‡∏´‡∏°‡∏π‡πà 5 ‡∏ï‡∏≥‡∏ö‡∏•‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ß‡∏µ‡∏¢‡∏á ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡∏î‡∏≠‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ï‡πâ ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏∞‡πÄ‡∏¢‡∏≤ ‡∏´‡∏£‡∏∑‡∏≠ (2) ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û ‡πÇ‡∏î‡∏¢‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° ‡∏ï‡πâ‡∏≠‡∏á‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢ 2 ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£ ‡∏à‡∏≤‡∏Å‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ ‡∏≠‡∏≤‡πÄ‡∏à‡∏µ‡∏¢‡∏ô ‡∏ñ‡πà‡∏≤‡∏¢‡πÄ‡∏´‡∏•‡∏ß /‡∏ñ‡πà‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≥ ‡∏ñ‡πà‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏°‡∏π‡∏Å‡πÄ‡∏•‡∏∑‡∏≠‡∏î ‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á / ‡∏õ‡∏ß‡∏î‡∏ö...\n",
      "\n",
      "============================================================\n",
      "STEP 3: CREATING SUMMARIES\n",
      "============================================================\n",
      "Using qwen/qwen3-32b for summarization...\n",
      "‚úì Summaries completed\n",
      "Situation summary length: 528 characters\n",
      "Findings summary length: 1674 characters\n",
      "\n",
      "SITUATION SUMMARY:\n",
      "----------------------------------------\n",
      "- ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà/‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå: ‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û ‡∏ì ‡∏´‡∏°‡∏π‡πà 5 ‡∏ö‡πâ‡∏≤‡∏ô‡∏ó‡∏∏‡πà‡∏á‡∏Å‡∏≤‡πÑ‡∏ä‡∏¢ ‡∏ï‡∏≥‡∏ö‡∏•‡∏Ñ‡∏∑‡∏≠‡πÄ‡∏ß‡∏µ‡∏¢‡∏á ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡∏î‡∏≠‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ï‡πâ ‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏∞‡πÄ‡∏¢‡∏≤  \n",
      "- ‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤/‡∏ß‡∏±‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç: 15-19 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô ‡∏û.‡∏®. 2568 ; ‡πÅ‡∏à‡∏Å‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤ 11.00‚Äì11.30 ‡∏ô. ; ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏£‡∏≤‡∏¢‡πÅ‡∏£‡∏Å‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤ 12.20 ‡∏ô.  \n",
      "- ‡∏Å‡∏•‡∏∏‡πà‡∏°‡πÄ‡∏õ‡πâ‡∏≤‡∏´‡∏°‡∏≤‡∏¢/‡∏ú‡∏π‡πâ‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á: ‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 400 ‡∏Ñ‡∏ô‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏°‡∏∑‡πâ‡∏≠‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à  \n",
      "- ‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏/‡∏¢‡∏≤‡∏ô‡∏û‡∏≤‡∏´‡∏ô‡∏∞‡∏™‡∏á‡∏™‡∏±‡∏¢ (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ): ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏°‡∏∑‡πâ‡∏≠‡∏Å‡∏•‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏à‡∏Å‡∏†‡∏≤‡∏¢‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à  \n",
      "- ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô: ‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡∏ô‡∏¥‡∏à‡∏â‡∏±‡∏¢‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î ; ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ó‡∏≤‡∏á‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤ ; ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏´‡∏•‡πà‡∏á‡πÇ‡∏£‡∏Ñ‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á ; ‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÇ‡∏£‡∏Ñ\n",
      "\n",
      "FINDINGS SUMMARY:\n",
      "----------------------------------------\n",
      "- ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢/‡∏™‡∏≥‡∏£‡∏ß‡∏à/‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏õ‡πà‡∏ß‡∏¢: ‡∏û‡∏ö‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢ 172 ‡∏£‡∏≤‡∏¢ ‡∏à‡∏≤‡∏Å 311 ‡∏£‡∏≤‡∏¢ (‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏õ‡πà‡∏ß‡∏¢ ‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞ 55.3) ‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏á‡∏≤‡∏ô  \n",
      "- ‡πÄ‡∏û‡∏®‚Äì‡∏≠‡∏≤‡∏¢‡∏∏: ‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏™‡πà‡∏ß‡∏ô‡πÄ‡∏û‡∏®‡∏ä‡∏≤‡∏¢‡∏ï‡πà‡∏≠‡πÄ‡∏û‡∏®‡∏´‡∏ç‡∏¥‡∏á 1 ‡∏ï‡πà‡∏≠ 1.5; ‡∏≠‡∏≤‡∏¢‡∏∏‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 3-94 ‡∏õ‡∏µ (‡∏°‡∏±‡∏ò‡∏¢‡∏ê‡∏≤‡∏ô‡∏≠‡∏≤‡∏¢‡∏∏ 56 ‡∏õ‡∏µ)  \n",
      "- ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏î‡πà‡∏ô: ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ó‡∏µ‡πà‡∏û‡∏ö‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà ‡∏Ñ‡∏•‡∏∑‡πà‡∏ô‡πÑ‡∏™‡πâ ‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞ 95.9; ‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤ ‡∏≠‡∏≤‡πÄ‡∏à‡∏µ‡∏¢‡∏ô ‡∏õ‡∏ß‡∏î‡∏ó‡πâ‡∏≠‡∏á/‡∏õ‡∏ß‡∏î‡∏ö‡∏¥‡∏î/‡∏°‡∏ß‡∏ô‡∏ó‡πâ‡∏≠‡∏á ‡∏ñ‡πà‡∏≤‡∏¢‡πÄ‡∏´‡∏•‡∏ß/‡∏ñ‡πà‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ô‡πâ‡∏≥ ‡πÑ‡∏Ç‡πâ ‡∏ñ‡πà‡∏≤‡∏¢‡∏õ‡∏ô‡∏°‡∏π‡∏Å/‡πÄ‡∏•‡∏∑‡∏≠‡∏î ‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞ 87.8, 77.9, 54.7, 12.8 ‡πÅ‡∏•‡∏∞ 2.9 ‡∏ï‡∏≤‡∏°‡∏•‡∏≥‡∏î‡∏±‡∏ö  \n",
      "- ‡πÄ‡∏™‡πâ‡∏ô‡πÇ‡∏Ñ‡πâ‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î: ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏£‡∏≤‡∏¢‡πÅ‡∏£‡∏Å‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏°‡∏µ‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 15 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô 2568 ‡πÄ‡∏ß‡∏•‡∏≤ 12.20 ‡∏ô. (‡∏´‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 60 ‡∏ô‡∏≤‡∏ó‡∏µ); ‡∏û‡∏ö‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÄ‡∏ß‡∏•‡∏≤ 15.00-15.59 ‡∏ô. (‡∏´‡∏•‡∏±‡∏á‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ó‡∏≤‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 4 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á); ‡∏û‡∏ö‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏£‡∏≤‡∏¢‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏ß‡∏•‡∏≤ 21.00 ‡∏ô. (10 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏à‡∏Å‡∏à‡πà‡∏≤‡∏¢‡∏≠‡∏≤‡∏´‡∏≤‡∏£)  \n",
      "- ‡∏¢‡∏≤‡∏ô‡∏û‡∏≤‡∏´‡∏ô‡∏∞‡∏™‡∏á‡∏™‡∏±‡∏¢: ‡πÑ‡∏°‡πà‡∏£‡∏∞‡∏ö‡∏∏  \n",
      "- ‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤: ‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ó‡∏µ‡πà‡∏™‡∏ñ‡∏≤‡∏ô‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏• 119 ‡∏£‡∏≤‡∏¢ ‡πÅ‡∏ö‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ô‡∏≠‡∏Å 110 ‡∏£‡∏≤‡∏¢ ‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÉ‡∏ô 9 ‡∏£‡∏≤‡∏¢  \n",
      "- ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏û‡∏∑‡πâ‡∏ô‡∏ó‡∏µ‡πà: ‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏°‡∏µ‡∏†‡∏π‡∏°‡∏¥‡∏•‡∏≥‡πÄ‡∏ô‡∏≤‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏à‡∏±‡∏á‡∏´‡∏ß‡∏±‡∏î‡∏û‡∏∞‡πÄ‡∏¢‡∏≤ ‡πÇ‡∏î‡∏¢‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡∏î‡∏≠‡∏Å‡∏Ñ‡∏≥‡πÉ‡∏ï‡πâ 169 ‡∏£‡∏≤‡∏¢ (‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞ 98.3); ‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡πÄ‡∏°‡∏∑‡∏≠‡∏á 2 ‡∏£‡∏≤‡∏¢ (‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞ 1.2); ‡πÅ‡∏•‡∏∞‡∏≠‡∏≥‡πÄ‡∏†‡∏≠‡∏†‡∏π‡∏ã‡∏≤‡∏á 1 ‡∏£‡∏≤‡∏¢ (‡∏£‡πâ‡∏≠‡∏¢‡∏•‡∏∞ 0.6)  \n",
      "- ‡∏ú‡∏•‡πÅ‡∏•‡πá‡∏ö: ‡∏ú‡∏• RT-PCR ‡∏û‡∏ö Enteropathogenic E.coli (EPEC) 5 ‡∏£‡∏≤‡∏¢, Enteroaggregative E.coli (EAEC) 1 ‡∏£‡∏≤‡∏¢; ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ä‡∏∑‡πâ‡∏≠ 4 ‡∏£‡∏≤‡∏¢; ‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏≤‡∏∞‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ä‡∏∑‡πâ‡∏≠ 9 ‡∏£‡∏≤‡∏¢ ‡∏£‡∏≠‡∏ú‡∏• 1 ‡∏£‡∏≤‡∏¢; ‡∏≠‡∏¢‡∏π‡πà‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏≠‡∏ú‡∏•‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡πÄ‡∏û‡∏≤‡∏∞‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏à‡∏≤‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô‡πÜ  \n",
      "- ‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°: ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏•‡∏≤‡∏ô‡∏Å‡∏ß‡πâ‡∏≤‡∏á ‡∏°‡∏µ‡∏´‡∏•‡∏±‡∏á‡∏Ñ‡∏≤ ‡∏´‡πà‡∏≤‡∏á‡∏à‡∏≤‡∏Å‡∏´‡πâ‡∏≠‡∏á‡∏ô‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏ö‡πà‡∏≠‡πÄ‡∏Å‡∏£‡∏≠‡∏∞‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 5 ‡πÄ‡∏°‡∏ï‡∏£; ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏≠‡πà‡∏≤‡∏á‡∏•‡πâ‡∏≤‡∏á‡∏°‡∏∑‡∏≠; ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏Ç‡∏≠‡∏≠‡∏ô‡∏∏‡∏ç‡∏≤‡∏ï‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏≥‡∏´‡∏ô‡πà‡∏≤‡∏¢‡∏≠‡∏≤‡∏´‡∏≤‡∏£ ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏ö‡∏£‡∏°‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£; ‡∏û‡∏ö‡πÅ‡∏ú‡∏•‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡πâ‡∏ß‡∏Å‡πâ‡∏≠‡∏¢‡∏°‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏≠‡∏≤‡∏´‡∏≤‡∏£  \n",
      "- ‡πÑ‡∏ó‡∏°‡πå‡πÑ‡∏•‡∏ô‡πå‡∏≠‡∏≤‡∏´‡∏≤‡∏£: ‡∏á‡∏≤‡∏ô‡∏®‡∏û‡∏à‡∏±‡∏î‡∏Ç‡∏∂‡πâ‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 12-14 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô 2568; ‡∏ß‡∏±‡∏ô‡∏ó‡∏µ‡πà 15 ‡∏°‡∏¥‡∏ñ‡∏∏‡∏ô‡∏≤‡∏¢‡∏ô 2568 ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û ‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 400 ‡∏Ñ‡∏ô; ‡πÅ‡∏à‡∏Å‡∏à‡πà‡∏≤‡∏¢‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏ß‡∏•‡∏≤ 11.00-11.30 ‡∏ô. ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà ‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á ‡πÑ‡∏≠‡∏®‡∏Å‡∏£‡∏µ‡∏°‡∏Å‡∏∞‡∏ó‡∏¥/‡∏ã‡πà‡∏≤‡∏´‡∏£‡∏¥‡πà‡∏° ‡∏ô‡∏°‡πÄ‡∏õ‡∏£‡∏µ‡πâ‡∏¢‡∏ß ‡∏ô‡πâ‡∏≥‡πÄ‡∏õ‡∏•‡πà‡∏≤ (‡∏Ç‡∏ß‡∏î) ‡∏ô‡πâ‡∏≥‡πÄ‡∏õ‡∏•‡πà‡∏≤ (‡πÅ‡∏Å‡πâ‡∏ß)\n",
      "\n",
      "============================================================\n",
      "STEP 4: RUNNING ANALYSES WITH BOTH MODELS\n",
      "============================================================\n",
      "\n",
      "Running Qwen No-Summary with qwen/qwen3-32b...\n",
      "‚úì Completed in 62.66s\n",
      "\n",
      "Running Qwen Summary with qwen/qwen3-32b...\n",
      "‚úì Completed in 30.29s\n",
      "\n",
      "Running Llama No-Summary with llama-3.1-8b-instant...\n",
      "‚úì Completed in 1.15s\n",
      "\n",
      "Running Llama Summary with llama-3.1-8b-instant...\n",
      "‚úì Completed in 17.26s\n",
      "\n",
      "============================================================\n",
      "STEP 5: PROCESSING SUMMARY\n",
      "============================================================\n",
      "‚úì File: Exsum_food_poisoning\n",
      "‚úì Qwen no-summary: 62.66s\n",
      "‚úì Qwen summary: 30.29s\n",
      "‚úì Llama no-summary: 1.15s\n",
      "‚úì Llama summary: 17.26s\n",
      "‚úì Total model time: 111.36s\n",
      "‚úì All analyses completed successfully\n",
      "\n",
      "Response lengths comparison:\n",
      "  Qwen no-summary: 2027 chars\n",
      "  Qwen summary: 1549 chars\n",
      "  Llama no-summary: 1267 chars\n",
      "  Llama summary: 1433 chars\n",
      "\n",
      "============================================================\n",
      "STEP 6: RAG ENHANCEMENT (MERGE WITH GUIDELINES)\n",
      "============================================================\n",
      "‚úì RAG system ready (0.00s)\n",
      "üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG Enhancement\n",
      "============================================================\n",
      "üìã ‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ 6 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ query:\n",
      "   1. [Next_24-72_hr] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏£‡∏ß‡∏°‡∏ñ...\n",
      "   2. [Next_24-72_hr] ‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±...\n",
      "   3. [Next_1-2_weeks] ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏¢...\n",
      "   4. [Next_1-2_weeks] ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡πÅ‡∏û‡∏£‡πà‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á...\n",
      "   5. [Future_plan_add-ons] ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏ï‡πà‡∏•...\n",
      "   6. [Future_plan_add-ons] ‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏≤...\n",
      "\n",
      "üìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á query 6 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô knowledge base...\n",
      "\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á query 6 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô RAG system...\n",
      "üîç Searching: '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏†‡∏≤‡∏û‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î ‡∏£‡∏ß‡∏°‡∏ñ‡∏∂‡∏á‡∏Å‡∏≤‡∏£‡∏•‡πâ‡∏≤‡∏á‡∏°‡∏∑‡∏≠‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5911)\n",
      "üîç Searching: '‡∏ï‡∏¥‡∏î‡∏ï‡∏≤‡∏°‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏ï‡∏±‡∏ß‡πÉ‡∏ô‡πÇ‡∏£‡∏á‡∏û‡∏¢‡∏≤‡∏ö‡∏≤‡∏•‡πÅ‡∏•‡∏∞‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5674)\n",
      "üîç Searching: '‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏≠‡∏ö‡∏£‡∏°‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏ö‡∏ö'\n",
      "  Found 1 results\n",
      "   1. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.4952)\n",
      "üîç Searching: '‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡πÄ‡∏ä‡∏¥‡∏á‡∏•‡∏∂‡∏Å‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏∞‡∏ö‡∏∏‡πÅ‡∏´‡∏•‡πà‡∏á‡πÅ‡∏û‡∏£‡πà‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÅ‡∏•‡∏∞‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô'\n",
      "  Found 1 results\n",
      "   1. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.6430)\n",
      "üîç Searching: '‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ä‡∏¥‡∏á‡∏õ‡∏£‡∏¥‡∏°‡∏≤‡∏ì‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ä‡∏ô‡∏¥‡∏î‡∏Å‡∏±‡∏ö‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡πà‡∏ß‡∏¢'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.6497)\n",
      "üîç Searching: '‡∏ö‡∏π‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡∏ú‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡∏ß‡∏¥‡∏ó‡∏¢‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏‡πÄ‡∏ä‡∏¥‡∏á‡∏ä‡∏µ‡∏ß‡∏†‡∏≤‡∏û'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.6177)\n",
      "\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\n",
      "\n",
      "üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM...\n",
      "\n",
      "üß© ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå RAG ‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏î‡∏¥‡∏°...\n",
      "\n",
      "‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG Enhancement ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "‚úì Qwen summary ‚Üí RAG enhanced in 30.24s\n",
      "üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG Enhancement\n",
      "============================================================\n",
      "üìã ‡πÅ‡∏¢‡∏Å‡πÑ‡∏î‡πâ 6 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ query:\n",
      "   1. [Next_24-72_hr] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏†‡∏≤‡∏¢‡πÉ‡∏ô...\n",
      "   2. [Next_24-72_hr] ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´...\n",
      "   3. [Next_1-2_weeks] ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£...\n",
      "   4. [Next_1-2_weeks] ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å...\n",
      "   5. [Future_plan_add-ons] ‡∏Ñ‡∏ß‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏å‡∏≤...\n",
      "   6. [Future_plan_add-ons] ‡∏Ñ‡∏ß‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö...\n",
      "\n",
      "üìö ‡∏Å‡∏≥‡∏•‡∏±‡∏á query 6 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô knowledge base...\n",
      "\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á query 6 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÉ‡∏ô RAG system...\n",
      "üîç Searching: '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 24-72 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5557)\n",
      "üîç Searching: '‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 24-72 ‡∏ä‡∏±‡πà‡∏ß‡πÇ‡∏°‡∏á'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5739)\n",
      "üîç Searching: '‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 1-2 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5686)\n",
      "üîç Searching: '‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏†‡∏≤‡∏¢‡πÉ‡∏ô 1-2 ‡∏™‡∏±‡∏õ‡∏î‡∏≤‡∏´‡πå'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5379)\n",
      "üîç Searching: '‡∏Ñ‡∏ß‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏™‡∏¥‡πà‡∏á‡πÅ‡∏ß‡∏î‡∏•‡πâ‡∏≠‡∏°‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ó‡∏µ‡πà‡∏à‡∏±‡∏î‡∏á‡∏≤‡∏ô‡∏å‡∏≤‡∏õ‡∏ô‡∏Å‡∏¥‡∏à‡∏®‡∏û‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î'\n",
      "  Found 1 results\n",
      "   1. ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏™‡∏≠‡∏ö‡∏™‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏£‡∏∞‡∏ö‡∏≤‡∏î‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5091)\n",
      "üîç Searching: '‡∏Ñ‡∏ß‡∏£‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡πÅ‡∏•‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏£‡∏∞‡∏ö‡∏ö‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á'\n",
      "  Found 1 results\n",
      "   1. ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏Ç‡∏≠‡∏á‡πÇ‡∏£‡∏Ñ‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏û‡∏¥‡∏© (‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô: 0.5034)\n",
      "\n",
      "üîç ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•...\n",
      "\n",
      "üß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏î‡πâ‡∏ß‡∏¢ LLM...\n",
      "\n",
      "üß© ‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå RAG ‡∏Å‡∏±‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏î‡∏¥‡∏°...\n",
      "\n",
      "‚úÖ ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ RAG Enhancement ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô\n",
      "‚úì Llama summary ‚Üí RAG enhanced in 51.04s\n",
      "‚úì RAG enhancement completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå PDF ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏• (‡∏ß‡∏¥‡∏ò‡∏µ A: ‡∏£‡∏∞‡∏ö‡∏∏‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£‡πÄ‡∏≠‡∏á) ---\n",
    "pdf_paths = [\n",
    "    \"Docs/Exsum_food_poisoning.pdf\",\n",
    "    # \"Docs/Another_report.pdf\",\n",
    "]\n",
    "\n",
    "# ‡∏£‡∏±‡∏ô pipeline ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏´‡∏•‡∏≤‡∏¢‡πÑ‡∏ü‡∏•‡πå (‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á/‡πÇ‡∏´‡∏•‡∏î RAG system ‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÅ‡∏•‡∏∞ reuse)\n",
    "df_multi = process_multiple_pdfs(pdf_paths, verbose=True)\n",
    "\n",
    "# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå (‡πÉ‡∏ä‡πâ utf-8-sig ‡∏´‡∏≤‡∏Å‡∏à‡∏∞‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡πâ‡∏ß‡∏¢ Excel ‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á)\n",
    "df_multi.to_csv(\"analysis_results_rag_multi.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be518965",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7f3636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>situation_findings_summary</th>\n",
       "      <th>qwen/qwen3-32b_no-summary</th>\n",
       "      <th>qwen/qwen3-32b_summary</th>\n",
       "      <th>qwen/qwen3-32b_summary_rag</th>\n",
       "      <th>llama3.1-8b_no-summary</th>\n",
       "      <th>llama3.1-8b_summary</th>\n",
       "      <th>llama3.1-8b_summary_rag</th>\n",
       "      <th>processing_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exsum_food_poisoning</td>\n",
       "      <td>SITUATION SUMMARY\\n---------------------------...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file                         situation_findings_summary  \\\n",
       "0  Exsum_food_poisoning  SITUATION SUMMARY\\n---------------------------...   \n",
       "\n",
       "                           qwen/qwen3-32b_no-summary  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                              qwen/qwen3-32b_summary  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                          qwen/qwen3-32b_summary_rag  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                              llama3.1-8b_no-summary  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                                 llama3.1-8b_summary  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                             llama3.1-8b_summary_rag  processing_success  \n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...                True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rag = pd.read_csv('analysis_results_rag_latest.csv')\n",
    "display(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "655fb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.to_excel('analysis_results_rag_latest.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d003c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Actions_Adequacy\": {\n",
      "    \"Adequacy\": false,\n",
      "    \"Recommendations\": [\n",
      "      \"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß‡∏à‡∏ô‡∏Å‡∏ß‡πà‡∏≤‡∏à‡∏∞‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ö‡∏£‡∏≠‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£\",\n",
      "      \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÇ‡∏î‡∏¢‡πÄ‡∏ô‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡πÅ‡∏¢‡∏Å‡∏†‡∏≤‡∏ä‡∏ô‡∏∞‡∏î‡∏¥‡∏ö/‡∏™‡∏∏‡∏Å‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏∞‡∏≠‡∏≤‡∏î‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏î‡πâ‡∏ß‡∏¢‡∏™‡∏≤‡∏£‡∏Ü‡πà‡∏≤‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°\",\n",
      "      \"‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏£‡πà‡∏ß‡∏°‡∏á‡∏≤‡∏ô‡∏®‡∏û‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≤‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏ß‡∏•‡∏≤ 7 ‡∏ß‡∏±‡∏ô (‡∏£‡∏∞‡∏¢‡∏∞‡∏ü‡∏±‡∏Å‡∏ï‡∏±‡∏ß‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á EPEC ‡∏Ñ‡∏∑‡∏≠ 1-3 ‡∏ß‡∏±‡∏ô)\",\n",
      "      \"‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏ö‡∏£‡∏°‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ï‡∏≤‡∏°‡∏°‡∏≤‡∏ï‡∏£‡∏ê‡∏≤‡∏ô‡∏Å‡∏£‡∏°‡∏≠‡∏ô‡∏≤‡∏°‡∏±‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡πÄ‡∏õ‡∏¥‡∏î‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏µ‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á\"\n",
      "    ]\n",
      "  },\n",
      "  \"Actionable_Instructions\": {\n",
      "    \"Next_24-72_hr\": [\n",
      "      \"‡∏™‡∏±‡πà‡∏á‡∏´‡πâ‡∏≤‡∏°‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏à‡∏≥‡∏´‡∏ô‡πà‡∏≤‡∏¢‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏à‡∏≤‡∏Å‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\",\n",
      "      \"‡πÄ‡∏Å‡πá‡∏ö‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≥‡∏à‡∏≤‡∏Å‡∏ö‡πâ‡∏≤‡∏ô‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏ß‡∏à‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÅ‡∏•‡∏∞‡πÇ‡∏Ñ‡∏•‡∏¥‡∏ü‡∏≠‡∏£‡πå‡∏°\",\n",
      "      \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡πÅ‡∏´‡∏•‡πà‡∏á‡πÅ‡∏û‡∏£‡πà‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏£‡∏≠‡∏á\"\n",
      "    ],\n",
      "    \"Next_1-2_weeks\": [\n",
      "      \"‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏≠‡∏ö‡∏£‡∏°‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÉ‡∏´‡πâ‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏û‡∏ô‡∏±‡∏Å‡∏á‡∏≤‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î\",\n",
      "      \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û‡∏ô‡πâ‡∏≥‡πÅ‡∏•‡∏∞‡∏™‡∏∏‡∏Ç‡∏≤‡∏†‡∏¥‡∏ö‡∏≤‡∏•‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏£‡πâ‡∏≤‡∏ô‡∏Ç‡πâ‡∏≤‡∏ß‡∏´‡∏°‡∏π‡πÅ‡∏î‡∏á‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ï‡πà‡∏≠‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏•‡∏≤ 3 ‡πÄ‡∏î‡∏∑‡∏≠‡∏ô\"\n",
      "    ]\n",
      "  },\n",
      "  \"Flaws_Gaps\": [\n",
      "    \"‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Å‡∏≤‡∏£‡πÅ‡∏û‡∏£‡πà‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô ‡∏Å‡∏≤‡∏£‡∏õ‡∏¥‡∏î‡∏£‡πâ‡∏≤‡∏ô‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏Ñ‡∏∑‡∏ô‡∏≠‡∏≤‡∏´‡∏≤‡∏£\",\n",
      "    \"‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÄ‡∏ù‡πâ‡∏≤‡∏£‡∏∞‡∏ß‡∏±‡∏á‡∏ú‡∏π‡πâ‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡πÉ‡∏Å‡∏•‡πâ‡∏ä‡∏¥‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏™‡∏π‡∏á (‡πÄ‡∏ä‡πà‡∏ô ‡∏ú‡∏π‡πâ‡∏ä‡πà‡∏ß‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏≠‡∏≤‡∏´‡∏≤‡∏£)\",\n",
      "    \"‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏î‡∏≥‡πÄ‡∏ô‡∏¥‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏ô‡πÄ‡∏õ‡∏∑‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∏‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£\"\n",
      "  ],\n",
      "  \"Future_plan_add-ons\": [\n",
      "    \"‡∏à‡∏±‡∏î‡∏ó‡∏≥‡πÅ‡∏ú‡∏ô‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÅ‡∏ö‡∏ö case-control ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏´‡∏•‡∏±‡∏Å\",\n",
      "    \"‡∏£‡∏ß‡∏°‡∏ú‡∏•‡∏ï‡∏£‡∏ß‡∏à‡∏´‡πâ‡∏≠‡∏á‡∏õ‡∏è‡∏¥‡∏ö‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏™‡∏≤‡πÄ‡∏´‡∏ï‡∏∏\",\n",
      "    \"‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏∞‡∏°‡∏≤‡∏ï‡∏£‡∏Å‡∏≤‡∏£‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß‡πÅ‡∏Å‡πà‡∏≠‡∏á‡∏Ñ‡πå‡∏Å‡∏£‡∏õ‡∏Å‡∏Ñ‡∏£‡∏≠‡∏á‡∏™‡πà‡∏ß‡∏ô‡∏ó‡πâ‡∏≠‡∏á‡∏ñ‡∏¥‡πà‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÄ‡∏´‡∏ï‡∏∏‡∏Å‡∏≤‡∏£‡∏ì‡πå‡∏ã‡πâ‡∏≥\"\n",
      "  ],\n",
      "  \"Rationale\": [\n",
      "    \"‡∏≠‡∏±‡∏ï‡∏£‡∏≤‡∏Å‡∏≤‡∏£‡∏õ‡πà‡∏ß‡∏¢‡∏™‡∏π‡∏á (55.3%) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡∏û‡∏ö EPEC ‡πÉ‡∏ô‡∏ó‡∏±‡πâ‡∏á‡∏ú‡∏π‡πâ‡∏õ‡πà‡∏ß‡∏¢‡πÅ‡∏•‡∏∞‡∏ú‡∏π‡πâ‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏Å‡∏≤‡∏£‡∏ä‡∏µ‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡πÅ‡∏´‡∏•‡πà‡∏á‡πÅ‡∏û‡∏£‡πà‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡∏ó‡∏±‡∏ô‡∏ó‡∏µ\",\n",
      "    \"‡∏Å‡∏≤‡∏£‡∏û‡∏ö‡∏Ñ‡∏•‡∏≠‡∏£‡∏µ‡∏ô‡∏≠‡∏¥‡∏™‡∏£‡∏∞‡∏Ñ‡∏á‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡∏ô‡πâ‡∏≠‡∏¢‡∏Å‡∏ß‡πà‡∏≤ 0.1 ppm ‡πÉ‡∏ô‡∏ô‡πâ‡∏≥‡∏≠‡∏∏‡∏õ‡πÇ‡∏†‡∏Ñ‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏õ‡∏ô‡πÄ‡∏õ‡∏∑‡πâ‡∏≠‡∏ô‡πÄ‡∏ä‡∏∑‡πâ‡∏≠‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏≠‡∏≤‡∏´‡∏≤‡∏£\",\n",
      "    \"‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏≠‡∏∏‡∏õ‡∏Å‡∏£‡∏ì‡πå‡∏™‡∏±‡∏°‡∏ú‡∏±‡∏™‡∏≠‡∏≤‡∏´‡∏≤‡∏£‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ñ‡∏∏‡∏á‡∏°‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏´‡∏•‡∏±‡∏Å‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡πÉ‡∏ô‡πÅ‡∏ú‡∏ô‡∏£‡∏∞‡∏¢‡∏∞‡∏¢‡∏≤‡∏ß\"\n",
      "  ],\n",
      "  \"Response_Time\": \"62.66 ‡∏ß‡∏¥‡∏ô‡∏≤‡∏ó‡∏µ\",\n",
      "  \"Model_Used\": \"qwen/qwen3-32b\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(rag.iloc[0, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myapp)",
   "language": "python",
   "name": "myapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
