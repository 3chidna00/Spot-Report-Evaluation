{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0abc4ec6",
   "metadata": {},
   "source": [
    "# RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7e847422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import Dict, List\n",
    "from dataclasses import dataclass\n",
    "from PyPDF2 import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class DocumentChunk:\n",
    "    text: str\n",
    "    metadata: Dict[str, any]\n",
    "    chunk_id: str\n",
    "\n",
    "class FocusedThaiMedicalProcessor:\n",
    "    def __init__(self, model_name: str = \"BAAI/bge-m3\"):\n",
    "        try:\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            print(\"BGE-M3 model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_name}: {e}\")\n",
    "            print(\"Trying fallback model...\")\n",
    "            self.model = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n",
    "            print(\"Fallback model loaded!\")\n",
    "\n",
    "        # Initialize storage for embeddings and chunks\n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "\n",
    "    def extract_focused_sections(self, pdf_path: str, page_offset: int = 5) -> Dict[str, List[Dict[str, any]]]:\n",
    "      \"\"\"Extract only the two target sections (page-wise with page numbers).\"\"\"\n",
    "      reader = PdfReader(pdf_path)\n",
    "\n",
    "      # Sections you want (TOC/page-number based, 1-indexed in doc)\n",
    "      target_sections = {\n",
    "          'general_knowledge': (4, 23),            # ความรู้ทั่วไปของโรคอาหารเป็นพิษ\n",
    "          'investigation_guidelines': (24, 61),    # แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ\n",
    "      }\n",
    "\n",
    "      extracted_sections: Dict[str, List[Dict[str, any]]] = {}\n",
    "\n",
    "      for section_name, (start_page, end_page) in target_sections.items():\n",
    "          # Convert to 0-indexed actual PDF pages with offset\n",
    "          actual_start = start_page + page_offset - 1\n",
    "          actual_end_exclusive = end_page + page_offset - 1  # we'll use as exclusive in range()\n",
    "\n",
    "          print(\n",
    "              f\"Extracting {section_name}: TOC pages {start_page}-{end_page} \"\n",
    "              f\"(actual PDF pages {actual_start+1}-{actual_end_exclusive})\"\n",
    "          )\n",
    "\n",
    "          pages_list: List[Dict[str, any]] = []\n",
    "          # Iterate page-by-page; note: range() stop is exclusive\n",
    "          for p in range(actual_start, min(actual_end_exclusive, len(reader.pages))):\n",
    "              page_text = reader.pages[p].extract_text() or \"\"\n",
    "              page_text = self._clean_text(page_text)\n",
    "              pages_list.append({\n",
    "                  \"page_number\": p + 1,  # human-readable (1-based)\n",
    "                  \"text\": page_text\n",
    "              })\n",
    "\n",
    "          extracted_sections[section_name] = pages_list\n",
    "\n",
    "      return extracted_sections\n",
    "\n",
    "\n",
    "    def _clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean up extracted text\"\"\"\n",
    "        # Remove multiple newlines\n",
    "        text = re.sub(r'\\n\\s*\\n', '\\n\\n', text)\n",
    "        # Remove extra spaces\n",
    "        text = re.sub(r' +', ' ', text)\n",
    "        # Keep Thai characters, English, numbers, and basic punctuation\n",
    "        text = re.sub(r'[^\\u0E00-\\u0E7F\\u0020-\\u007E\\u00A0-\\u00FF\\n\\r\\t]', '', text)\n",
    "        return text.strip()\n",
    "\n",
    "    def create_focused_chunks(self, sections: Dict[str, List[Dict[str, any]]], chunk_size: int = 800) -> List[DocumentChunk]:\n",
    "      \"\"\"\n",
    "      Create chunks from page-wise sections and carry page numbers into metadata.\n",
    "\n",
    "      Expected sections format:\n",
    "      {\n",
    "        'section_name': [\n",
    "          { 'page_number': int, 'text': str },\n",
    "          ...\n",
    "        ],\n",
    "        ...\n",
    "      }\n",
    "      \"\"\"\n",
    "      all_chunks: List[DocumentChunk] = []\n",
    "\n",
    "      for section_name, pages in sections.items():\n",
    "          # Backward compatibility: if old code passes a big string, wrap it as a single pseudo-page\n",
    "          if isinstance(pages, str):\n",
    "              pages = [{\"page_number\": None, \"text\": pages}]\n",
    "\n",
    "          total_chars = sum(len(p[\"text\"]) for p in pages if p.get(\"text\"))\n",
    "          print(f\"\\nChunking {section_name} ({total_chars} characters across {len(pages)} pages)\")\n",
    "\n",
    "          # Build a list of (paragraph_text, page_number)\n",
    "          para_items: List[Dict[str, any]] = []\n",
    "          for item in pages:\n",
    "              pnum = item.get(\"page_number\")\n",
    "              ptxt = item.get(\"text\") or \"\"\n",
    "              # split by double newline into paragraphs (like old behavior)\n",
    "              for para in (ptxt.split(\"\\n\\n\") if ptxt else []):\n",
    "                  para = para.strip()\n",
    "                  if para:\n",
    "                      para_items.append({\"page_number\": pnum, \"text\": para})\n",
    "\n",
    "          # Greedy pack paragraphs into chunks\n",
    "          current_text = \"\"\n",
    "          current_pages_set = set()\n",
    "          chunks_for_section = []\n",
    "          for para in para_items:\n",
    "              candidate = (current_text + (\"\\n\\n\" if current_text else \"\") + para[\"text\"])\n",
    "              if current_text and len(candidate) > chunk_size:\n",
    "                  # flush current chunk\n",
    "                  if current_text.strip():\n",
    "                      page_list = sorted([p for p in current_pages_set if p is not None])\n",
    "                      page_start = page_list[0] if page_list else None\n",
    "                      page_end = page_list[-1] if page_list else None\n",
    "                      chunks_for_section.append({\n",
    "                          \"text\": current_text.strip(),\n",
    "                          \"page_start\": page_start,\n",
    "                          \"page_end\": page_end,\n",
    "                          \"pages\": page_list\n",
    "                      })\n",
    "                  # reset with current paragraph\n",
    "                  current_text = para[\"text\"]\n",
    "                  current_pages_set = set([para[\"page_number\"]])\n",
    "              else:\n",
    "                  # accumulate\n",
    "                  current_text = candidate\n",
    "                  if para[\"page_number\"] is not None:\n",
    "                      current_pages_set.add(para[\"page_number\"])\n",
    "\n",
    "          # flush last chunk\n",
    "          if current_text.strip():\n",
    "              page_list = sorted([p for p in current_pages_set if p is not None])\n",
    "              page_start = page_list[0] if page_list else None\n",
    "              page_end = page_list[-1] if page_list else None\n",
    "              chunks_for_section.append({\n",
    "                  \"text\": current_text.strip(),\n",
    "                  \"page_start\": page_start,\n",
    "                  \"page_end\": page_end,\n",
    "                  \"pages\": page_list\n",
    "              })\n",
    "\n",
    "          # Convert into DocumentChunk objects with page metadata\n",
    "          for i, ch in enumerate(chunks_for_section):\n",
    "              chunk = DocumentChunk(\n",
    "                  text=ch[\"text\"],\n",
    "                  metadata={\n",
    "                      'section_name': section_name,\n",
    "                      'chunk_index': i,\n",
    "                      'section_total_chunks': len(chunks_for_section),\n",
    "                      'language': 'thai',\n",
    "                      'source': 'thai_medical_guide',\n",
    "                      'length': len(ch[\"text\"]),\n",
    "                      # NEW: page info\n",
    "                      'page_start': ch[\"page_start\"],\n",
    "                      'page_end': ch[\"page_end\"],\n",
    "                      'pages': ch[\"pages\"],  # list[int]\n",
    "                  },\n",
    "                  chunk_id=f\"{section_name}_chunk_{i}\"\n",
    "              )\n",
    "              all_chunks.append(chunk)\n",
    "\n",
    "          print(f\"  Created {len(chunks_for_section)} chunks for {section_name}\")\n",
    "\n",
    "      print(f\"\\nTotal chunks created: {len(all_chunks)}\")\n",
    "      return all_chunks\n",
    "\n",
    "\n",
    "    def _split_into_chunks(self, text: str, chunk_size: int) -> List[str]:\n",
    "        \"\"\"Split text into chunks by paragraphs and size\"\"\"\n",
    "        paragraphs = text.split('\\n\\n')\n",
    "        chunks = []\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for paragraph in paragraphs:\n",
    "            # If adding this paragraph would exceed chunk size and we have content\n",
    "            if len(current_chunk + paragraph) > chunk_size and current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = paragraph\n",
    "            else:\n",
    "                current_chunk += \"\\n\\n\" + paragraph if current_chunk else paragraph\n",
    "\n",
    "        # Add the last chunk\n",
    "        if current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def generate_embeddings(self, chunks: List[DocumentChunk], batch_size: int = 32) -> np.ndarray:\n",
    "        \"\"\"Generate embeddings for chunks\"\"\"\n",
    "        texts = [chunk.text for chunk in chunks]\n",
    "\n",
    "        print(f\"Generating embeddings for {len(texts)} chunks...\")\n",
    "\n",
    "        try:\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                batch_size=batch_size,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            print(f\"Embeddings generated! Shape: {embeddings.shape}\")\n",
    "            return embeddings\n",
    "        except Exception as e:\n",
    "            print(f\"Error with batch_size {batch_size}: {e}\")\n",
    "            print(\"Retrying with smaller batch...\")\n",
    "            embeddings = self.model.encode(\n",
    "                texts,\n",
    "                batch_size=8,\n",
    "                show_progress_bar=True,\n",
    "                convert_to_numpy=True\n",
    "            )\n",
    "            print(f\"Embeddings generated with smaller batch! Shape: {embeddings.shape}\")\n",
    "            return embeddings\n",
    "\n",
    "    def build_system(self, pdf_path: str, chunk_size: int = 800):\n",
    "        \"\"\"Build the complete system\"\"\"\n",
    "        print(\"=== Building Focused Thai Medical System ===\")\n",
    "\n",
    "        # Step 1: Extract focused sections\n",
    "        print(\"\\nStep 1: Extracting target sections...\")\n",
    "        sections = self.extract_focused_sections(pdf_path)\n",
    "\n",
    "        # Show what we extracted\n",
    "        for section_name, content in sections.items():\n",
    "            print(f\"  {section_name}: {len(content)} characters\")\n",
    "\n",
    "        # Step 2: Create chunks\n",
    "        print(\"\\nStep 2: Creating chunks...\")\n",
    "        self.chunks = self.create_focused_chunks(sections, chunk_size)\n",
    "\n",
    "        # Step 3: Generate embeddings\n",
    "        print(\"\\nStep 3: Generating embeddings...\")\n",
    "        self.embeddings = self.generate_embeddings(self.chunks)\n",
    "\n",
    "        print(\"\\n=== System Ready! ===\")\n",
    "        print(f\"Total chunks: {len(self.chunks)}\")\n",
    "\n",
    "        # Show chunk distribution by section\n",
    "        section_counts = {}\n",
    "        for chunk in self.chunks:\n",
    "            section = chunk.metadata['section_name']\n",
    "            section_counts[section] = section_counts.get(section, 0) + 1\n",
    "        print(f\"Chunk distribution: {section_counts}\")\n",
    "\n",
    "    def save_system(self, filepath: str):\n",
    "        \"\"\"Save the system to local files\"\"\"\n",
    "        print(f\"💾 Saving system to {filepath}...\")\n",
    "\n",
    "        # Prepare data to save\n",
    "        data = {\n",
    "            'chunks': self.chunks,\n",
    "            'embeddings': self.embeddings,\n",
    "            'model_info': {\n",
    "                'model_name': 'BAAI/bge-m3',\n",
    "                'embedding_dim': self.embeddings.shape[1] if self.embeddings is not None else None,\n",
    "                'num_chunks': len(self.chunks)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Save with pickle\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(data, f)\n",
    "\n",
    "        file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB\n",
    "        print(f\"✅ System saved successfully! File size: {file_size:.2f} MB\")\n",
    "\n",
    "    def load_system(self, filepath: str):\n",
    "        \"\"\"Load the system from local files\"\"\"\n",
    "        print(f\"📂 Loading system from {filepath}...\")\n",
    "\n",
    "        with open(filepath, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        self.chunks = data['chunks']\n",
    "        self.embeddings = data['embeddings']\n",
    "\n",
    "        print(f\"✅ System loaded successfully!\")\n",
    "        print(f\"  Chunks: {len(self.chunks)}\")\n",
    "        print(f\"  Embeddings shape: {self.embeddings.shape}\")\n",
    "        print(f\"  Model info: {data.get('model_info', 'N/A')}\")\n",
    "\n",
    "    def search(self, query: str, top_k: int = 5, section_filter: str = None) -> List[Dict]:\n",
    "        \"\"\"Search for relevant chunks\"\"\"\n",
    "        if self.embeddings is None or not self.chunks:\n",
    "            raise ValueError(\"System not built or loaded. Please build or load system first.\")\n",
    "\n",
    "        print(f\"🔍 Searching: '{query}'\")\n",
    "\n",
    "        # Filter chunks by section if specified\n",
    "        if section_filter:\n",
    "            filtered_indices = [\n",
    "                i for i, chunk in enumerate(self.chunks)\n",
    "                if chunk.metadata['section_name'] == section_filter\n",
    "            ]\n",
    "            filtered_embeddings = self.embeddings[filtered_indices]\n",
    "            filtered_chunks = [self.chunks[i] for i in filtered_indices]\n",
    "            print(f\"  Filtering to section: {section_filter} ({len(filtered_chunks)} chunks)\")\n",
    "        else:\n",
    "            filtered_embeddings = self.embeddings\n",
    "            filtered_chunks = self.chunks\n",
    "            filtered_indices = list(range(len(self.chunks)))\n",
    "\n",
    "        # Generate query embedding\n",
    "        query_embedding = self.model.encode([query])\n",
    "\n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, filtered_embeddings)[0]\n",
    "\n",
    "        # Get top-k results\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            chunk = filtered_chunks[idx]\n",
    "            results.append({\n",
    "                'text': chunk.text,\n",
    "                'metadata': chunk.metadata,\n",
    "                'score': float(similarities[idx]),\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'relevance_score': float(similarities[idx])\n",
    "            })\n",
    "\n",
    "        print(f\"  Found {len(results)} results\")\n",
    "        return results\n",
    "\n",
    "#     def answer_question(self, question: str, top_k: int = 3, section_filter: str = None) -> str:\n",
    "#         \"\"\"Answer question based on retrieved context\"\"\"\n",
    "#         # Search for relevant chunks\n",
    "#         results = self.search(question, top_k, section_filter)\n",
    "\n",
    "#         if not results:\n",
    "#             return \"ไม่พบข้อมูลที่เกี่ยวข้องกับคำถาม\"\n",
    "\n",
    "#         # Combine context\n",
    "#         context_parts = []\n",
    "#         for r in results:\n",
    "#             section_name = r['metadata']['section_name']\n",
    "#             context_parts.append(f\"[{section_name}]: {r['text'][:200]}\")\n",
    "\n",
    "#         context = f\"{'...'*3}\\n\\n\".join(context_parts)\n",
    "\n",
    "#         # Generate answer\n",
    "#         answer = f\"\"\"ตามเอกสารแนวทางการสอบสวนและควบคุมโรคอาหารเป็นพิษ:\n",
    "\n",
    "# {context}\n",
    "\n",
    "# หมายเหตุ: ข้อมูลจากส่วน {', '.join(set([r['metadata']['section_name'] for r in results]))}\n",
    "# คะแนนความเกี่ยวข้อง: {[f\"{r['score']:.3f}\" for r in results]}\"\"\"\n",
    "\n",
    "#         return answer\n",
    "\n",
    "def create_system(pdf_path: str, save_filename: str = \"medical_system.pkl\",\n",
    "                 chunk_size: int = 800, model_name: str = \"models/bge-m3\"):\n",
    "    \"\"\"Create or load system with custom filename\"\"\"\n",
    "\n",
    "    # Ensure .pkl extension\n",
    "    if not save_filename.endswith('.pkl'):\n",
    "        save_filename += '.pkl'\n",
    "\n",
    "    processor = FocusedThaiMedicalProcessor(model_name)\n",
    "\n",
    "    # Check if saved system exists\n",
    "    if os.path.exists(save_filename):\n",
    "        print(f\"📂 Found existing system: {save_filename}\")\n",
    "        processor.load_system(save_filename)\n",
    "        return processor\n",
    "\n",
    "    # Build new system\n",
    "    print(f\"🚀 Building new system: {save_filename}\")\n",
    "    processor.build_system(pdf_path, chunk_size)\n",
    "\n",
    "    # Auto save\n",
    "    processor.save_system(save_filename)\n",
    "\n",
    "    return processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd590db2",
   "metadata": {},
   "source": [
    "## RAG System Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff21be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "from groq import Groq\n",
    "\n",
    "# Initialize Groq client\n",
    "GROQ_API_KEY = \"\"\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "class RAGEnhancementSystem:\n",
    "    def __init__(self, rag_system):\n",
    "        \"\"\"\n",
    "        Initialize with existing RAG system\n",
    "        \n",
    "        Args:\n",
    "            rag_system: ระบบ FocusedThaiMedicalProcessor ที่โหลดแล้ว\n",
    "        \"\"\"\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "    def extract_actionable_items(self, analysis_json: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        แยกข้อมูลจาก Actionable_Instructions และ Future_plan_add-ons\n",
    "        \n",
    "        Args:\n",
    "            analysis_json: JSON string จากผลลัพธ์ pipeline หลัก\n",
    "            \n",
    "        Returns:\n",
    "            List of actionable items to query\n",
    "        \"\"\"\n",
    "        try:\n",
    "            data = json.loads(analysis_json)\n",
    "            items = []\n",
    "            \n",
    "            # Extract from Actionable_Instructions\n",
    "            actionable = data.get(\"Actionable_Instructions\", {})\n",
    "            \n",
    "            # Next_24-72_hr\n",
    "            next_24_72 = actionable.get(\"Next_24-72_hr\", [])\n",
    "            for item in next_24_72:\n",
    "                items.append({\n",
    "                    \"text\": item,\n",
    "                    \"category\": \"Next_24-72_hr\",\n",
    "                    \"type\": \"actionable\"\n",
    "                })\n",
    "            \n",
    "            # Next_1-2_weeks\n",
    "            next_1_2_weeks = actionable.get(\"Next_1-2_weeks\", [])\n",
    "            for item in next_1_2_weeks:\n",
    "                items.append({\n",
    "                    \"text\": item,\n",
    "                    \"category\": \"Next_1-2_weeks\", \n",
    "                    \"type\": \"actionable\"\n",
    "                })\n",
    "            \n",
    "            # Future_plan_add-ons\n",
    "            future_plans = data.get(\"Future_plan_add-ons\", [])\n",
    "            for item in future_plans:\n",
    "                items.append({\n",
    "                    \"text\": item,\n",
    "                    \"category\": \"Future_plan_add-ons\",\n",
    "                    \"type\": \"future_plan\"\n",
    "                })\n",
    "            \n",
    "            print(f\"📋 แยกได้ {len(items)} รายการสำหรับการ query:\")\n",
    "            for i, item in enumerate(items, 1):\n",
    "                print(f\"   {i}. [{item['category']}] {item['text'][:60]}...\")\n",
    "            \n",
    "            return items\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"❌ Error parsing JSON: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error extracting items: {e}\")\n",
    "            return []\n",
    "\n",
    "    def query_each_item(self, items: List[Dict], top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Query แต่ละ item ใน RAG system\n",
    "        \n",
    "        Args:\n",
    "            items: List of items จาก extract_actionable_items\n",
    "            top_k: จำนวน results ที่ต้องการจากแต่ละ query\n",
    "            \n",
    "        Returns:\n",
    "            List ที่มีผลลัพธ์การ query ทั้งหมด\n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        \n",
    "        print(f\"\\n🔍 กำลัง query {len(items)} รายการใน RAG system...\")\n",
    "        \n",
    "        for i, item in enumerate(items, 1):\n",
    "            # print(f\"\\n--- Query {i}/{len(items)} ---\")\n",
    "            # print(f\"หมวด: {item['category']}\")\n",
    "            # print(f\"ข้อความ: {item['text']}\")\n",
    "            \n",
    "            # Query ใน RAG system\n",
    "            try:\n",
    "                results = self.rag_system.search(item['text'], top_k=top_k)\n",
    "                \n",
    "                item_result = {\n",
    "                    'original_item': item,\n",
    "                    'query_results': results,\n",
    "                    'query_success': True\n",
    "                }\n",
    "                \n",
    "                # print(f\"✅ พบ {len(results)} ผลลัพธ์\")\n",
    "                for j, result in enumerate(results, 1):\n",
    "                    section = self._translate_section_name(result['metadata']['section_name'])\n",
    "                    score = result['score']\n",
    "                    print(f\"   {j}. {section} (คะแนน: {score:.4f})\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Error querying: {e}\")\n",
    "                item_result = {\n",
    "                    'original_item': item,\n",
    "                    'query_results': [],\n",
    "                    'query_success': False,\n",
    "                    'error': str(e)\n",
    "                }\n",
    "            \n",
    "            all_results.append(item_result)\n",
    "        \n",
    "        return all_results\n",
    "\n",
    "    def _translate_section_name(self, section_name: str) -> str:\n",
    "        \"\"\"แปลงชื่อ section เป็นภาษาไทย\"\"\"\n",
    "        translations = {\n",
    "            'general_knowledge': 'ความรู้ทั่วไปของโรคอาหารเป็นพิษ',\n",
    "            'investigation_guidelines': 'แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ'\n",
    "        }\n",
    "        return translations.get(section_name, section_name)\n",
    "\n",
    "    def analyze_with_llm(self, original_json: str, rag_results: List[Dict]) -> str:\n",
    "        \"\"\"\n",
    "        ใช้ LLM วิเคราะห์ว่าข้อมูลจาก RAG มีประโยชน์หรือไม่\n",
    "        \n",
    "        Args:\n",
    "            original_json: JSON เดิมจาก pipeline\n",
    "            rag_results: ผลลัพธ์จาก RAG queries\n",
    "            \n",
    "        Returns:\n",
    "            Enhanced analysis with RAG information\n",
    "        \"\"\"\n",
    "        \n",
    "        # สร้าง context จาก RAG results\n",
    "        rag_context = self._format_rag_context(rag_results)\n",
    "        \n",
    "        # สร้าง prompt\n",
    "        system_prompt = self._create_analysis_prompt()\n",
    "        user_prompt = self._create_user_prompt(original_json, rag_context)\n",
    "        \n",
    "        print(\"\\n🧠 กำลังวิเคราะห์ด้วย LLM...\")\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"qwen/qwen3-32b\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "                max_tokens=4000,\n",
    "                timeout=60\n",
    "            )\n",
    "            \n",
    "            result = response.choices[0].message.content.strip()\n",
    "            result = strip_think(result)\n",
    "\n",
    "            try:\n",
    "                parsed = json.loads(result)\n",
    "                return json.dumps(parsed, ensure_ascii=False, indent=2)\n",
    "            except Exception as e:\n",
    "                print(f\"❌ JSON parse error: {e}\")\n",
    "                return result\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error in LLM analysis: {e}\")\n",
    "            return f\"Error in analysis: {str(e)}\"\n",
    "\n",
    "    def _format_rag_context(self, rag_results: List[Dict]) -> str:\n",
    "        \"\"\"จัดรูปแบบข้อมูลจาก RAG เป็น context สำหรับ LLM\"\"\"\n",
    "        \n",
    "        context_parts = []\n",
    "        \n",
    "        for i, result in enumerate(rag_results, 1):\n",
    "            original_item = result['original_item']\n",
    "            query_results = result.get('query_results', [])\n",
    "            \n",
    "            context_parts.append(f\"\\n=== รายการที่ {i} ===\")\n",
    "            context_parts.append(f\"หมวด: {original_item['category']}\")\n",
    "            context_parts.append(f\"ข้อเสนอแนะเดิม: {original_item['text']}\")\n",
    "            \n",
    "            if query_results:\n",
    "                context_parts.append(f\"ข้อมูลสนับสนุนจากเอกสาร ({len(query_results)} รายการ):\")\n",
    "                \n",
    "                for j, qr in enumerate(query_results, 1):\n",
    "                    section_thai = self._translate_section_name(qr['metadata']['section_name'])\n",
    "                    score = qr['score']\n",
    "                    text_preview = qr['text'][:300] + \"...\" if len(qr['text']) > 300 else qr['text']\n",
    "                    \n",
    "                    context_parts.append(f\"\\n  {j}. แหล่งที่มา: {section_thai}\")\n",
    "                    context_parts.append(f\"     คะแนนความเกี่ยวข้อง: {score:.4f}\")\n",
    "                    context_parts.append(f\"     เนื้อหา: {text_preview}\")\n",
    "                    context_parts.append(f\"     หน้า: {qr['metadata'].get('page_start','?')}-{qr['metadata'].get('page_end','?')}\")\n",
    "            else:\n",
    "                context_parts.append(\"ไม่พบข้อมูลสนับสนุนที่เกี่ยวข้อง\")\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "\n",
    "    def _create_analysis_prompt(self) -> str:\n",
    "        \"\"\"Create system prompt for guideline-based analysis\"\"\"\n",
    "\n",
    "        return \"\"\"You are an expert in foodborne disease outbreak investigation and control.\n",
    "    You are tasked with refining preliminary recommendations using the official guideline document as a reference.\n",
    "\n",
    "    Your responsibilities:\n",
    "    1. Analyze whether information from the guideline is relevant to improve the recommendation.\n",
    "    2. If relevant, enhance the recommendation to make it more accurate, complete, and aligned with the guideline.\n",
    "    3. If not relevant, keep the original recommendation unchanged.\n",
    "    4. Clearly specify whether RAG information was used or not.\n",
    "\n",
    "    Evaluation criteria:\n",
    "    - Consistency with guideline content\n",
    "    - Scientific accuracy\n",
    "    - Practical feasibility\n",
    "    - Completeness of recommendations\n",
    "\n",
    "    Response format:\n",
    "    Return the output strictly in JSON format. \n",
    "    For each recommendation:\n",
    "    - Always include: Original, Enhanced, and Use_RAG.\n",
    "    - If Use_RAG = true → also include Guideline_Reference, Page_Number, and Relevance_Score (decimal with 4 digits).\n",
    "    - If Use_RAG = false → do not include those fields.\"\"\"\n",
    "\n",
    "\n",
    "    def _create_user_prompt(self, original_json: str, rag_context: str) -> str:\n",
    "        \"\"\"Create user prompt for guideline-based recommendation refinement\"\"\"\n",
    "\n",
    "        return f\"\"\"\n",
    "    Original Analysis JSON:\n",
    "    {original_json}\n",
    "\n",
    "    Supporting context from guideline document:\n",
    "    {rag_context}\n",
    "\n",
    "    Please analyze and return the results strictly as a valid JSON object in the following structure:\n",
    "\n",
    "    {{\n",
    "    \"Analysis_Summary\": {{\n",
    "        \"Total_Items_Analyzed\": <int>,\n",
    "        \"Items_Enhanced_With_Guidelines\": <int>,\n",
    "        \"Items_Kept_Original\": <int>\n",
    "    }},\n",
    "    \"Enhanced_Recommendations\": {{\n",
    "        \"Next_24-72_hr\": [\n",
    "        {{\n",
    "            \"Original\": \"<original recommendation>\",\n",
    "            \"Enhanced\": \"<enhanced recommendation>\",\n",
    "            \"Use_RAG\": true,\n",
    "            \"Guideline_Reference\": \"<specific section or paragraph reference>\",\n",
    "            \"Page_Number\": \"<page number>\",\n",
    "            \"Relevance_Score\": \"<decimal with 4 digits>\"\n",
    "        }},\n",
    "        {{\n",
    "            \"Original\": \"<original recommendation>\",\n",
    "            \"Enhanced\": \"<enhanced recommendation>\",\n",
    "            \"Use_RAG\": false\n",
    "        }}\n",
    "        ],\n",
    "        \"Next_1-2_weeks\": [\n",
    "        {{\n",
    "            \"Original\": \"<original recommendation>\",\n",
    "            \"Enhanced\": \"<enhanced recommendation>\",\n",
    "            \"Use_RAG\": true,\n",
    "            \"Guideline_Reference\": \"<specific section or paragraph reference>\",\n",
    "            \"Page_Number\": \"<page number>\",\n",
    "            \"Relevance_Score\": \"<decimal with 4 digits>\"\n",
    "        }}\n",
    "        ],\n",
    "        \"Future_plan_add-ons\": [\n",
    "        {{\n",
    "            \"Original\": \"<original recommendation>\",\n",
    "            \"Enhanced\": \"<enhanced recommendation>\",\n",
    "            \"Use_RAG\": false\n",
    "        }}\n",
    "        ]\n",
    "    }},\n",
    "    \"Enhancement_Notes\": [\n",
    "        \"<note about how the recommendation was enhanced or why it was unchanged>\"\n",
    "    ]\n",
    "    }}\n",
    "\n",
    "    Notes:\n",
    "    - All recommendations must remain in Thai, but all JSON keys and structure must be in English.\n",
    "    - If Use_RAG = false, copy Original into Enhanced.\n",
    "    - Output must be pure JSON only. Do not include <think>, comments, or markdown fences.\n",
    "    \"\"\"\n",
    "\n",
    "    def run_enhancement(self, analysis_json: str, top_k: int = 3) -> str:\n",
    "        \"\"\"\n",
    "        รันกระบวนการ enhancement แบบเต็ม แล้ว 'รวมผล' กลับไปเป็น JSON สุดท้าย\n",
    "        \"\"\"\n",
    "        print(\"🚀 เริ่มกระบวนการ RAG Enhancement\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        # Step 1: Extract\n",
    "        items = self.extract_actionable_items(analysis_json)\n",
    "        if not items:\n",
    "            return \"Error: ไม่สามารถแยกข้อมูลจาก JSON ได้\"\n",
    "\n",
    "        # Step 2: Query RAG\n",
    "        print(f\"\\n📚 กำลัง query {len(items)} รายการใน knowledge base...\")\n",
    "        rag_results = self.query_each_item(items, top_k=top_k)\n",
    "\n",
    "        # Step 3: LLM Analysis\n",
    "        print(f\"\\n🔍 กำลังวิเคราะห์ความเกี่ยวข้องของข้อมูล...\")\n",
    "        enhanced_result = self.analyze_with_llm(analysis_json, rag_results)\n",
    "\n",
    "        # Step 4: Merge เข้ากับ original\n",
    "        print(f\"\\n🧩 รวมผลลัพธ์ RAG กับผลลัพธ์เดิม...\")\n",
    "        merged_final = merge_enhancements_with_original(analysis_json, enhanced_result)\n",
    "\n",
    "        print(f\"\\n✅ กระบวนการ RAG Enhancement เสร็จสิ้น\")\n",
    "        return merged_final\n",
    "\n",
    "def strip_think(text: str) -> str:\n",
    "    \"\"\"Remove <think> tags and markdown code fences from text.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "    cleaned = re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL)\n",
    "    cleaned = re.sub(r\"```(?:json)?\", \"\", cleaned)  # remove ``` or ```json\n",
    "    return cleaned.strip()\n",
    "\n",
    "def _sanitize_item(it: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Enforce rules:\n",
    "      - If Use_RAG == False: Enhanced must equal Original, and remove guideline fields.\n",
    "      - If Use_RAG == True: ensure required fields exist and normalize Relevance_Score to 4 decimals.\n",
    "    \"\"\"\n",
    "    it = dict(it)  # shallow copy\n",
    "    original = it.get(\"Original\", \"\")\n",
    "    use_rag = bool(it.get(\"Use_RAG\", False))\n",
    "\n",
    "    it[\"Original\"] = original\n",
    "\n",
    "    if not use_rag:\n",
    "        it[\"Use_RAG\"] = False\n",
    "        it[\"Enhanced\"] = original\n",
    "        # drop guideline-only fields if present\n",
    "        for k in (\"Guideline_Reference\", \"Page_Number\", \"Relevance_Score\"):\n",
    "            it.pop(k, None)\n",
    "    else:\n",
    "        it[\"Use_RAG\"] = True\n",
    "        # ensure Enhanced exists\n",
    "        it[\"Enhanced\"] = it.get(\"Enhanced\", original)\n",
    "        # normalize Relevance_Score\n",
    "        rs = it.get(\"Relevance_Score\", None)\n",
    "        if rs is not None and rs != \"\":\n",
    "            try:\n",
    "                it[\"Relevance_Score\"] = f\"{float(rs):.4f}\"\n",
    "            except Exception:\n",
    "                # keep as-is if cannot coerce\n",
    "                pass\n",
    "        # coerce Page_Number to string if present\n",
    "        if \"Page_Number\" in it and it[\"Page_Number\"] is not None:\n",
    "            it[\"Page_Number\"] = str(it[\"Page_Number\"])\n",
    "\n",
    "    return it\n",
    "\n",
    "def _sanitize_llm_output(llm: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Sanitize LLM JSON in-place: enforce the Use_RAG rules across all buckets.\n",
    "    \"\"\"\n",
    "    llm = dict(llm)\n",
    "    enh = dict(llm.get(\"Enhanced_Recommendations\", {}))\n",
    "    for key in (\"Next_24-72_hr\", \"Next_1-2_weeks\", \"Future_plan_add-ons\"):\n",
    "        arr = enh.get(key) or []\n",
    "        new_arr = []\n",
    "        for it in arr:\n",
    "            if isinstance(it, dict):\n",
    "                new_arr.append(_sanitize_item(it))\n",
    "            else:\n",
    "                # if LLM returned raw strings unexpectedly, wrap them\n",
    "                new_arr.append({\"Original\": it, \"Enhanced\": it, \"Use_RAG\": False})\n",
    "        enh[key] = new_arr\n",
    "    llm[\"Enhanced_Recommendations\"] = enh\n",
    "    return llm\n",
    "\n",
    "def _index_by_original(items):\n",
    "    \"\"\"Build dict: original_text -> sanitized item_dict\"\"\"\n",
    "    idx = {}\n",
    "    for it in items or []:\n",
    "        if not isinstance(it, dict):\n",
    "            # tolerate stray strings\n",
    "            it = {\"Original\": str(it), \"Enhanced\": str(it), \"Use_RAG\": False}\n",
    "        it = _sanitize_item(it)  # <-- enforce rules here\n",
    "        orig = it.get(\"Original\", \"\")\n",
    "        if orig:\n",
    "            idx[orig] = it\n",
    "    return idx\n",
    "\n",
    "def _merge_bucket(orig_list, idx_enh):\n",
    "    \"\"\"\n",
    "    Merge a bucket with enforcement:\n",
    "      - Match by Original text.\n",
    "      - If found in idx_enh: use the sanitized item.\n",
    "      - If not found: produce {Original, Enhanced=Original, Use_RAG=False}.\n",
    "      - Preserve original order.\n",
    "    \"\"\"\n",
    "    merged = []\n",
    "    for entry in orig_list:\n",
    "        if isinstance(entry, dict):\n",
    "            original_text = entry.get(\"Original\") or entry.get(\"Enhanced\") or \"\"\n",
    "            if original_text in idx_enh:\n",
    "                merged.append(_sanitize_item(idx_enh[original_text]))  # ensure sanitized\n",
    "            else:\n",
    "                base = {\n",
    "                    \"Original\": original_text or entry,\n",
    "                    \"Enhanced\": entry.get(\"Enhanced\", original_text or entry),\n",
    "                    \"Use_RAG\": bool(entry.get(\"Use_RAG\", False)),\n",
    "                }\n",
    "                # final sanitize\n",
    "                merged.append(_sanitize_item(base))\n",
    "        else:\n",
    "            # entry is string\n",
    "            if entry in idx_enh:\n",
    "                merged.append(_sanitize_item(idx_enh[entry]))\n",
    "            else:\n",
    "                merged.append({\n",
    "                    \"Original\": entry,\n",
    "                    \"Enhanced\": entry,  # enforce Use_RAG=False => same as original\n",
    "                    \"Use_RAG\": False\n",
    "                })\n",
    "    return merged\n",
    "\n",
    "def merge_enhancements_with_original(original_json_str: str, llm_json_str: str) -> str:\n",
    "    \"\"\"\n",
    "    - parse both sides\n",
    "    - sanitize LLM output\n",
    "    - index and merge\n",
    "    - attach Enhancement_Notes (if any)\n",
    "    \"\"\"\n",
    "    # parse original\n",
    "    try:\n",
    "        original = json.loads(original_json_str)\n",
    "    except Exception as e:\n",
    "        return json.dumps({\"error\": f\"Cannot parse original_json: {e}\"}, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # parse LLM (strip <think> / fences first)\n",
    "    llm_clean = strip_think(llm_json_str)\n",
    "    try:\n",
    "        llm_raw = json.loads(llm_clean)\n",
    "    except Exception as e:\n",
    "        final_out = dict(original)\n",
    "        final_out[\"Enhancement_Notes\"] = [\"LLM output could not be parsed as JSON.\", str(e)]\n",
    "        return json.dumps(final_out, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # --- NEW: sanitize the whole LLM block first ---\n",
    "    llm = _sanitize_llm_output(llm_raw)\n",
    "\n",
    "    # pull enhanced buckets\n",
    "    enh = llm.get(\"Enhanced_Recommendations\", {})\n",
    "    e_24_72 = enh.get(\"Next_24-72_hr\") or []\n",
    "    e_1_2w  = enh.get(\"Next_1-2_weeks\") or []\n",
    "    e_future = enh.get(\"Future_plan_add-ons\") or []\n",
    "\n",
    "    # build indexes\n",
    "    idx_24_72 = _index_by_original(e_24_72)\n",
    "    idx_1_2w  = _index_by_original(e_1_2w)\n",
    "    idx_future = _index_by_original(e_future)\n",
    "\n",
    "    # merge Actionable_Instructions\n",
    "    actionable = original.get(\"Actionable_Instructions\", {})\n",
    "    orig_24_72 = actionable.get(\"Next_24-72_hr\", [])\n",
    "    orig_1_2w  = actionable.get(\"Next_1-2_weeks\", [])\n",
    "    actionable[\"Next_24-72_hr\"] = _merge_bucket(orig_24_72, idx_24_72)\n",
    "    actionable[\"Next_1-2_weeks\"] = _merge_bucket(orig_1_2w, idx_1_2w)\n",
    "    original[\"Actionable_Instructions\"] = actionable\n",
    "\n",
    "    # merge Future_plan-add-ons\n",
    "    orig_future = original.get(\"Future_plan_add-ons\", [])\n",
    "    original[\"Future_plan_add-ons\"] = _merge_bucket(orig_future, idx_future)\n",
    "\n",
    "    # attach Enhancement_Notes at root\n",
    "    notes = llm.get(\"Enhancement_Notes\") or []\n",
    "    if notes:\n",
    "        original[\"Enhancement_Notes\"] = notes\n",
    "\n",
    "    return json.dumps(original, ensure_ascii=False, indent=2)\n",
    "\n",
    "# ==================== ฟังก์ชันสำหรับทดสอบ ====================\n",
    "\n",
    "def test_rag_enhancement():\n",
    "    \"\"\"ทดสอบระบบ RAG Enhancement\"\"\"\n",
    "    \n",
    "    # ตัวอย่าง JSON จาก pipeline หลัก\n",
    "    sample_analysis = '''{\n",
    "    \"Actions_Adequacy\": {\n",
    "        \"Adequacy\": false,\n",
    "        \"Recommendations\": [\n",
    "        \"ดำเนินการปิดสถานที่ร้านข้าวหมูแดงทันทีเพื่อควบคุมการแพร่กระจายเชื้อและตรวจสอบสุขาภิบาลอย่างเข้มงวด\",\n",
    "        \"จัดการฝึกอบรมสุขาภิบาลอาหารให้ผู้ประกอบการและผู้ช่วยโดยเร็วที่สุด พร้อมติดตามผลการอบรมเป็นระยะ\",\n",
    "        \"สั่งห้ามการจัดส่งหรือจำหน่ายอาหารจากแหล่งที่เกิดเหตุจนกว่าจะได้รับการรับรองความปลอดภัยจากหน่วยงานสาธารณสุข\"\n",
    "        ]\n",
    "    },\n",
    "    \"Actionable_Instructions\": {\n",
    "        \"Next_24-72_hr\": [\n",
    "        \"ตรวจสอบและควบคุมคลอรีนในน้ำอุปโภคบริโภคของร้านข้าวหมูแดงให้สอดคล้องกับมาตรฐานสุขอนามัย\",\n",
    "        \"เก็บตัวอย่างอาหารที่เหลือและอุปกรณ์สัมผัสอาหารเพิ่มเติมเพื่อตรวจหาเชื้อในห้องปฏิบัติการ\",\n",
    "        \"จัดทำแผนเฝ้าระวังผู้สัมผัสใกล้ชิดจากงานฌาปนกิจศพที่ยังไม่ได้รับการติดตาม\"\n",
    "        ],\n",
    "        \"Next_1-2_weeks\": [\n",
    "        \"ดำเนินการตรวจสอบสภาพแวดล้อมและระบบสุขาภิบาลของร้านข้าวหมูแดงอย่างละเอียด\",\n",
    "        \"จัดทำรายงานสรุปการสอบสวนระบาดวิทยาและสาเหตุการระบาดเพื่อรายงานหน่วยงานที่เกี่ยวข้อง\",\n",
    "        \"ติดตามผลการตรวจห้องปฏิบัติการทุกช่องทางและปรับแผนควบคุมโรคตามข้อมูลใหม่\"\n",
    "        ]\n",
    "    },\n",
    "    \"Flaws_Gaps\": [\n",
    "        \"ไม่มีมาตรการควบคุมเร่งด่วนสำหรับการปิดหรือกักกันแหล่งอาหารที่เป็นแหล่งแพร่เชื้อ\",\n",
    "        \"ขาดการเฝ้าระวังผู้สัมผัสอาหารโดยตรง (เช่น ผู้ประกอบการและผู้ช่วย) อย่างเป็นระบบ\",\n",
    "        \"ไม่มีการบังคับใช้มาตรการปรับปรุงสุขาภิบาลสถานที่จัดงานหรือร้านข้าวหมูแดงในทันที\"\n",
    "    ],\n",
    "    \"Future_plan_add-ons\": [\n",
    "        \"เพิ่มการวิเคราะห์เชิงปริมาณความสัมพันธ์ระหว่างรายการอาหารและอาการผู้ป่วยเพื่อยืนยันสาเหตุหลัก\",\n",
    "        \"จัดทำแนวทางการป้องกันโรคอาหารเป็นพิษสำหรับงานพิธีกรรมในชุมชน\",\n",
    "        \"ติดตามผลการตรวจโคลิฟอร์มแบคทีเรียในน้ำและอุปกรณ์สัมผัสอาหารเพื่อประเมินความเสี่ยงระยะยาว\"\n",
    "    ],\n",
    "    \"Rationale\": [\n",
    "        \"การไม่ปิดแหล่งอาหารทันทีอาจนำไปสู่การระบาดซ้ำในกลุ่มผู้ร่วมงานอื่นหรือชุมชนใกล้เคียง\",\n",
    "        \"การไม่เฝ้าระวังผู้สัมผัสอาหารโดยตรงอาจทำให้ไม่สามารถตรวจจับการแพร่เชื้อในวงกว้างได้ทันเวลา\",\n",
    "        \"การไม่บังคับใช้มาตรการปรับปรุงสุขาภิบาลในทันทีอาจทำให้เกิดการปนเปื้อนซ้ำในอนาคต\"\n",
    "    ],\n",
    "    \"Response_Time\": \"65.71 วินาที\",\n",
    "    \"Model_Used\": \"qwen/qwen3-32b\"\n",
    "    }'''\n",
    "    \n",
    "    print(\"🧪 เริ่มทดสอบ RAG Enhancement System\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # โหลด RAG system (ต้องมีไฟล์ medical_system.pkl อยู่แล้ว)\n",
    "    try:\n",
    "        rag_system = create_system(\"Docs/Guideline.pdf\", \"my_medical_ragv2.pkl\")\n",
    "        \n",
    "        # สร้าง enhancement system\n",
    "        enhancer = RAGEnhancementSystem(rag_system)\n",
    "        \n",
    "        # รันการ enhancement\n",
    "        result = enhancer.run_enhancement(sample_analysis, top_k=1)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"📊 ผลลัพธ์การ Enhancement:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(result)\n",
    "        \n",
    "        # บันทึกผลลัพธ์\n",
    "        # with open(\"enhanced_analysis.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        #     f.write(result)\n",
    "        # print(f\"\\n💾 บันทึกผลลัพธ์ไว้ที่ enhanced_analysis.json\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in testing: {e}\")\n",
    "        print(\"กรุณาตรวจสอบว่ามีไฟล์ medical_system.pkl และ import ถูกต้อง\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a347c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 เริ่มทดสอบ RAG Enhancement System\n",
      "============================================================\n",
      "BGE-M3 model loaded successfully!\n",
      "📂 Found existing system: my_medical_ragv2.pkl\n",
      "📂 Loading system from my_medical_ragv2.pkl...\n",
      "✅ System loaded successfully!\n",
      "  Chunks: 69\n",
      "  Embeddings shape: (69, 1024)\n",
      "  Model info: {'model_name': 'BAAI/bge-m3', 'embedding_dim': 1024, 'num_chunks': 69}\n",
      "🚀 เริ่มกระบวนการ RAG Enhancement\n",
      "============================================================\n",
      "📋 แยกได้ 9 รายการสำหรับการ query:\n",
      "   1. [Next_24-72_hr] ตรวจสอบและควบคุมคลอรีนในน้ำอุปโภคบริโภคของร้านข้าวหมูแดงให้ส...\n",
      "   2. [Next_24-72_hr] เก็บตัวอย่างอาหารที่เหลือและอุปกรณ์สัมผัสอาหารเพิ่มเติมเพื่อ...\n",
      "   3. [Next_24-72_hr] จัดทำแผนเฝ้าระวังผู้สัมผัสใกล้ชิดจากงานฌาปนกิจศพที่ยังไม่ได้...\n",
      "   4. [Next_1-2_weeks] ดำเนินการตรวจสอบสภาพแวดล้อมและระบบสุขาภิบาลของร้านข้าวหมูแดง...\n",
      "   5. [Next_1-2_weeks] จัดทำรายงานสรุปการสอบสวนระบาดวิทยาและสาเหตุการระบาดเพื่อรายง...\n",
      "   6. [Next_1-2_weeks] ติดตามผลการตรวจห้องปฏิบัติการทุกช่องทางและปรับแผนควบคุมโรคตา...\n",
      "   7. [Future_plan_add-ons] เพิ่มการวิเคราะห์เชิงปริมาณความสัมพันธ์ระหว่างรายการอาหารและ...\n",
      "   8. [Future_plan_add-ons] จัดทำแนวทางการป้องกันโรคอาหารเป็นพิษสำหรับงานพิธีกรรมในชุมชน...\n",
      "   9. [Future_plan_add-ons] ติดตามผลการตรวจโคลิฟอร์มแบคทีเรียในน้ำและอุปกรณ์สัมผัสอาหารเ...\n",
      "\n",
      "📚 กำลัง query 9 รายการใน knowledge base...\n",
      "\n",
      "🔍 กำลัง query 9 รายการใน RAG system...\n",
      "🔍 Searching: 'ตรวจสอบและควบคุมคลอรีนในน้ำอุปโภคบริโภคของร้านข้าวหมูแดงให้สอดคล้องกับมาตรฐานสุขอนามัย'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5311)\n",
      "🔍 Searching: 'เก็บตัวอย่างอาหารที่เหลือและอุปกรณ์สัมผัสอาหารเพิ่มเติมเพื่อตรวจหาเชื้อในห้องปฏิบัติการ'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.6529)\n",
      "🔍 Searching: 'จัดทำแผนเฝ้าระวังผู้สัมผัสใกล้ชิดจากงานฌาปนกิจศพที่ยังไม่ได้รับการติดตาม'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5264)\n",
      "🔍 Searching: 'ดำเนินการตรวจสอบสภาพแวดล้อมและระบบสุขาภิบาลของร้านข้าวหมูแดงอย่างละเอียด'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5473)\n",
      "🔍 Searching: 'จัดทำรายงานสรุปการสอบสวนระบาดวิทยาและสาเหตุการระบาดเพื่อรายงานหน่วยงานที่เกี่ยวข้อง'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.6796)\n",
      "🔍 Searching: 'ติดตามผลการตรวจห้องปฏิบัติการทุกช่องทางและปรับแผนควบคุมโรคตามข้อมูลใหม่'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5823)\n",
      "🔍 Searching: 'เพิ่มการวิเคราะห์เชิงปริมาณความสัมพันธ์ระหว่างรายการอาหารและอาการผู้ป่วยเพื่อยืนยันสาเหตุหลัก'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.6212)\n",
      "🔍 Searching: 'จัดทำแนวทางการป้องกันโรคอาหารเป็นพิษสำหรับงานพิธีกรรมในชุมชน'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5611)\n",
      "🔍 Searching: 'ติดตามผลการตรวจโคลิฟอร์มแบคทีเรียในน้ำและอุปกรณ์สัมผัสอาหารเพื่อประเมินความเสี่ยงระยะยาว'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5788)\n",
      "\n",
      "🔍 กำลังวิเคราะห์ความเกี่ยวข้องของข้อมูล...\n",
      "\n",
      "🧠 กำลังวิเคราะห์ด้วย LLM...\n",
      "\n",
      "🧩 รวมผลลัพธ์ RAG กับผลลัพธ์เดิม...\n",
      "\n",
      "✅ กระบวนการ RAG Enhancement เสร็จสิ้น\n",
      "\n",
      "============================================================\n",
      "📊 ผลลัพธ์การ Enhancement:\n",
      "============================================================\n",
      "{\n",
      "  \"Actions_Adequacy\": {\n",
      "    \"Adequacy\": false,\n",
      "    \"Recommendations\": [\n",
      "      \"ดำเนินการปิดสถานที่ร้านข้าวหมูแดงทันทีเพื่อควบคุมการแพร่กระจายเชื้อและตรวจสอบสุขาภิบาลอย่างเข้มงวด\",\n",
      "      \"จัดการฝึกอบรมสุขาภิบาลอาหารให้ผู้ประกอบการและผู้ช่วยโดยเร็วที่สุด พร้อมติดตามผลการอบรมเป็นระยะ\",\n",
      "      \"สั่งห้ามการจัดส่งหรือจำหน่ายอาหารจากแหล่งที่เกิดเหตุจนกว่าจะได้รับการรับรองความปลอดภัยจากหน่วยงานสาธารณสุข\"\n",
      "    ]\n",
      "  },\n",
      "  \"Actionable_Instructions\": {\n",
      "    \"Next_24-72_hr\": [\n",
      "      {\n",
      "        \"Original\": \"ตรวจสอบและควบคุมคลอรีนในน้ำอุปโภคบริโภคของร้านข้าวหมูแดงให้สอดคล้องกับมาตรฐานสุขอนามัย\",\n",
      "        \"Enhanced\": \"ตรวจสอบคลอรีนอิสระคงเหลือในน้ำอุปโภคบริโภคของร้านข้าวหมูแดงโดยใช้ชุดทดสอบคลอรีนอิสระคงเหลือ (Free Chlorine Test Kit) และควบคุมให้อยู่ในช่วง 0.2-0.5 mg/L ตามมาตรฐานสุขอนามัย\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"3) การตรวจความสะอาดของน้ำด้วยชุดทดสอบคลอรีนอิสระคงเหลือ (Free Chlorine Test Kit)\",\n",
      "        \"Page_Number\": \"61-61\",\n",
      "        \"Relevance_Score\": \"0.5311\"\n",
      "      },\n",
      "      {\n",
      "        \"Original\": \"เก็บตัวอย่างอาหารที่เหลือและอุปกรณ์สัมผัสอาหารเพิ่มเติมเพื่อตรวจหาเชื้อในห้องปฏิบัติการ\",\n",
      "        \"Enhanced\": \"เก็บตัวอย่างอาหารที่เหลือ อุปกรณ์สัมผัสอาหาร และน้ำในสถานที่เกิดเหตุอย่างถูกต้องตามขั้นตอนในเอกสารแนวทางเพื่อส่งตรวจหาเชื้อก่อโรคระบบทางเดินอาหารในห้องปฏิบัติการ\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"ขั้นตอนที่ 7 การศึกษาทางห้องปฏิบัติการ การเก็บตัวอย่างสิ่งส่งตรวจจากแหล่งต่างๆ อย่างถูกต้องและเหมาะสม\",\n",
      "        \"Page_Number\": \"52-52\",\n",
      "        \"Relevance_Score\": \"0.6529\"\n",
      "      },\n",
      "      {\n",
      "        \"Original\": \"จัดทำแผนเฝ้าระวังผู้สัมผัสใกล้ชิดจากงานฌาปนกิจศพที่ยังไม่ได้รับการติดตาม\",\n",
      "        \"Enhanced\": \"จัดทำแผนเฝ้าระวังผู้สัมผัสใกล้ชิดจากงานฌาปนกิจศพที่ยังไม่ได้รับการติดตาม\",\n",
      "        \"Use_RAG\": false\n",
      "      }\n",
      "    ],\n",
      "    \"Next_1-2_weeks\": [\n",
      "      {\n",
      "        \"Original\": \"ดำเนินการตรวจสอบสภาพแวดล้อมและระบบสุขาภิบาลของร้านข้าวหมูแดงอย่างละเอียด\",\n",
      "        \"Enhanced\": \"ดำเนินการตรวจสอบสภาพแวดล้อมและระบบสุขาภิบาลของร้านข้าวหมูแดงอย่างละเอียดเพื่อระบุกลไกการแพร่กระจายเชื้อและปัจจัยเสี่ยงที่เกี่ยวข้องกับการระบาด\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"การศึกษาสภาพแวดล้อมในพื้นที่เพื่อหาข้อมูลสนับสนุนการระบาด\",\n",
      "        \"Page_Number\": \"62-62\",\n",
      "        \"Relevance_Score\": \"0.5473\"\n",
      "      },\n",
      "      {\n",
      "        \"Original\": \"จัดทำรายงานสรุปการสอบสวนระบาดวิทยาและสาเหตุการระบาดเพื่อรายงานหน่วยงานที่เกี่ยวข้อง\",\n",
      "        \"Enhanced\": \"จัดทำรายงานสรุปการสอบสวนระบาดวิทยาและสาเหตุการระบาดในรูปแบบรายงานเบื้องต้น (Preliminary Report) และรายงานสรุปเพื่อรายงานหน่วยงานที่เกี่ยวข้องตามขั้นตอนในเอกสารแนวทาง\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"ขั้นตอนที่ 9 การเขียนรายงานและติดตามผลการควบคุมป้องกันโรค\",\n",
      "        \"Page_Number\": \"64-64\",\n",
      "        \"Relevance_Score\": \"0.6796\"\n",
      "      },\n",
      "      {\n",
      "        \"Original\": \"ติดตามผลการตรวจห้องปฏิบัติการทุกช่องทางและปรับแผนควบคุมโรคตามข้อมูลใหม่\",\n",
      "        \"Enhanced\": \"ติดตามผลการตรวจห้องปฏิบัติการทุกช่องทางอย่างต่อเนื่อง และปรับแผนควบคุมโรคตามข้อมูลใหม่ที่ได้รับเพื่อให้สอดคล้องกับแนวทางการสอบสวนการระบาด\",\n",
      "        \"Use_RAG\": true,\n",
      "        \"Guideline_Reference\": \"การติดตามผลการควบคุมป้องกันโรคและปรับแผนตามข้อมูลใหม่\",\n",
      "        \"Page_Number\": \"64-64\",\n",
      "        \"Relevance_Score\": \"0.5823\"\n",
      "      }\n",
      "    ]\n",
      "  },\n",
      "  \"Flaws_Gaps\": [\n",
      "    \"ไม่มีมาตรการควบคุมเร่งด่วนสำหรับการปิดหรือกักกันแหล่งอาหารที่เป็นแหล่งแพร่เชื้อ\",\n",
      "    \"ขาดการเฝ้าระวังผู้สัมผัสอาหารโดยตรง (เช่น ผู้ประกอบการและผู้ช่วย) อย่างเป็นระบบ\",\n",
      "    \"ไม่มีการบังคับใช้มาตรการปรับปรุงสุขาภิบาลสถานที่จัดงานหรือร้านข้าวหมูแดงในทันที\"\n",
      "  ],\n",
      "  \"Future_plan_add-ons\": [\n",
      "    {\n",
      "      \"Original\": \"เพิ่มการวิเคราะห์เชิงปริมาณความสัมพันธ์ระหว่างรายการอาหารและอาการผู้ป่วยเพื่อยืนยันสาเหตุหลัก\",\n",
      "      \"Enhanced\": \"เพิ่มการวิเคราะห์เชิงปริมาณความสัมพันธ์ระหว่างรายการอาหารและอาการผู้ป่วยโดยพิจารณาลักษณะทางคลินิกของผู้ป่วยเพื่อยืนยันสาเหตุหลักของการระบาด\",\n",
      "      \"Use_RAG\": true,\n",
      "      \"Guideline_Reference\": \"พิจารณาจากลักษณะทางคลินิกของผู้ป่วยเพื่อช่วยในการวินิจฉัยสาเหตุ\",\n",
      "      \"Page_Number\": \"36-36\",\n",
      "      \"Relevance_Score\": \"0.6212\"\n",
      "    },\n",
      "    {\n",
      "      \"Original\": \"จัดทำแนวทางการป้องกันโรคอาหารเป็นพิษสำหรับงานพิธีกรรมในชุมชน\",\n",
      "      \"Enhanced\": \"จัดทำแนวทางการป้องกันโรคอาหารเป็นพิษสำหรับงานพิธีกรรมในชุมชน\",\n",
      "      \"Use_RAG\": false\n",
      "    },\n",
      "    {\n",
      "      \"Original\": \"ติดตามผลการตรวจโคลิฟอร์มแบคทีเรียในน้ำและอุปกรณ์สัมผัสอาหารเพื่อประเมินความเสี่ยงระยะยาว\",\n",
      "      \"Enhanced\": \"ติดตามผลการตรวจโคลิฟอร์มแบคทีเรียในน้ำและอุปกรณ์สัมผัสอาหารโดยใช้ชุดทดสอบภาคสนาม (Test Kits) เพื่อประเมินความเสี่ยงระยะยาว\",\n",
      "      \"Use_RAG\": true,\n",
      "      \"Guideline_Reference\": \"การ Swab มือและภาชนะด้วยชุดทดสอบความสะอาดของภาชนะและมือ\",\n",
      "      \"Page_Number\": \"60-61\",\n",
      "      \"Relevance_Score\": \"0.5788\"\n",
      "    }\n",
      "  ],\n",
      "  \"Rationale\": [\n",
      "    \"การไม่ปิดแหล่งอาหารทันทีอาจนำไปสู่การระบาดซ้ำในกลุ่มผู้ร่วมงานอื่นหรือชุมชนใกล้เคียง\",\n",
      "    \"การไม่เฝ้าระวังผู้สัมผัสอาหารโดยตรงอาจทำให้ไม่สามารถตรวจจับการแพร่เชื้อในวงกว้างได้ทันเวลา\",\n",
      "    \"การไม่บังคับใช้มาตรการปรับปรุงสุขาภิบาลในทันทีอาจทำให้เกิดการปนเปื้อนซ้ำในอนาคต\"\n",
      "  ],\n",
      "  \"Response_Time\": \"65.71 วินาที\",\n",
      "  \"Model_Used\": \"qwen/qwen3-32b\",\n",
      "  \"Enhancement_Notes\": [\n",
      "    \"ปรับปรุงคำแนะนำให้สอดคล้องกับขั้นตอนการใช้ชุดทดสอบคลอรีนอิสระคงเหลือตามเอกสารแนวทาง\",\n",
      "    \"เพิ่มข้อกำหนดการเก็บตัวอย่างสิ่งส่งตรวจอย่างถูกต้องตามขั้นตอนในเอกสาร\",\n",
      "    \"ระบุช่วงค่าคลอรีนอิสระคงเหลือที่ยอมรับได้ตามมาตรฐานสุขอนามัย\",\n",
      "    \"เพิ่มการวิเคราะห์ลักษณะทางคลินิกของผู้ป่วยเพื่อช่วยยืนยันสาเหตุการระบาด\",\n",
      "    \"ปรับปรุงคำแนะนำให้ครอบคลุมการตรวจสอบกลไกการแพร่กระจายเชื้อในสภาพแวดล้อม\",\n",
      "    \"เพิ่มข้อกำหนดการจัดทำรายงานเบื้องต้นและรายงานสรุปตามขั้นตอนในเอกสาร\",\n",
      "    \"ระบุการใช้ชุดทดสอบภาคสนามในการตรวจโคลิฟอร์มแบคทีเรียตามแนวทาง\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "test_rag_enhancement()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae16356",
   "metadata": {},
   "source": [
    "# Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8d2c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from pypdf import PdfReader\n",
    "from groq import Groq\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "# ==================== CONFIG ====================\n",
    "# MODEL_NAME = \"qwen/qwen3-32b\"\n",
    "MODEL_NAME_QWEN = \"qwen/qwen3-32b\"\n",
    "MODEL_NAME_LLAMA = \"llama-3.1-8b-instant\"\n",
    "LLAMA_SHORT_NAME = \"llama3.1-8b\"\n",
    "\n",
    "os.environ['GROQ_API_KEY'] = \"\"\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "if not GROQ_API_KEY:\n",
    "    raise ValueError(\"Please set GROQ_API_KEY environment variable\")\n",
    "\n",
    "client = Groq(api_key=GROQ_API_KEY)\n",
    "\n",
    "# ==================== PDF EXTRACTION ====================\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Extract text from PDF file with basic cleanup.\"\"\"\n",
    "    reader = PdfReader(pdf_path)\n",
    "    text = \"\\n\".join(p.extract_text() or \"\" for p in reader.pages)\n",
    "    # Basic cleanup\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    text = re.sub(r\"\\n{2,}\", \"\\n\\n\", text).strip()\n",
    "    return text\n",
    "\n",
    "# ==================== SECTION PARSING ====================\n",
    "SECTION_KEYWORDS = (\n",
    "    \"ความเป็นมา\", \"ผลการสอบสวน\", \"สิ่งที่ดำเนินการไปแล้ว\",\n",
    "    \"สิ่งที่จะดำเนินการต่อไป\", \"ข้อเสนอแนะ\", \"ข้อเสนอแนะเพื่อพิจารณา\",\n",
    "    \"ลงชื่อ\", \"ทีมปฏิบัติการสอบสวน\"\n",
    ")\n",
    "\n",
    "BULLET_START = re.compile(r'^\\s*(?:\\d+(?:\\.\\d+)*[.)]|[•\\-–—])\\s+')\n",
    "SENT_END = re.compile(r'[.!?…]|[)\\]]|”|’|\"|\\'|น\\.$')\n",
    "\n",
    "CONTINUATION_START = re.compile(\n",
    "    r'^\\s*(?:\\d+[^\\d\\s]|และ|หรือ|โดย|แต่|รวมถึง|ทั้งนี้|ซึ่ง|ที่|จาก|ใน|ของ|ต่อมา|อีกทั้ง|รวมถึง)\\b'\n",
    ")\n",
    "\n",
    "HEADER_FULLLINE = [\n",
    "    re.compile(rf'^\\s*{re.escape(k)}\\s*$', flags=re.I) \n",
    "    for k in SECTION_KEYWORDS\n",
    "]\n",
    "\n",
    "def is_header(text: str) -> bool:\n",
    "    \"\"\"Check if text is a section header.\"\"\"\n",
    "    s = text.strip()\n",
    "    if not s:\n",
    "        return False\n",
    "    \n",
    "    # Exact-line header match\n",
    "    if any(pat.match(s) for pat in HEADER_FULLLINE):\n",
    "        return True\n",
    "    \n",
    "    # Heuristic header shape\n",
    "    if len(s) <= 40 and not SENT_END.search(s) and not BULLET_START.match(s):\n",
    "        return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def split_report(text: str) -> List[str]:\n",
    "    \"\"\"Split report into paragraphs.\"\"\"\n",
    "    # Normalize\n",
    "    t = re.sub(r'\\r\\n?', '\\n', text)\n",
    "    t = re.sub(r'[ \\t]+$', '', t, flags=re.M)\n",
    "    t = t.replace('\\f', '')\n",
    "    \n",
    "    # Mark headers\n",
    "    def mark_headers(tt):\n",
    "        for k in SECTION_KEYWORDS:\n",
    "            tt = re.sub(rf'(?m)^\\s*({re.escape(k)})(?:\\s*)$', r'\\n\\1\\n', tt)\n",
    "        return tt\n",
    "    \n",
    "    t = mark_headers(t)\n",
    "    raw = [p.strip() for p in re.split(r'\\n\\s*\\n+', t) if p.strip()]\n",
    "    \n",
    "    # Fix paragraph merging\n",
    "    fixed = []\n",
    "    for cur in raw:\n",
    "        if not fixed:\n",
    "            fixed.append(cur)\n",
    "            continue\n",
    "        \n",
    "        prev = fixed[-1]\n",
    "        \n",
    "        if is_header(cur):\n",
    "            fixed.append(cur)\n",
    "            continue\n",
    "        \n",
    "        lines = cur.splitlines()\n",
    "        all_bullets = lines and all(BULLET_START.match(x) or not x.strip() for x in lines)\n",
    "        if all_bullets:\n",
    "            fixed.append(cur)\n",
    "            continue\n",
    "        \n",
    "        # Merge criteria\n",
    "        short = len(cur) < 80\n",
    "        prev_not_end = not SENT_END.search(prev.splitlines()[-1])\n",
    "        contish = CONTINUATION_START.match(cur) or re.match(r'^\\s*\\d+([^\\d]|\\s|$)', cur)\n",
    "        single_line = len(lines) == 1\n",
    "        \n",
    "        if (short and single_line and (prev_not_end or contish)):\n",
    "            fixed[-1] = (prev.rstrip() + ' ' + cur.lstrip()).strip()\n",
    "        else:\n",
    "            fixed.append(cur)\n",
    "    \n",
    "    # Fix hard wraps within paragraphs\n",
    "    final = []\n",
    "    for para in fixed:\n",
    "        ls = [x.strip() for x in para.splitlines() if x.strip()]\n",
    "        if not ls:\n",
    "            continue\n",
    "        out = []\n",
    "        for line in ls:\n",
    "            if not out:\n",
    "                out.append(line)\n",
    "                continue\n",
    "            if BULLET_START.match(line) or BULLET_START.match(out[-1]):\n",
    "                out.append(line)\n",
    "            else:\n",
    "                out[-1] = (out[-1].rstrip() + ' ' + line.lstrip()).strip()\n",
    "        final.append('\\n'.join(out).strip())\n",
    "    \n",
    "    return final\n",
    "\n",
    "def extract_sections(paras: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Extract sections from paragraphs.\"\"\"\n",
    "    sections = {}\n",
    "    i = 0\n",
    "    while i < len(paras):\n",
    "        if is_header(paras[i]):\n",
    "            head = paras[i].strip()\n",
    "            i += 1\n",
    "            buf = []\n",
    "            while i < len(paras) and not is_header(paras[i]):\n",
    "                buf.append(paras[i].strip())\n",
    "                i += 1\n",
    "            sections[head] = \"\\n\\n\".join(buf).strip()\n",
    "        else:\n",
    "            i += 1\n",
    "    return sections\n",
    "\n",
    "ALIAS = {\n",
    "    \"situation\": [\"ความเป็นมา\", \"สถานการณ์\", \"background\", \"situation\"],\n",
    "    \"findings\": [\"ผลการสอบสวน\", \"ผลการตรวจสอบ\", \"ผลการศึกษา\", \"findings\"],\n",
    "    \"actions_done\": [\"สิ่งที่ดำเนินการไปแล้ว\", \"มาตรการที่ดำเนินการไปแล้ว\", \"actions taken\", \"actions done\"],\n",
    "    \"actions_next\": [\"สิ่งที่จะดำเนินการต่อไป\", \"มาตรการที่จะดำเนินการ\", \"next steps\", \"next actions\"],\n",
    "}\n",
    "\n",
    "def pick_section(sections: Dict[str, str], key: str) -> str:\n",
    "    \"\"\"Pick section text by key alias.\"\"\"\n",
    "    targets = ALIAS.get(key, [])\n",
    "    for h, txt in sections.items():\n",
    "        for t in targets:\n",
    "            if re.search(re.escape(t), h, flags=re.I):\n",
    "                return txt\n",
    "    return \"\"\n",
    "\n",
    "# ==================== SUMMARIZATION ====================\n",
    "def strip_think(text: str) -> str:\n",
    "    \"\"\"Remove <think> tags from text.\"\"\"\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL).strip()\n",
    "\n",
    "def run_groq(system_prompt: str, user_prompt: str, model_name: str, max_tokens: int = 2000) -> str:\n",
    "    \"\"\"Run Groq API call with error handling and model selection.\"\"\"\n",
    "    try:\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.2,\n",
    "            top_p=0.9,\n",
    "            max_tokens=max_tokens,\n",
    "            timeout=60\n",
    "        )\n",
    "        return strip_think(resp.choices[0].message.content)\n",
    "    except Exception as e:\n",
    "        print(f\"Groq API Error with {model_name}: {e}\")\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "\n",
    "# Summarization prompts\n",
    "SITUATION_SYSTEM = (\n",
    "    \"You are an assistant that summarizes outbreak investigation reports in Thai.\\n\"\n",
    "    \"Task: Summarize the 'ความเป็นมา' (Situation) section concisely and EXACTLY in the format below.\\n\"\n",
    "    \"STRICT FORMAT RULES:\\n\"\n",
    "    \"1) Return EXACTLY 5 lines. No extra lines, no blank lines, no leading/trailing spaces.\\n\"\n",
    "    \"2) Each line starts with a hyphen, a space, the header, a colon, a single space, then the content.\\n\"\n",
    "    \"3) Content must be on the SAME LINE as the header (do NOT break to a new line).\\n\"\n",
    "    \"4) Use semicolons ' ; ' to separate multiple facts in the SAME LINE.\\n\"\n",
    "    \"5) Use only information from the source text (no guessing). Numbers/dates/places must be exact.\\n\"\n",
    "    \"6) If a field is missing, write 'ไม่ระบุ'.\\n\"\n",
    "    \"OUTPUT (exactly these 5 lines, in this order):\\n\"\n",
    "    \"- สถานที่/เหตุการณ์: <content>\\n\"\n",
    "    \"- ช่วงเวลา/วันสำคัญ: <content>\\n\"\n",
    "    \"- กลุ่มเป้าหมาย/ผู้เสี่ยง: <content>\\n\"\n",
    "    \"- สาเหตุ/ยานพาหนะสงสัย (ถ้ามี): <content>\\n\"\n",
    "    \"- วัตถุประสงค์การสอบสวน: <content>\"\n",
    ")\n",
    "\n",
    "FINDINGS_SYSTEM = (\n",
    "    \"You are an assistant that summarizes outbreak investigation reports in Thai.\\n\"\n",
    "    \"Task: Summarize the 'ผลการสอบสวน' (Findings) section concisely and EXACTLY in the format below.\\n\"\n",
    "    \"STRICT FORMAT RULES:\\n\"\n",
    "    \"1) Return EXACTLY 10 lines. No extra lines, no blank lines, no leading/trailing spaces.\\n\"\n",
    "    \"2) Each line starts with a hyphen, a space, the header, a colon, a single space, then the content.\\n\"\n",
    "    \"3) All content MUST remain on the SAME LINE as its header (do NOT wrap to a new line).\\n\"\n",
    "    \"4) Use semicolons ' ; ' to separate multiple facts in the SAME LINE; keep original numbers/dates/times.\\n\"\n",
    "    \"5) Use only information from the source text (no guessing). If missing, write 'ไม่ระบุ'.\\n\"\n",
    "    \"OUTPUT (exactly these 10 lines, in this order):\\n\"\n",
    "    \"- ผู้ป่วย/สำรวจ/อัตราป่วย: <content>\\n\"\n",
    "    \"- เพศ–อายุ: <content>\\n\"\n",
    "    \"- อาการเด่น: <content>\\n\"\n",
    "    \"- เส้นโค้งการระบาด: <content>\\n\"\n",
    "    \"- ยานพาหนะสงสัย: <content>\\n\"\n",
    "    \"- การรักษา: <content>\\n\"\n",
    "    \"- การกระจายพื้นที่: <content>\\n\"\n",
    "    \"- ผลแล็บ: <content>\\n\"\n",
    "    \"- สิ่งแวดล้อม: <content>\\n\"\n",
    "    \"- ไทม์ไลน์อาหาร: <content>\"\n",
    ")\n",
    "\n",
    "\n",
    "def summarize_sections(situation_text: str, findings_text: str, verbose: bool = True) -> Tuple[str, str]:\n",
    "    \"\"\"Summarize situation and findings sections with optional verbose output.\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STEP 3: CREATING SUMMARIES\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Using {MODEL_NAME_QWEN} for summarization...\")\n",
    "    \n",
    "    # Use Qwen for summarization (since it's better at structured output)\n",
    "    situation_summary = run_groq(SITUATION_SYSTEM, f'{situation_text} /nothink', MODEL_NAME_QWEN, 1500)\n",
    "    findings_summary = run_groq(FINDINGS_SYSTEM, f'{findings_text} /nothink', MODEL_NAME_QWEN, 1500)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"✓ Summaries completed\")\n",
    "        print(f\"Situation summary length: {len(situation_summary)} characters\")\n",
    "        print(f\"Findings summary length: {len(findings_summary)} characters\")\n",
    "        print(\"\\nSITUATION SUMMARY:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(situation_summary)\n",
    "        \n",
    "        print(\"\\nFINDINGS SUMMARY:\")\n",
    "        print(\"-\" * 40)\n",
    "        print(findings_summary)\n",
    "        \n",
    "    \n",
    "    return situation_summary, findings_summary\n",
    "\n",
    "# ==================== ANALYSIS ====================\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a Thai-speaking field epidemiology reviewer. \"\n",
    "    \"Input: Situation, Findings, Actions, and Future Plan summaries from the Spot Report \"\n",
    "    \"(รายงานการสอบสวนเบื้องต้น) of a foodborne outbreak. \"\n",
    "    \"Task: evaluate adequacy of Actions already taken (สิ่งที่ดำเนินการไปแล้ว), \"\n",
    "    \"state if they are sufficient, and suggest improvements if not. \"\n",
    "    \"Also provide recommendations to strengthen the Future Plan. \"\n",
    "    \"Output must be raw JSON only with English key names and Thai content. \"\n",
    "    \"Use only these exact key names: Actions_Adequacy, Adequacy, Recommendations, \"\n",
    "    \"Actionable_Instructions, Next_24-72_hr, Next_1-2_weeks, Flaws_Gaps, Future_plan_add-ons, Rationale, Response_Time. \"\n",
    "    \"Start directly with { and end with }. No markdown, no explanatory text.\"\n",
    ")\n",
    "\n",
    "RUBRIC = \"\"\"\n",
    "เกณฑ์พิจารณา:\n",
    "- สอดคล้องกับภาพระบาดวิทยาและสถานการณ์\n",
    "- มาตรการควบคุมเร่งด่วน (อาหาร น้ำ สุขาภิบาล ผู้สัมผัสอาหาร อุปกรณ์ดิบ/สุก)\n",
    "- การจัดการผู้ป่วย/เฝ้าระวัง\n",
    "- ตรวจสิ่งแวดล้อมและสุขาภิบาลสถานที่\n",
    "- การเก็บตัวอย่าง/ผลแลบ\n",
    "- แผนวิเคราะห์เชิงสถิติและการบูรณาการแลบ\n",
    "\"\"\"\n",
    "\n",
    "USER_PROMPT_TEMPLATE = \"\"\"\n",
    "[SITUATION]\n",
    "{SITUATION}\n",
    "\n",
    "[FINDINGS]\n",
    "{FINDINGS}\n",
    "\n",
    "[ACTIONS]\n",
    "{ACTIONS}\n",
    "\n",
    "[FUTURE_PLAN]\n",
    "{FUTURE_PLAN}\n",
    "\n",
    "[เกณฑ์ประเมิน]\n",
    "{RUBRIC}\n",
    "\n",
    "คำสั่งเอาต์พุต: สร้าง JSON โดยใช้ key names เป็นภาษาอังกฤษเหมือนตัวอย่างนี้ทุกประการ:\n",
    "\n",
    "{{\n",
    "  \"Actions_Adequacy\": {{\n",
    "    \"Adequacy\": false,\n",
    "    \"Recommendations\": [\n",
    "      \"ข้อเสนอแนะภาษาไทย 1\",\n",
    "      \"ข้อเสนอแนะภาษาไทย 2\"\n",
    "    ]\n",
    "  }},\n",
    "  \"Actionable_Instructions\": {{\n",
    "    \"Next_24-72_hr\": [\n",
    "      \"การกระทำเร่งด่วนภาษาไทย 1\",\n",
    "      \"การกระทำเร่งด่วนภาษาไทย 2\"\n",
    "    ],\n",
    "    \"Next_1-2_weeks\": [\n",
    "      \"การกระทำระยะกลางภาษาไทย 1\",\n",
    "      \"การกระทำระยะกลางภาษาไทย 2\"\n",
    "    ]\n",
    "  }},\n",
    "  \"Flaws_Gaps\": [\n",
    "    \"ข้อบกพร่องภาษาไทย 1\",\n",
    "    \"ข้อบกพร่องภาษาไทย 2\"\n",
    "  ],\n",
    "  \"Future_plan_add-ons\": [\n",
    "    \"สิ่งที่ควรเพิ่มภาษาไทย 1\",\n",
    "    \"สิ่งที่ควรเพิ่มภาษาไทย 2\"\n",
    "  ],\n",
    "  \"Rationale\": [\n",
    "    \"เหตุผลภาษาไทย 1\",\n",
    "    \"เหตุผลภาษาไทย 2\"\n",
    "  ],\n",
    "  \"Response_Time\": \"21 วัน\"\n",
    "}}\n",
    "\n",
    "ข้อกำหนดสำคัญ:\n",
    "- ใช้ key names เป็นภาษาอังกฤษเท่านั้น เหมือนตัวอย่างข้างต้นทุกประการ\n",
    "- ห้ามใช้หัวข้อ # หรือข้อความอื่นเป็น key names\n",
    "- เนื้อหาใน values และ arrays เป็นภาษาไทย\n",
    "- Adequacy ใส่ true หรือ false เท่านั้น\n",
    "- ต้องอ้างอิงบริบทเหตุการณ์นี้โดยเฉพาะ (อาหาร สถานที่ คลอรีน อุปกรณ์ดิบ/สุก ผู้สัมผัสอาหาร)\n",
    "- ถ้าไม่พบข้อบกพร่อง ให้ใส่ [\"ไม่พบข้อบกพร่องสำคัญ\"] ใน Flaws_Gaps\n",
    "- เอาต์พุตต้องเป็น JSON ที่ valid โดยตรง ห้ามมี ```json wrapper หรือ markdown formatting\n",
    "- ห้ามมีข้อความอธิบายหรือคำอธิบายเพิ่มเติมนอกจาก JSON\n",
    "\"\"\".strip()\n",
    "\n",
    "def analyze_report_with_model(situation: str, findings: str, actions: str, future_plan: str, \n",
    "                             model_name: str, analysis_type: str = \"\", verbose: bool = True) -> Tuple[str, float]:\n",
    "    \"\"\"Analyze report with specified model and return JSON output with response time.\"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nRunning {analysis_type} with {model_name}...\")\n",
    "    \n",
    "    user_prompt = USER_PROMPT_TEMPLATE.format(\n",
    "        SITUATION=situation.strip(),\n",
    "        FINDINGS=findings.strip(),\n",
    "        ACTIONS=actions.strip() or \"(ไม่ได้ใส่ข้อมูลมา)\",\n",
    "        FUTURE_PLAN=future_plan.strip() or \"(ไม่ได้ใส่ข้อมูลมา)\",\n",
    "        RUBRIC=RUBRIC.strip()\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    output = run_groq(SYSTEM_PROMPT, user_prompt, model_name, 3000)\n",
    "    response_time = time.time() - start_time\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✓ Completed in {response_time:.2f}s\")\n",
    "    \n",
    "    # Try to parse and add response time to JSON\n",
    "    try:\n",
    "        parsed = json.loads(output)\n",
    "        parsed[\"Response_Time\"] = f\"{response_time:.2f} วินาที\"\n",
    "        parsed[\"Model_Used\"] = model_name\n",
    "        output = json.dumps(parsed, ensure_ascii=False, indent=2)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    return output, response_time\n",
    "\n",
    "\n",
    "def _update_response_time(json_str: str, model_seconds: float, rag_seconds: float, model_name: str) -> str:\n",
    "    total = float(model_seconds) + float(rag_seconds)\n",
    "    try:\n",
    "        obj = json.loads(json_str)\n",
    "        obj[\"Response_Time\"] = f\"{total:.2f} วินาที\"\n",
    "        obj[\"Model_Used\"] = model_name\n",
    "        return json.dumps(obj, ensure_ascii=False, indent=2)\n",
    "    except Exception:\n",
    "        # ถ้า parse ไม่ได้ ก็คืนค่าเดิม\n",
    "        return json_str\n",
    "\n",
    "# ==================== MAIN PIPELINE ====================\n",
    "def process_pdf_file(pdf_path: str, verbose: bool = True, rag_system: Optional[object] = None) -> Dict[str, any]:\n",
    "    \"\"\"Process a single PDF file with both models and return analysis results.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: path to PDF\n",
    "        verbose: print progress\n",
    "        rag_system: pre-built RAG system to reuse; if None, will create one here\n",
    "    \"\"\"\n",
    "    filename = Path(pdf_path).stem\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"PROCESSING FILE: {filename}\")\n",
    "        print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract text\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 1: EXTRACTING TEXT FROM PDF\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        raw_text = extract_text_from_pdf(pdf_path)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Text extracted: {len(raw_text)} characters\")\n",
    "            print(f\"Preview (first 200 chars): {raw_text[:200]}...\")\n",
    "        \n",
    "        # Step 2: Parse sections\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 2: PARSING SECTIONS\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        paras = split_report(raw_text)\n",
    "        sections = extract_sections(paras)\n",
    "        \n",
    "        situation_text = pick_section(sections, \"situation\")\n",
    "        findings_text = pick_section(sections, \"findings\")\n",
    "        actions_text = pick_section(sections, \"actions_done\")\n",
    "        future_text = pick_section(sections, \"actions_next\")\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"✓ Found {len(paras)} paragraphs\")\n",
    "            print(f\"✓ Identified {len(sections)} sections:\")\n",
    "            for section_name in sections.keys():\n",
    "                print(f\"    - {section_name}\")\n",
    "            \n",
    "            print(f\"\\nExtracted key sections:\")\n",
    "            print(f\"  Situation: {len(situation_text)} characters\")\n",
    "            print(f\"  Findings: {len(findings_text)} characters\")\n",
    "            print(f\"  Actions: {len(actions_text)} characters\") \n",
    "            print(f\"  Future: {len(future_text)} characters\")\n",
    "            \n",
    "            # Show preview of original sections\n",
    "            print(\"\\nORIGINAL SITUATION (first 300 chars):\")\n",
    "            print(\"-\" * 40)\n",
    "            print(situation_text[:300] + (\"...\" if len(situation_text) > 300 else \"\"))\n",
    "            \n",
    "            print(\"\\nORIGINAL FINDINGS (first 300 chars):\")\n",
    "            print(\"-\" * 40)\n",
    "            print(findings_text[:300] + (\"...\" if len(findings_text) > 300 else \"\"))\n",
    "        \n",
    "        # Step 3: Summarize (using Qwen)\n",
    "        situation_summary, findings_summary = summarize_sections(\n",
    "            situation_text, findings_text, verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Step 4: Run all 4 analyses\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 4: RUNNING ANALYSES WITH BOTH MODELS\")\n",
    "            print(\"=\"*60)\n",
    "        \n",
    "        # Qwen analyses\n",
    "        qwen_no_summary_result, qwen_no_summary_time = analyze_report_with_model(\n",
    "            situation_text, findings_text, actions_text, future_text,\n",
    "            MODEL_NAME_QWEN, \"Qwen No-Summary\", verbose=verbose\n",
    "        )\n",
    "        \n",
    "        qwen_summary_result, qwen_summary_time = analyze_report_with_model(\n",
    "            situation_summary, findings_summary, actions_text, future_text,\n",
    "            MODEL_NAME_QWEN, \"Qwen Summary\", verbose=verbose\n",
    "        )\n",
    "        \n",
    "        # Llama analyses\n",
    "        llama_no_summary_result, llama_no_summary_time = analyze_report_with_model(\n",
    "            situation_text, findings_text, actions_text, future_text,\n",
    "            MODEL_NAME_LLAMA, \"Llama No-Summary\", verbose=verbose\n",
    "        )\n",
    "        \n",
    "        llama_summary_result, llama_summary_time = analyze_report_with_model(\n",
    "            situation_summary, findings_summary, actions_text, future_text,\n",
    "            MODEL_NAME_LLAMA, \"Llama Summary\", verbose=verbose\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 5: PROCESSING SUMMARY\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"✓ File: {filename}\")\n",
    "            print(f\"✓ Qwen no-summary: {qwen_no_summary_time:.2f}s\")\n",
    "            print(f\"✓ Qwen summary: {qwen_summary_time:.2f}s\")  \n",
    "            print(f\"✓ Llama no-summary: {llama_no_summary_time:.2f}s\")\n",
    "            print(f\"✓ Llama summary: {llama_summary_time:.2f}s\")\n",
    "            print(f\"✓ Total model time: {(qwen_no_summary_time + qwen_summary_time + llama_no_summary_time + llama_summary_time):.2f}s\")\n",
    "            print(f\"✓ All analyses completed successfully\")\n",
    "            \n",
    "            # Show response length comparison\n",
    "            print(f\"\\nResponse lengths comparison:\")\n",
    "            print(f\"  Qwen no-summary: {len(qwen_no_summary_result)} chars\")\n",
    "            print(f\"  Qwen summary: {len(qwen_summary_result)} chars\")\n",
    "            print(f\"  Llama no-summary: {len(llama_no_summary_result)} chars\")\n",
    "            print(f\"  Llama summary: {len(llama_summary_result)} chars\")\n",
    "\n",
    "        # ---------------------------\n",
    "        # STEP 6: RAG ENHANCEMENT\n",
    "        # ---------------------------\n",
    "        if verbose:\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"STEP 6: RAG ENHANCEMENT (MERGE WITH GUIDELINES)\")\n",
    "            print(\"=\"*60)\n",
    "\n",
    "        # rag_total_start = time.time()\n",
    "\n",
    "        # Setup/Load RAG system (once or per call if None)\n",
    "        rag_setup_start = time.time()\n",
    "        if rag_system is None:\n",
    "            if verbose:\n",
    "                print(\"• Loading/Building RAG system...\")\n",
    "            rag_system = create_system(\"Docs/Guideline.pdf\", \"my_medical_ragv2.pkl\")\n",
    "        rag_setup_time = time.time() - rag_setup_start\n",
    "        if verbose:\n",
    "            print(f\"✓ RAG system ready ({rag_setup_time:.2f}s)\")\n",
    "\n",
    "        enhancer = RAGEnhancementSystem(rag_system)\n",
    "\n",
    "        # Qwen RAG\n",
    "        rag_qwen_start = time.time()\n",
    "        qwen_summary_rag = enhancer.run_enhancement(qwen_summary_result, top_k=1)\n",
    "        rag_qwen_time = time.time() - rag_qwen_start\n",
    "        qwen_summary_rag = _update_response_time(\n",
    "            qwen_summary_rag, \n",
    "            model_seconds=qwen_summary_time, \n",
    "            rag_seconds=rag_qwen_time, \n",
    "            model_name=MODEL_NAME_QWEN\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"✓ Qwen summary → RAG enhanced in {rag_qwen_time:.2f}s\")\n",
    "\n",
    "        # Llama RAG\n",
    "        rag_llama_start = time.time()\n",
    "        llama_summary_rag = enhancer.run_enhancement(llama_summary_result, top_k=1)\n",
    "        rag_llama_time = time.time() - rag_llama_start\n",
    "        if verbose:\n",
    "            print(f\"✓ Llama summary → RAG enhanced in {rag_llama_time:.2f}s\")\n",
    "        # rag_total_time = time.time() - rag_total_start\n",
    "        \n",
    "        llama_summary_rag = _update_response_time(\n",
    "            llama_summary_rag, \n",
    "            model_seconds=llama_summary_time, \n",
    "            rag_seconds=rag_llama_time, \n",
    "            model_name=MODEL_NAME_LLAMA\n",
    "        )\n",
    "        if verbose:\n",
    "            # print(f\"✓ Total RAG time: {rag_total_time:.2f}s\")\n",
    "            print(\"✓ RAG enhancement completed\")\n",
    "\n",
    "        summaries_output = f\"\"\"SITUATION SUMMARY\n",
    "{'-'*40}\n",
    "{situation_summary}\n",
    "        \n",
    "FINDINGS SUMMARY\n",
    "{'-'*40}\n",
    "{findings_summary}\"\"\"\n",
    "        \n",
    "        return {\n",
    "            \"file\": Path(pdf_path).stem,\n",
    "            \"situation_findings_summary\": summaries_output,\n",
    "            f\"{MODEL_NAME_QWEN}_no-summary\": qwen_no_summary_result,\n",
    "            f\"{MODEL_NAME_QWEN}_summary\": qwen_summary_result,\n",
    "            f\"{MODEL_NAME_QWEN}_summary_rag\": qwen_summary_rag,\n",
    "            f\"{LLAMA_SHORT_NAME}_no-summary\": llama_no_summary_result,\n",
    "            f\"{LLAMA_SHORT_NAME}_summary\": llama_summary_result,\n",
    "            f\"{LLAMA_SHORT_NAME}_summary_rag\": llama_summary_rag,\n",
    "            \"processing_success\": True\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"\\n❌ ERROR: {str(e)}\")\n",
    "            print(\"Processing failed\")\n",
    "        \n",
    "        return {\n",
    "            \"file\": filename,\n",
    "            f\"{MODEL_NAME_QWEN}_no-summary\": f\"Error: {str(e)}\",\n",
    "            f\"{MODEL_NAME_QWEN}_summary\": f\"Error: {str(e)}\",\n",
    "            f\"{MODEL_NAME_QWEN}_summary_rag\": f\"Error: {str(e)}\",\n",
    "            f\"{LLAMA_SHORT_NAME}_no-summary\": f\"Error: {str(e)}\",\n",
    "            f\"{LLAMA_SHORT_NAME}_summary\": f\"Error: {str(e)}\",\n",
    "            f\"{LLAMA_SHORT_NAME}_summary_rag\": f\"Error: {str(e)}\",\n",
    "            \"processing_success\": False\n",
    "        }\n",
    "\n",
    "def process_multiple_pdfs(pdf_paths: List[str], verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Process many PDFs. Build/load RAG system once and reuse.\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # โหลด/สร้าง RAG system ครั้งเดียว\n",
    "    rag_system = create_system(\"Docs/Guideline.pdf\", \"my_medical_ragv2.pkl\")\n",
    "\n",
    "    for pdf_path in pdf_paths:\n",
    "        result = process_pdf_file(pdf_path, verbose=verbose, rag_system=rag_system)  # ส่งเข้าไปเลย\n",
    "        results.append(result)\n",
    "        time.sleep(2)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    columns = [\n",
    "        \"file\",\n",
    "        \"situation_findings_summary\",\n",
    "        f\"{MODEL_NAME_QWEN}_no-summary\",\n",
    "        f\"{MODEL_NAME_QWEN}_summary\",\n",
    "        f\"{MODEL_NAME_QWEN}_summary_rag\",\n",
    "        f\"{LLAMA_SHORT_NAME}_no-summary\",\n",
    "        f\"{LLAMA_SHORT_NAME}_summary\",\n",
    "        f\"{LLAMA_SHORT_NAME}_summary_rag\",\n",
    "        \"processing_success\"\n",
    "    ]\n",
    "    for c in columns:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "    return df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3b5e452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGE-M3 model loaded successfully!\n",
      "📂 Found existing system: my_medical_ragv2.pkl\n",
      "📂 Loading system from my_medical_ragv2.pkl...\n",
      "✅ System loaded successfully!\n",
      "  Chunks: 69\n",
      "  Embeddings shape: (69, 1024)\n",
      "  Model info: {'model_name': 'BAAI/bge-m3', 'embedding_dim': 1024, 'num_chunks': 69}\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FILE: Exsum_food_poisoning\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "STEP 1: EXTRACTING TEXT FROM PDF\n",
      "============================================================\n",
      "✓ Text extracted: 10049 characters\n",
      "Preview (first 200 chars): รายงานการสอบสวนเบื้องต้น การระบาดโรคอาหารเป็นพิษ และอุจจาระร่วงเฉียบพลัน \n",
      " ตำบลคือเวียง อำเภอดอกคำใต้ จังหวัดพะเยา วันที่ 17-19 มิถุนายน 2568 \n",
      " \n",
      "ความเป็นมา \n",
      "วันที่ 15 มิถุนายน พ.ศ. 2568 เวลา 18.00 น. ...\n",
      "\n",
      "============================================================\n",
      "STEP 2: PARSING SECTIONS\n",
      "============================================================\n",
      "✓ Found 14 paragraphs\n",
      "✓ Identified 5 sections:\n",
      "    - ความเป็นมา\n",
      "    - ผลการสอบสวน\n",
      "    - สิ่งที่ดำเนินการไปแล้ว\n",
      "    - สิ่งที่จะดำเนินการต่อไป\n",
      "    - ข้อเสนอแนะเพื่อพิจารณา\n",
      "\n",
      "Extracted key sections:\n",
      "  Situation: 1024 characters\n",
      "  Findings: 6041 characters\n",
      "  Actions: 230 characters\n",
      "  Future: 102 characters\n",
      "\n",
      "ORIGINAL SITUATION (first 300 chars):\n",
      "----------------------------------------\n",
      "วันที่ 15 มิถุนายน พ.ศ. 2568 เวลา 18.00 น. สำนักงานสาธารณสุขจังหวัดพะเยาได้รับรายงานจาก โรงพยาบาลดอกคำใต้ พบกลุ่มผู้ป่วยโรคอาหารเป็นพิษประมาณ 40 ราย มีอาการคลื่นไส้ อาเจียน ปวดบิด แน่นท้อง และเวียนศีรษะ ภายหลังรับประทานอาหารมื้อกลางวันที่แจกภายในงานฌาปนกิจศพ ณ หมู่ 5 บ้านทุ่งกาไชย ตำบลคือเวียง อำเภอ...\n",
      "\n",
      "ORIGINAL FINDINGS (first 300 chars):\n",
      "----------------------------------------\n",
      "จากการค้นหาผู้ป่วยเพิ่มเติมด้วยนิยาม คือ (1) ประชาชนที่มาร่วมงานฌาปนกิจศพ หมู่ 5 ตำบลคือเวียง อำเภอดอกคำใต้ จังหวัดพะเยา หรือ (2) ประชาชนที่ได้รับประทานอาหารจากงานฌาปนกิจศพ โดยผู้ป่วยเพิ่มเติม ต้องมีอาการอย่างน้อย 2 อาการ จากอาการคลื่นไส้ อาเจียน ถ่ายเหลว /ถ่ายเป็นน้ำ ถ่ายเป็นมูกเลือด ปวดท้อง / ปวดบ...\n",
      "\n",
      "============================================================\n",
      "STEP 3: CREATING SUMMARIES\n",
      "============================================================\n",
      "Using qwen/qwen3-32b for summarization...\n",
      "✓ Summaries completed\n",
      "Situation summary length: 528 characters\n",
      "Findings summary length: 1674 characters\n",
      "\n",
      "SITUATION SUMMARY:\n",
      "----------------------------------------\n",
      "- สถานที่/เหตุการณ์: งานฌาปนกิจศพ ณ หมู่ 5 บ้านทุ่งกาไชย ตำบลคือเวียง อำเภอดอกคำใต้ จังหวัดพะเยา  \n",
      "- ช่วงเวลา/วันสำคัญ: 15-19 มิถุนายน พ.ศ. 2568 ; แจกอาหารเวลา 11.00–11.30 น. ; ผู้ป่วยรายแรกแสดงอาการเวลา 12.20 น.  \n",
      "- กลุ่มเป้าหมาย/ผู้เสี่ยง: ประชาชนประมาณ 400 คนที่รับประทานอาหารมื้อกลางวันในงานฌาปนกิจ  \n",
      "- สาเหตุ/ยานพาหนะสงสัย (ถ้ามี): อาหารมื้อกลางวันที่แจกภายในงานฌาปนกิจ  \n",
      "- วัตถุประสงค์การสอบสวน: ยืนยันการวินิจฉัยและการระบาด ; อธิบายลักษณะทางระบาดวิทยา ; ค้นหาแหล่งโรคและปัจจัยเสี่ยง ; ให้คำแนะนำมาตรการป้องกันและควบคุมโรค\n",
      "\n",
      "FINDINGS SUMMARY:\n",
      "----------------------------------------\n",
      "- ผู้ป่วย/สำรวจ/อัตราป่วย: พบผู้ป่วย 172 ราย จาก 311 ราย (อัตราป่วย ร้อยละ 55.3) จากประชาชนที่เข้าร่วมงานหรือรับประทานอาหารจากงาน  \n",
      "- เพศ–อายุ: อัตราส่วนเพศชายต่อเพศหญิง 1 ต่อ 1.5; อายุระหว่าง 3-94 ปี (มัธยฐานอายุ 56 ปี)  \n",
      "- อาการเด่น: อาการที่พบมากที่สุด ได้แก่ คลื่นไส้ ร้อยละ 95.9; รองลงมา อาเจียน ปวดท้อง/ปวดบิด/มวนท้อง ถ่ายเหลว/ถ่ายเป็นน้ำ ไข้ ถ่ายปนมูก/เลือด ร้อยละ 87.8, 77.9, 54.7, 12.8 และ 2.9 ตามลำดับ  \n",
      "- เส้นโค้งการระบาด: ผู้ป่วยรายแรกเริ่มมีอาการวันที่ 15 มิถุนายน 2568 เวลา 12.20 น. (หลังรับประทานอาหารประมาณ 60 นาที); พบผู้ป่วยมากที่สุดเวลา 15.00-15.59 น. (หลังรับประทานอาหารประมาณ 4 ชั่วโมง); พบผู้ป่วยรายสุดท้ายเวลา 21.00 น. (10 ชั่วโมงหลังแจกจ่ายอาหาร)  \n",
      "- ยานพาหนะสงสัย: ไม่ระบุ  \n",
      "- การรักษา: มีผู้ป่วยเข้ารับการรักษาที่สถานพยาบาล 119 ราย แบ่งเป็นผู้ป่วยนอก 110 ราย และผู้ป่วยใน 9 ราย  \n",
      "- การกระจายพื้นที่: ผู้ป่วยทั้งหมดมีภูมิลำเนาอยู่ในจังหวัดพะเยา โดยอยู่ในอำเภอดอกคำใต้ 169 ราย (ร้อยละ 98.3); อำเภอเมือง 2 ราย (ร้อยละ 1.2); และอำเภอภูซาง 1 ราย (ร้อยละ 0.6)  \n",
      "- ผลแล็บ: ผล RT-PCR พบ Enteropathogenic E.coli (EPEC) 5 ราย, Enteroaggregative E.coli (EAEC) 1 ราย; ไม่พบเชื้อ 4 ราย; ผลการเพาะเชื้อไม่พบเชื้อ 9 ราย รอผล 1 ราย; อยู่ระหว่างรอผลการตรวจเพาะเชื้อจากตัวอย่างอื่นๆ  \n",
      "- สิ่งแวดล้อม: สถานที่ประกอบอาหารเป็นลานกว้าง มีหลังคา ห่างจากห้องน้ำและบ่อเกรอะประมาณ 5 เมตร; ไม่มีอ่างล้างมือ; ผู้ประกอบการยังไม่ขออนุญาตสถานที่จำหน่ายอาหาร และยังไม่ผ่านการอบรมสุขาภิบาลอาหาร; พบแผลที่นิ้วก้อยมือของผู้ประกอบอาหาร  \n",
      "- ไทม์ไลน์อาหาร: งานศพจัดขึ้นระหว่างวันที่ 12-14 มิถุนายน 2568; วันที่ 15 มิถุนายน 2568 มีการจัดงานฌาปนกิจศพ มีผู้ร่วมงานประมาณ 400 คน; แจกจ่ายอาหารเวลา 11.00-11.30 น. ได้แก่ ข้าวหมูแดง ไอศกรีมกะทิ/ซ่าหริ่ม นมเปรี้ยว น้ำเปล่า (ขวด) น้ำเปล่า (แก้ว)\n",
      "\n",
      "============================================================\n",
      "STEP 4: RUNNING ANALYSES WITH BOTH MODELS\n",
      "============================================================\n",
      "\n",
      "Running Qwen No-Summary with qwen/qwen3-32b...\n",
      "✓ Completed in 62.66s\n",
      "\n",
      "Running Qwen Summary with qwen/qwen3-32b...\n",
      "✓ Completed in 30.29s\n",
      "\n",
      "Running Llama No-Summary with llama-3.1-8b-instant...\n",
      "✓ Completed in 1.15s\n",
      "\n",
      "Running Llama Summary with llama-3.1-8b-instant...\n",
      "✓ Completed in 17.26s\n",
      "\n",
      "============================================================\n",
      "STEP 5: PROCESSING SUMMARY\n",
      "============================================================\n",
      "✓ File: Exsum_food_poisoning\n",
      "✓ Qwen no-summary: 62.66s\n",
      "✓ Qwen summary: 30.29s\n",
      "✓ Llama no-summary: 1.15s\n",
      "✓ Llama summary: 17.26s\n",
      "✓ Total model time: 111.36s\n",
      "✓ All analyses completed successfully\n",
      "\n",
      "Response lengths comparison:\n",
      "  Qwen no-summary: 2027 chars\n",
      "  Qwen summary: 1549 chars\n",
      "  Llama no-summary: 1267 chars\n",
      "  Llama summary: 1433 chars\n",
      "\n",
      "============================================================\n",
      "STEP 6: RAG ENHANCEMENT (MERGE WITH GUIDELINES)\n",
      "============================================================\n",
      "✓ RAG system ready (0.00s)\n",
      "🚀 เริ่มกระบวนการ RAG Enhancement\n",
      "============================================================\n",
      "📋 แยกได้ 6 รายการสำหรับการ query:\n",
      "   1. [Next_24-72_hr] ตรวจสอบสภาพแวดล้อมและสุขาภิบาลสถานที่จัดงานอย่างละเอียด รวมถ...\n",
      "   2. [Next_24-72_hr] ติดตามอาการผู้ป่วยที่รักษาตัวในโรงพยาบาลและเฝ้าระวังผู้สัมผั...\n",
      "   3. [Next_1-2_weeks] จัดการฝึกอบรมสุขาภิบาลอาหารให้ผู้ประกอบการและผู้เกี่ยวข้องอย...\n",
      "   4. [Next_1-2_weeks] ดำเนินการสอบสวนเชิงลึกเพื่อระบุแหล่งแพร่เชื้อและปัจจัยเสี่ยง...\n",
      "   5. [Future_plan_add-ons] วิเคราะห์ข้อมูลเชิงปริมาณเพื่อหาความสัมพันธ์ระหว่างอาหารแต่ล...\n",
      "   6. [Future_plan_add-ons] บูรณาการผลตรวจห้องปฏิบัติการกับข้อมูลระบาดวิทยาเพื่อยืนยันสา...\n",
      "\n",
      "📚 กำลัง query 6 รายการใน knowledge base...\n",
      "\n",
      "🔍 กำลัง query 6 รายการใน RAG system...\n",
      "🔍 Searching: 'ตรวจสอบสภาพแวดล้อมและสุขาภิบาลสถานที่จัดงานอย่างละเอียด รวมถึงการล้างมือของผู้ประกอบการ'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5911)\n",
      "🔍 Searching: 'ติดตามอาการผู้ป่วยที่รักษาตัวในโรงพยาบาลและเฝ้าระวังผู้สัมผัสใกล้ชิด'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5674)\n",
      "🔍 Searching: 'จัดการฝึกอบรมสุขาภิบาลอาหารให้ผู้ประกอบการและผู้เกี่ยวข้องอย่างเป็นระบบ'\n",
      "  Found 1 results\n",
      "   1. ความรู้ทั่วไปของโรคอาหารเป็นพิษ (คะแนน: 0.4952)\n",
      "🔍 Searching: 'ดำเนินการสอบสวนเชิงลึกเพื่อระบุแหล่งแพร่เชื้อและปัจจัยเสี่ยงที่ชัดเจน'\n",
      "  Found 1 results\n",
      "   1. ความรู้ทั่วไปของโรคอาหารเป็นพิษ (คะแนน: 0.6430)\n",
      "🔍 Searching: 'วิเคราะห์ข้อมูลเชิงปริมาณเพื่อหาความสัมพันธ์ระหว่างอาหารแต่ละชนิดกับอัตราการป่วย'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.6497)\n",
      "🔍 Searching: 'บูรณาการผลตรวจห้องปฏิบัติการกับข้อมูลระบาดวิทยาเพื่อยืนยันสาเหตุเชิงชีวภาพ'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.6177)\n",
      "\n",
      "🔍 กำลังวิเคราะห์ความเกี่ยวข้องของข้อมูล...\n",
      "\n",
      "🧠 กำลังวิเคราะห์ด้วย LLM...\n",
      "\n",
      "🧩 รวมผลลัพธ์ RAG กับผลลัพธ์เดิม...\n",
      "\n",
      "✅ กระบวนการ RAG Enhancement เสร็จสิ้น\n",
      "✓ Qwen summary → RAG enhanced in 30.24s\n",
      "🚀 เริ่มกระบวนการ RAG Enhancement\n",
      "============================================================\n",
      "📋 แยกได้ 6 รายการสำหรับการ query:\n",
      "   1. [Next_24-72_hr] ตรวจสอบสิ่งแวดล้อมและสุขาภิบาลสถานที่ที่จัดงานฌาปนกิจศพภายใน...\n",
      "   2. [Next_24-72_hr] ตรวจสอบและปรับปรุงระบบสุขาภิบาลอาหารของผู้ประกอบการร้านข้าวห...\n",
      "   3. [Next_1-2_weeks] ดำเนินการตรวจสอบและปรับปรุงระบบสุขาภิบาลอาหารของผู้ประกอบการ...\n",
      "   4. [Next_1-2_weeks] ดำเนินการตรวจสอบสิ่งแวดล้อมและสุขาภิบาลสถานที่ที่จัดงานฌาปนก...\n",
      "   5. [Future_plan_add-ons] ควรดำเนินการตรวจสอบสิ่งแวดล้อมและสุขาภิบาลสถานที่ที่จัดงานฌา...\n",
      "   6. [Future_plan_add-ons] ควรดำเนินการตรวจสอบและปรับปรุงระบบสุขาภิบาลอาหารของผู้ประกอบ...\n",
      "\n",
      "📚 กำลัง query 6 รายการใน knowledge base...\n",
      "\n",
      "🔍 กำลัง query 6 รายการใน RAG system...\n",
      "🔍 Searching: 'ตรวจสอบสิ่งแวดล้อมและสุขาภิบาลสถานที่ที่จัดงานฌาปนกิจศพภายใน 24-72 ชั่วโมง'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5557)\n",
      "🔍 Searching: 'ตรวจสอบและปรับปรุงระบบสุขาภิบาลอาหารของผู้ประกอบการร้านข้าวหมูแดงภายใน 24-72 ชั่วโมง'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5739)\n",
      "🔍 Searching: 'ดำเนินการตรวจสอบและปรับปรุงระบบสุขาภิบาลอาหารของผู้ประกอบการร้านข้าวหมูแดงภายใน 1-2 สัปดาห์'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5686)\n",
      "🔍 Searching: 'ดำเนินการตรวจสอบสิ่งแวดล้อมและสุขาภิบาลสถานที่ที่จัดงานฌาปนกิจศพภายใน 1-2 สัปดาห์'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5379)\n",
      "🔍 Searching: 'ควรดำเนินการตรวจสอบสิ่งแวดล้อมและสุขาภิบาลสถานที่ที่จัดงานฌาปนกิจศพอย่างละเอียด'\n",
      "  Found 1 results\n",
      "   1. แนวทางการสอบสวนการระบาดโรคอาหารเป็นพิษ (คะแนน: 0.5091)\n",
      "🔍 Searching: 'ควรดำเนินการตรวจสอบและปรับปรุงระบบสุขาภิบาลอาหารของผู้ประกอบการร้านข้าวหมูแดง'\n",
      "  Found 1 results\n",
      "   1. ความรู้ทั่วไปของโรคอาหารเป็นพิษ (คะแนน: 0.5034)\n",
      "\n",
      "🔍 กำลังวิเคราะห์ความเกี่ยวข้องของข้อมูล...\n",
      "\n",
      "🧠 กำลังวิเคราะห์ด้วย LLM...\n",
      "\n",
      "🧩 รวมผลลัพธ์ RAG กับผลลัพธ์เดิม...\n",
      "\n",
      "✅ กระบวนการ RAG Enhancement เสร็จสิ้น\n",
      "✓ Llama summary → RAG enhanced in 51.04s\n",
      "✓ RAG enhancement completed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- เลือกไฟล์ PDF ที่ต้องการประมวลผล (วิธี A: ระบุรายการเอง) ---\n",
    "pdf_paths = [\n",
    "    \"Docs/Exsum_food_poisoning.pdf\",\n",
    "    # \"Docs/Another_report.pdf\",\n",
    "]\n",
    "\n",
    "# รัน pipeline สำหรับหลายไฟล์ (ฟังก์ชันนี้จะสร้าง/โหลด RAG system ครั้งเดียวและ reuse)\n",
    "df_multi = process_multiple_pdfs(pdf_paths, verbose=True)\n",
    "\n",
    "# บันทึกผลลัพธ์ (ใช้ utf-8-sig หากจะเปิดด้วย Excel ภาษาไทยให้แสดงถูกต้อง)\n",
    "df_multi.to_csv(\"analysis_results_rag_multi.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be518965",
   "metadata": {},
   "source": [
    "## Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7f3636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>situation_findings_summary</th>\n",
       "      <th>qwen/qwen3-32b_no-summary</th>\n",
       "      <th>qwen/qwen3-32b_summary</th>\n",
       "      <th>qwen/qwen3-32b_summary_rag</th>\n",
       "      <th>llama3.1-8b_no-summary</th>\n",
       "      <th>llama3.1-8b_summary</th>\n",
       "      <th>llama3.1-8b_summary_rag</th>\n",
       "      <th>processing_success</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Exsum_food_poisoning</td>\n",
       "      <td>SITUATION SUMMARY\\n---------------------------...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>{\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   file                         situation_findings_summary  \\\n",
       "0  Exsum_food_poisoning  SITUATION SUMMARY\\n---------------------------...   \n",
       "\n",
       "                           qwen/qwen3-32b_no-summary  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                              qwen/qwen3-32b_summary  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                          qwen/qwen3-32b_summary_rag  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                              llama3.1-8b_no-summary  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                                 llama3.1-8b_summary  \\\n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...   \n",
       "\n",
       "                             llama3.1-8b_summary_rag  processing_success  \n",
       "0  {\\n  \"Actions_Adequacy\": {\\n    \"Adequacy\": fa...                True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rag = pd.read_csv('analysis_results_rag_latest.csv')\n",
    "display(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "655fb4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag.to_excel('analysis_results_rag_latest.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d003c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Actions_Adequacy\": {\n",
      "    \"Adequacy\": false,\n",
      "    \"Recommendations\": [\n",
      "      \"ดำเนินการปิดสถานที่ร้านข้าวหมูแดงชั่วคราวจนกว่าจะได้รับการรับรองมาตรฐานสุขาภิบาลอาหาร\",\n",
      "      \"จัดทำแผนควบคุมการแพร่กระจายเชื้อโดยเน้นการแยกภาชนะดิบ/สุกและทำความสะอาดอุปกรณ์ทั้งหมดด้วยสารฆ่าเชื้อที่เหมาะสม\",\n",
      "      \"จัดการเฝ้าระวังผู้ร่วมงานศพที่ยังไม่แสดงอาการเป็นระยะเวลา 7 วัน (ระยะฟักตัวเฉลี่ยของ EPEC คือ 1-3 วัน)\",\n",
      "      \"จัดการให้ผู้ประกอบการผ่านการอบรมสุขาภิบาลอาหารตามมาตรฐานกรมอนามัยก่อนเปิดดำเนินการอีกครั้ง\"\n",
      "    ]\n",
      "  },\n",
      "  \"Actionable_Instructions\": {\n",
      "    \"Next_24-72_hr\": [\n",
      "      \"สั่งห้ามการจัดจำหน่ายอาหารจากร้านข้าวหมูแดงทันที\",\n",
      "      \"เก็บตัวอย่างน้ำจากบ้านผู้ประกอบการเพิ่มเติมเพื่อตรวจคลอรีนอิสระคงเหลือและโคลิฟอร์ม\",\n",
      "      \"จัดทำบันทึกประวัติการสัมผัสอาหารของผู้ป่วยทั้งหมดเพื่อหาแหล่งแพร่เชื้อรอง\"\n",
      "    ],\n",
      "    \"Next_1-2_weeks\": [\n",
      "      \"ดำเนินการอบรมสุขาภิบาลอาหารให้ผู้ประกอบการและพนักงานทั้งหมด\",\n",
      "      \"จัดทำแผนตรวจสอบคุณภาพน้ำและสุขาภิบาลสถานที่ร้านข้าวหมูแดงอย่างต่อเนื่องเป็นเวลา 3 เดือน\"\n",
      "    ]\n",
      "  },\n",
      "  \"Flaws_Gaps\": [\n",
      "    \"ไม่มีมาตรการควบคุมการแพร่กระจายเชื้อในระยะเริ่มต้น เช่น การปิดร้านหรือเรียกคืนอาหาร\",\n",
      "    \"ไม่ได้จัดการเฝ้าระวังผู้สัมผัสใกล้ชิดที่มีความเสี่ยงสูง (เช่น ผู้ช่วยประกอบอาหาร)\",\n",
      "    \"ไม่ได้ดำเนินการตรวจสอบการปนเปื้อนของเชื้อในอุปกรณ์สัมผัสอาหารที่ใช้ร่วมกันระหว่างการปรุงอาหาร\"\n",
      "  ],\n",
      "  \"Future_plan_add-ons\": [\n",
      "    \"จัดทำแผนวิเคราะห์เชิงสถิติแบบ case-control เพื่อยืนยันปัจจัยเสี่ยงหลัก\",\n",
      "    \"รวมผลตรวจห้องปฏิบัติการของตัวอย่างอาหารและผู้ป่วยเข้ากับการวิเคราะห์เชิงสาเหตุ\",\n",
      "    \"เสนอแนะมาตรการป้องกันระยะยาวแก่องค์กรปกครองส่วนท้องถิ่นเพื่อป้องกันเหตุการณ์ซ้ำ\"\n",
      "  ],\n",
      "  \"Rationale\": [\n",
      "    \"อัตราการป่วยสูง (55.3%) และการพบ EPEC ในทั้งผู้ป่วยและผู้ประกอบการชี้ให้เห็นความจำเป็นต้องควบคุมแหล่งแพร่เชื้อทันที\",\n",
      "    \"การพบคลอรีนอิสระคงเหลือน้อยกว่า 0.1 ppm ในน้ำอุปโภคบ่งชี้ถึงความเสี่ยงต่อการปนเปื้อนเชื้อในทุกขั้นตอนการจัดการอาหาร\",\n",
      "    \"การใช้อุปกรณ์สัมผัสอาหารร่วมกันโดยไม่เปลี่ยนถุงมือเป็นปัจจัยเสี่ยงหลักที่ต้องแก้ไขในแผนระยะยาว\"\n",
      "  ],\n",
      "  \"Response_Time\": \"62.66 วินาที\",\n",
      "  \"Model_Used\": \"qwen/qwen3-32b\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(rag.iloc[0, 2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myapp)",
   "language": "python",
   "name": "myapp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
